{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.utils as utils\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "### hyper params\n",
    "# model\n",
    "ctx_len = 128\n",
    "n_emb = 128\n",
    "dropout = 0.1\n",
    "head_size = 128\n",
    "n_heads = 4\n",
    "n_layers = 3\n",
    "\n",
    "# training\n",
    "num_epochs=20\n",
    "batch_size=64\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "itos = {i:c for i,c in enumerate(vocab)} # int to string\n",
    "stoi = {c:i for i,c in enumerate(vocab)} # string to int\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "data = encode(text)\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "\n",
    "### Data Prep\n",
    "ctx_len = 8\n",
    "X_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])\n",
    "y_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])\n",
    "X_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n",
    "y_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n",
    "\n",
    "\n",
    "def get_batches(X, y, b_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        ix = np.arange(X.shape[0])\n",
    "        np.random.shuffle(ix)\n",
    "        ix = mx.array(ix)\n",
    "        X = X[ix]\n",
    "        y = y[ix]\n",
    "    for i in range(0, X.shape[0], b_size):\n",
    "        input = X[i:i+b_size]\n",
    "        label = y[i:i+b_size]\n",
    "        yield input, label\n",
    "\n",
    "### Model Definition\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_emb)\n",
    "        self.wpe = nn.Embedding(ctx_len, n_emb)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)],\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(dims=n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        self._init_parameters()\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.wte(x)\n",
    "        pos_emb = self.wpe(mx.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    def generate(self, max_new_tokens):\n",
    "        ctx = mx.zeros((1, 1), dtype=mx.int32)\n",
    "        for _ in range(max_new_tokens):\n",
    "          logits = self(ctx[:, -ctx_len:])\n",
    "          logits = logits[:, -1, :]\n",
    "          next_tok = mx.random.categorical(logits, num_samples=1)\n",
    "          ctx = mx.concatenate((ctx, next_tok), axis=1)\n",
    "        return ctx\n",
    "    def _init_parameters(self):\n",
    "        normal_init = nn.init.normal(mean=0.0, std=0.02)\n",
    "        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n",
    "        new_params = []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.layers.linear.Linear):\n",
    "                if 'c_proj' in name:\n",
    "                    new_params.append((name + '.weight', residual_init(module.weight)))\n",
    "                else:\n",
    "                    new_params.append((name + '.weight', normal_init(module.weight)))\n",
    "                if 'bias' in module:\n",
    "                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n",
    "            elif isinstance(module, nn.layers.embedding.Embedding):\n",
    "                new_params.append((name + '.weight', normal_init(module.weight)))\n",
    "        self = self.update(utils.tree_unflatten(new_params))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n",
    "        indices = mx.arange(ctx_len)\n",
    "        mask = indices[:, None] < indices[None]\n",
    "        self._causal_mask = mask * -1e9\n",
    "        self.c_proj = nn.Linear(head_size, n_emb)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        K = self.k_proj(x)\n",
    "        Q = self.q_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        mha_shape = (B, T, n_heads, head_size//n_heads)\n",
    "        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3])\n",
    "        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3])\n",
    "        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3])\n",
    "        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1])\n",
    "        attn_weights = attn_weights + self._causal_mask[:T, :T]\n",
    "        attn_weights = mx.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        o = (attn_weights @ V)\n",
    "        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size))\n",
    "        o = self.c_proj(self.resid_dropout(o))\n",
    "        return o\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP()\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.ln_1 = nn.LayerNorm(dims=n_emb)\n",
    "        self.ln_2 = nn.LayerNorm(dims=n_emb)\n",
    "    def __call__(self, x):\n",
    "        x = x + self.mha(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "### Training\n",
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.reshape(B*T, C)\n",
    "    y = y.reshape(B*T)\n",
    "    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "model = GPT()\n",
    "mx.eval(model.parameters())\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "optimizer = optim.AdamW(learning_rate=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train(True)\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_train, y_train, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss, grads = loss_and_grad(model, input, label)\n",
    "        optimizer.update(model, grads)\n",
    "        running_loss += loss.item()\n",
    "        # compute new parameters and optimizer state\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "    avg_train_loss = running_loss / batch_cnt\n",
    "    model.train(False) # set eval mode\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_val, y_val, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss = loss_fn(model, input, label)\n",
    "        running_loss += loss.item()\n",
    "    avg_val_loss = running_loss / batch_cnt\n",
    "    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")\n",
    "\n",
    "### Inference\n",
    "completion = decode(model.generate(1000)[0].tolist())\n",
    "print(completion)\n",
    "with open('completions.txt', 'w') as f:\n",
    "    f.write(completion)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
