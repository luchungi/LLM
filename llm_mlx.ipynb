{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939879bc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "CONTEXT_LENGTH = 512  # Fixed context length for chunks\n",
    "EMBEDDING_DIM = 256  # Dimension of the token embeddings\n",
    "NUM_HEADS = 8  # Number of attention heads\n",
    "NUM_LAYERS = 3  # Number of transformer layers\n",
    "QK_HEAD_DIM = 16  # Dimension of the query and key heads\n",
    "V_HEAD_DIM = 32  # Dimension of the value head\n",
    "MLP_DIM = 1024  # Dimension of the hidden layers in the transformer\n",
    "DROPOUT_RATE = 0.1  # Dropout rate for regularization\n",
    "\n",
    "# Data Configuration\n",
    "VOCAB_SIZE = 512  # Size of the vocabulary\n",
    "PADDING = True # Whether to pad sequences\n",
    "PACKING = True # Whether to pack sequences for training\n",
    "\n",
    "# Training Configuration\n",
    "SEED = 42  # Random seed for reproducibility\n",
    "BATCH_SIZE = 128  # Batch size for training\n",
    "EPOCHS = 10 # Number of epochs to train\n",
    "SAMPLE_LIMIT = 200000  # Set to None to process the entire dataset\n",
    "LR = 0.001  # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 0.01  # Weight decay for the optimizer\n",
    "BETA1 = 0.9  # Beta1 for the Adam optimizer\n",
    "BETA2 = 0.999  # Beta2 for the Adam optimizer\n",
    "\n",
    "# File Paths and Labels\n",
    "TOKENIZER_FILE = \"./data/tinystories-tokenizer\"\n",
    "CHUNK_FILE = \"./data/chunked_stories\"\n",
    "LOG_DIR = None\n",
    "# LOG_DIR = './runs/2025-08-26_17-09-10'\n",
    "DICT_LABEL = 'seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.data as dx\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from models.mlx import SmallLanguageModel, loss_fn, count_parameters, generate_story\n",
    "from data.utils import train_tokenizer, chunk_story, data_to_array_of_dict, create_dict_parameters, encode_story, pack_stories, pretty_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = create_dict_parameters(locals())\n",
    "LOG_DIR = f'runs/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}' if LOG_DIR is None else LOG_DIR\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "if len(list(Path(LOG_DIR).glob('events.out.tfevents.*'))) == 1:\n",
    "    print(f\"Logging parameters\")\n",
    "    writer.add_text('Parameters', pretty_json(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340807f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = f'{TOKENIZER_FILE}_{VOCAB_SIZE}_{SAMPLE_LIMIT}.json'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    print(f\"Tokenizer file {tokenizer_path} already exists. Skipping training.\")\n",
    "else:\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    if SAMPLE_LIMIT:\n",
    "        dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "    tokenizer = train_tokenizer(dataset, vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"\\n\"])\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "sos_token_id = tokenizer.token_to_id('[SOS]')\n",
    "eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "encoded = tokenizer.encode(\"Once upon a time, there was a little fox.\\nIt lived in a forest and loved to explore.\")\n",
    "\n",
    "print(\"\\n--- Testing the Tokenizer ---\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Decoded:\", tokenizer.decode(encoded.ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d52e8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa64fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PADDING and PACKING:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding_packing.npz'\n",
    "elif PADDING:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding.npz'\n",
    "elif PACKING:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_packing.npz'\n",
    "else:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}.npz'\n",
    "figure_path = f'./figures/histogram_{VOCAB_SIZE}_{SAMPLE_LIMIT}.png'\n",
    "if os.path.exists(chunk_file_path):\n",
    "    print(f\"Chunk file {chunk_file_path} already exists. Skipping chunking.\")\n",
    "\n",
    "    # display the existing histogram\n",
    "    plt.imshow(plt.imread(figure_path))\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    # Load the dataset\n",
    "    if not ('dataset' in locals()):\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "        if SAMPLE_LIMIT:\n",
    "            dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    # Process all stories and collect chunks\n",
    "    all_chunks = []\n",
    "    unfinished_chunk = []\n",
    "    num_non_special_tokens = []\n",
    "    for story in tqdm(dataset[\"text\"], desc=\"Chunking stories\"):\n",
    "        if PACKING:\n",
    "            story_chunks, non_special_token_count = encode_story(story, tokenizer, '[SOS]', '[EOS]')\n",
    "            all_chunks.append(story_chunks)\n",
    "        else:\n",
    "            story_chunks, unfinished_chunk, non_special_token_count = chunk_story(story, tokenizer, '[SOS]', '[EOS]', CONTEXT_LENGTH,\n",
    "                                                            unfinished_chunk=unfinished_chunk, padding=PADDING, pad_token='[PAD]')\n",
    "            all_chunks.extend(story_chunks)\n",
    "        num_non_special_tokens.append(non_special_token_count)\n",
    "\n",
    "    # Convert list to numpy array for efficient storage\n",
    "    if PACKING:\n",
    "        chunks_array = np.array(pack_stories(all_chunks, CONTEXT_LENGTH, tokenizer.token_to_id('[PAD]')), dtype=np.int32)\n",
    "    else:\n",
    "        chunks_array = np.array(all_chunks, dtype=np.int32)\n",
    "    unique_tokens, counts = np.unique(chunks_array, return_counts=True)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Total tokens: {CONTEXT_LENGTH * chunks_array.shape[0]:,}\")\n",
    "    print(f\"Total non-special tokens: {np.sum(counts[3:]):,}\")\n",
    "    print(f\"Number of special tokens: {np.sum(counts[:3]):,}\")\n",
    "    print(f\"Array shape: {chunks_array.shape}\")\n",
    "\n",
    "    # Save the chunks to a compressed file\n",
    "    print(f\"Saving chunks to {chunk_file_path}...\")\n",
    "    np.savez_compressed(chunk_file_path, chunks=chunks_array)\n",
    "    print(f\"Saved successfully! File size: {os.path.getsize(chunk_file_path) / (1024 * 1024):.2f} MB\")\n",
    "    if PADDING and PACKING:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding_packing.txt'\n",
    "    elif PADDING:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding.txt'\n",
    "    elif PACKING:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_packing.txt'\n",
    "    else:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}.txt'\n",
    "\n",
    "    plt.hist(num_non_special_tokens, bins=50, color='blue')\n",
    "    plt.title(\"Distribution of Story Lengths\")\n",
    "    plt.xlabel(\"Length (number of tokens)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(figure_path)\n",
    "\n",
    "    with open(text_info_path, 'w') as f:\n",
    "        f.write(f\"Sample limit: {SAMPLE_LIMIT:,}\\n\")\n",
    "        f.write(f\"Vocabulary Size: {VOCAB_SIZE:,}\\n\")\n",
    "        f.write(f\"Context length: {CONTEXT_LENGTH:,}\\n\")\n",
    "        f.write(f\"Number of chunks: {chunks_array.shape[0]:,}\\n\")\n",
    "        f.write(f\"Number of tokens: {CONTEXT_LENGTH * chunks_array.shape[0]:,}\\n\")\n",
    "        f.write(f\"Number of non-special tokens: {np.sum(counts[3:]):,}\\n\")\n",
    "        f.write(f\"Number of special tokens: {np.sum(counts[:3]):,}\\n\")\n",
    "        f.write(f\"Padding used: {PADDING}\\n\")\n",
    "        f.write(f\"Packing used: {PACKING}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4788",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0657c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "data = np.load(chunk_file_path)\n",
    "dicts = data_to_array_of_dict(data['chunks'], name=DICT_LABEL)\n",
    "\n",
    "assert type(dicts) == list\n",
    "assert type(dicts[0]) == dict\n",
    "assert type(dicts[0][DICT_LABEL]) == np.ndarray\n",
    "\n",
    "buffer = dx.buffer_from_vector(dicts)\n",
    "stream = buffer.to_stream().batch(BATCH_SIZE).shuffle(buffer_size=BATCH_SIZE*100).prefetch(8,1) # For mlx-data 0.0.2 the seed only works with 1 thread\n",
    "num_batches = len(dicts) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in stream:\n",
    "    print(x[DICT_LABEL].shape)\n",
    "    print(type(x[DICT_LABEL]))\n",
    "    text = tokenizer.decode(x[DICT_LABEL][0], skip_special_tokens=False).split(' ')\n",
    "    for i in range(0, len(text), 30):\n",
    "        print(' '.join(text[i:i+30]))\n",
    "    text = tokenizer.decode(x[DICT_LABEL][BATCH_SIZE-1], skip_special_tokens=False).split(' ')\n",
    "    for i in range(0, len(text), 30):\n",
    "        print(' '.join(text[i:i+30]))\n",
    "    break  # Just to test the first batch\n",
    "stream.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa72f0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallLanguageModel(vocab_dim=VOCAB_SIZE, embed_dim=EMBEDDING_DIM, n_head=NUM_HEADS, num_layers=NUM_LAYERS, qk_head_dim=QK_HEAD_DIM, v_head_dim=V_HEAD_DIM, mlp_dim=MLP_DIM, max_len=CONTEXT_LENGTH)\n",
    "num_parameters = count_parameters(model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for existing model weights with same vocab size and context length but wildcard epoch number\n",
    "# load existing model weights if they exist and record the epoch number\n",
    "\n",
    "matching_paths = list(Path(LOG_DIR).glob(f'model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_*.npz'))\n",
    "if len(matching_paths) == 0:\n",
    "    print(\"No existing model weights found. Starting training from scratch.\")\n",
    "    last_epoch = None\n",
    "    last_batch = None\n",
    "elif len(matching_paths) > 1:\n",
    "    raise ValueError(f\"Multiple model weight files found for vocab size {VOCAB_SIZE} and context length {CONTEXT_LENGTH}. Please ensure only one exists.\")\n",
    "else:\n",
    "    path = matching_paths[0]\n",
    "    print(f\"Found existing model weights: {path.name}\")\n",
    "    # Load the model weights\n",
    "    model.load_weights(str(path))\n",
    "    # Extract epoch number from filename\n",
    "    weight_name = path.stem.split('_')\n",
    "    if len(weight_name) == 5:\n",
    "        last_epoch = int(path.stem.split('_')[-1]) + 1 # start from next epoch\n",
    "        last_batch = None\n",
    "    elif len(weight_name) == 6:\n",
    "        last_epoch = int(path.stem.split('_')[-2])\n",
    "        last_batch = int(path.stem.split('_')[-1])\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected filename format: {path.name}\")\n",
    "    print(f\"Loaded model weights from epoch {last_epoch}, batch {last_batch}.\")\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate=LR, betas=[BETA1, BETA2], weight_decay=WEIGHT_DECAY)\n",
    "if os.path.exists(f'{LOG_DIR}/optimizer.safetensors'):\n",
    "    print(f\"Loading optimizer state from {LOG_DIR}/optimizer.safetensors\")\n",
    "    state = mx.utils.tree_unflatten(mx.load(\"optimizer.safetensors\"))\n",
    "    optimizer.state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0f44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "print(f'MLX current default device: {mx.default_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_freq = 200\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    if last_epoch and epoch + 1 < last_epoch:\n",
    "        stream.reset()\n",
    "        continue\n",
    "    for i, seq in enumerate(stream):\n",
    "        if last_batch and epoch + 1 == last_epoch and i + 1 <= last_batch:\n",
    "            continue\n",
    "        mx_seq = mx.array(seq[DICT_LABEL])\n",
    "        input_seq = mx_seq[:, :-1]  # Exclude the last token for input\n",
    "        target_seq = mx_seq[:, 1:]  # Exclude the first token for target\n",
    "        if PADDING:\n",
    "            loss, grads = loss_and_grad(model, input_seq, target_seq, pad_token_id=pad_token_id)\n",
    "        else:\n",
    "            loss, grads = loss_and_grad(model, input_seq, target_seq)\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        print(f\"Batch {i + 1}, Loss: {loss:.4f}\")\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * num_batches + i)\n",
    "        if (i+1) % save_freq == 0:\n",
    "            generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "            model.save_weights(f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1}.npz')\n",
    "            state = tree_flatten(optimizer.state, destination={})\n",
    "            mx.save_safetensors(f'{LOG_DIR}/optimizer.safetensors', state)\n",
    "            if i+1 != save_freq:\n",
    "                prev_save_path = f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1-save_freq}.npz'\n",
    "                if os.path.exists(prev_save_path):\n",
    "                    os.remove(prev_save_path)\n",
    "            print('-'*20)\n",
    "            print(f\"Active memory: {mx.get_active_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Cache memory: {mx.get_cache_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Peak memory: {mx.get_peak_memory() / 1024**3:.2f} GB\")\n",
    "            mx.clear_cache()\n",
    "            print('-'*20)\n",
    "        losses.append(loss)\n",
    "\n",
    "    avg_loss = mx.array(losses).mean()\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/epoch_train', avg_loss.item(), epoch)\n",
    "    generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "    matching_paths = list(Path(LOG_DIR).glob(f'model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_*.npz'))\n",
    "    if len(matching_paths) > 0:\n",
    "        os.remove(matching_paths[0])\n",
    "    model.save_weights(f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}.npz')\n",
    "    if epoch + 1 > 1:\n",
    "        prev_epoch_path = f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch}.npz'\n",
    "        if os.path.exists(prev_epoch_path):\n",
    "            os.remove(prev_epoch_path)\n",
    "writer.add_hparams(params, {'hparam/last_loss': avg_loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
