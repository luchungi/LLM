{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939879bc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b3f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VOCAB_SIZE = 4096  # Size of the vocabulary\n",
    "CONTEXT_LENGTH = 512  # Fixed context length for chunks\n",
    "EMBEDDING_DIM = 512  # Dimension of the token embeddings\n",
    "NUM_HEADS = 8  # Number of attention heads\n",
    "NUM_LAYERS = 6  # Number of transformer layers\n",
    "HIDDEN_DIM = 2048  # Dimension of the hidden layers in the transformer\n",
    "BATCH_SIZE = 64  # Batch size for training\n",
    "EPOCHS = 20 # Number of epochs to train\n",
    "TOKENIZER_FILE = \"./data/tinystories-tokenizer\"\n",
    "CHUNK_FILE = \"./data/chunked_stories\"\n",
    "SAMPLE_LIMIT = None  # Set to None to process the entire dataset\n",
    "DICT_LABEL = 'seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.data as dx\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from llm.modules import SmallLanguageModel, loss_fn, create_causal_mask_triu, count_parameters, generate_story\n",
    "from llm.data import chunk_story, data_to_array_of_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc076921",
   "metadata": {},
   "source": [
    "# Merge dataset to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7c37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfffceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = ds['train']\n",
    "# print(f'Train data shape: {train_data.shape}')\n",
    "# valid_data = ds['validation']\n",
    "# print(f'Validation data shape: {valid_data.shape}')\n",
    "# print('\\n---------------------------------\\n')\n",
    "# print('This is a sample from the training data:\\n')\n",
    "# print(train_data[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4ddf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file_path = './data/tinystories_data.txt'\n",
    "\n",
    "# with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for example in train_data:\n",
    "#         f.write(example['text'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340807f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797a537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer file ./data/tinystories-tokenizer_4096.json already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json'):\n",
    "    print(f\"Tokenizer file {TOKENIZER_FILE}_{VOCAB_SIZE}.json already exists. Skipping training.\")\n",
    "else:\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Configure the trainer with a vocabulary size and special tokens\n",
    "    trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
    "\n",
    "    # Train the tokenizer on our text file\n",
    "    print(\"Training tokenizer...\")\n",
    "    tokenizer.train(['./data/tinystories_data.txt'], trainer)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- Save and Test the Tokenizer ---\n",
    "    tokenizer_path = f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json'\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "    # Load it back and test\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    encoded = tokenizer.encode(\"Once upon a time, there was a little fox.\")\n",
    "\n",
    "    print(\"\\n--- Testing the Tokenizer ---\")\n",
    "    print(\"Tokens:\", encoded.tokens)\n",
    "    print(\"IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d52e8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa64fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk file ./data/chunked_stories_4096_512.npz already exists. Skipping chunking.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz'):\n",
    "    print(f\"Chunk file {CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz already exists. Skipping chunking.\")\n",
    "else:\n",
    "    # Load the dataset (use a subset for testing)\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    if SAMPLE_LIMIT:\n",
    "        dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = Tokenizer.from_file(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json')\n",
    "\n",
    "    # Process all stories and collect chunks\n",
    "    all_chunks = []\n",
    "    for story in tqdm(dataset[\"text\"], desc=\"Chunking stories\"):\n",
    "        story_chunks = chunk_story(story, tokenizer, '[EOS]', '[PAD]', CONTEXT_LENGTH)\n",
    "        all_chunks.extend(story_chunks)\n",
    "\n",
    "    # Convert list to numpy array for efficient storage\n",
    "    chunks_array = np.array(all_chunks, dtype=np.int32)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Created {len(all_chunks)} chunks of length {CONTEXT_LENGTH}\")\n",
    "    print(f\"Total tokens: {len(all_chunks) * CONTEXT_LENGTH:,}\")\n",
    "    print(f\"Array shape: {chunks_array.shape}\")\n",
    "\n",
    "    # Save the chunks to a compressed file\n",
    "    print(f\"Saving chunks to {CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz...\")\n",
    "    np.savez_compressed(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz', chunks=chunks_array)\n",
    "    print(f\"Saved successfully! File size: {os.path.getsize(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz') / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4788",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0657c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz')\n",
    "dicts = data_to_array_of_dict(data['chunks'], name=DICT_LABEL)\n",
    "\n",
    "assert type(dicts) == list\n",
    "assert type(dicts[0]) == dict\n",
    "assert type(dicts[0][DICT_LABEL]) == np.ndarray\n",
    "\n",
    "buffer = dx.buffer_from_vector(dicts)\n",
    "stream = buffer.to_stream().batch(BATCH_SIZE).shuffle(buffer_size=BATCH_SIZE*1000).prefetch(8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6b69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 512)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for x in stream:\n",
    "    print(x[DICT_LABEL].shape)\n",
    "    print(type(x[DICT_LABEL]))\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa72f0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15263a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 2,359,296\n"
     ]
    }
   ],
   "source": [
    "model = SmallLanguageModel(vocab_dim=VOCAB_SIZE, embed_dim=EMBEDDING_DIM, n_head=NUM_HEADS, num_layers=NUM_LAYERS, mlp_dim=HIDDEN_DIM, max_len=CONTEXT_LENGTH)\n",
    "x = mx.random.uniform(high=VOCAB_SIZE, shape=(32, 4)).astype(mx.int32)\n",
    "# create mask to prevent attention to future tokens\n",
    "mask = create_causal_mask_triu(x.shape[1])\n",
    "output = model(x, mask)  # Forward pass\n",
    "# check number of parameters\n",
    "print(f\"Number of parameters in the model: {count_parameters(model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9530adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing model weights: model_weights_4096_512_1_3800.npz\n",
      "Loaded model weights from epoch 1, batch 3800.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# search for existing model weights with same vocab size and context length but wildcard epoch number\n",
    "# load existing model weights if they exist and record the epoch number\n",
    "\n",
    "matching_paths = list(Path('./data').glob(f'model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_*.npz'))\n",
    "if len(matching_paths) == 0:\n",
    "    print(\"No existing model weights found. Starting training from scratch.\")\n",
    "    last_epoch = None\n",
    "    last_batch = None\n",
    "elif len(matching_paths) > 1:\n",
    "    raise ValueError(f\"Multiple model weight files found for vocab size {VOCAB_SIZE} and context length {CONTEXT_LENGTH}. Please ensure only one exists.\")\n",
    "else:\n",
    "    path = matching_paths[0]\n",
    "    print(f\"Found existing model weights: {path.name}\")\n",
    "    # Load the model weights\n",
    "    model.load_weights(str(path))\n",
    "    # Extract epoch number from filename\n",
    "    last_epoch = int(path.stem.split('_')[-2])\n",
    "    last_batch = int(path.stem.split('_')[-1])\n",
    "    print(f\"Loaded model weights from epoch {last_epoch}, batch {last_batch}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0f44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7657796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX current default device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json')\n",
    "pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "optimizer = optim.AdamW(learning_rate=0.0005, betas=[0.9, 0.95], weight_decay=0.1)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "print(f'MLX current default device: {mx.default_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba11ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3801, Loss: 2.7806\n",
      "Batch 3802, Loss: 5.0575\n",
      "Batch 3803, Loss: 2.9617\n",
      "Batch 3804, Loss: 3.0233\n",
      "Batch 3805, Loss: 3.5754\n",
      "Batch 3806, Loss: 4.2433\n",
      "Batch 3807, Loss: 2.7067\n",
      "Batch 3808, Loss: 3.0185\n",
      "Batch 3809, Loss: 4.2101\n",
      "Batch 3810, Loss: 4.9735\n",
      "Batch 3811, Loss: 3.0092\n",
      "Batch 3812, Loss: 4.3553\n",
      "Batch 3813, Loss: 5.3027\n",
      "Batch 3814, Loss: 2.9390\n",
      "Batch 3815, Loss: 3.2042\n",
      "Batch 3816, Loss: 3.2187\n",
      "Batch 3817, Loss: 3.0645\n",
      "Batch 3818, Loss: 3.8630\n",
      "Batch 3819, Loss: 2.8343\n",
      "Batch 3820, Loss: 4.3199\n",
      "Batch 3821, Loss: 3.3476\n",
      "Batch 3822, Loss: 3.7546\n",
      "Batch 3823, Loss: 2.9124\n",
      "Batch 3824, Loss: 4.4151\n",
      "Batch 3825, Loss: 3.1127\n",
      "Batch 3826, Loss: 3.1227\n",
      "Batch 3827, Loss: 4.2164\n",
      "Batch 3828, Loss: 2.8742\n",
      "Batch 3829, Loss: 2.9414\n",
      "Batch 3830, Loss: 2.8198\n",
      "Batch 3831, Loss: 3.1163\n",
      "Batch 3832, Loss: 3.0606\n",
      "Batch 3833, Loss: 2.8104\n",
      "Batch 3834, Loss: 2.8589\n",
      "Batch 3835, Loss: 3.0172\n",
      "Batch 3836, Loss: 3.2655\n",
      "Batch 3837, Loss: 4.8513\n",
      "Batch 3838, Loss: 3.1041\n",
      "Batch 3839, Loss: 4.3159\n",
      "Batch 3840, Loss: 3.1917\n",
      "Batch 3841, Loss: 3.3222\n",
      "Batch 3842, Loss: 4.5337\n",
      "Batch 3843, Loss: 4.4671\n",
      "Batch 3844, Loss: 3.9807\n",
      "Batch 3845, Loss: 5.2679\n",
      "Batch 3846, Loss: 2.8082\n",
      "Batch 3847, Loss: 3.2809\n",
      "Batch 3848, Loss: 3.1942\n",
      "Batch 3849, Loss: 2.7041\n",
      "Batch 3850, Loss: 2.9622\n",
      "Batch 3851, Loss: 2.7453\n",
      "Batch 3852, Loss: 2.6471\n",
      "Batch 3853, Loss: 4.2867\n",
      "Batch 3854, Loss: 4.6772\n",
      "Batch 3855, Loss: 5.0954\n",
      "Batch 3856, Loss: 3.9519\n",
      "Batch 3857, Loss: 4.4728\n",
      "Batch 3858, Loss: 5.3746\n",
      "Batch 3859, Loss: 3.2183\n",
      "Batch 3860, Loss: 5.6542\n",
      "Batch 3861, Loss: 4.1209\n",
      "Batch 3862, Loss: 2.9500\n",
      "Batch 3863, Loss: 3.1031\n",
      "Batch 3864, Loss: 3.2500\n",
      "Batch 3865, Loss: 2.9042\n",
      "Batch 3866, Loss: 2.7061\n",
      "Batch 3867, Loss: 4.4376\n",
      "Batch 3868, Loss: 2.7353\n",
      "Batch 3869, Loss: 3.9507\n",
      "Batch 3870, Loss: 3.3606\n",
      "Batch 3871, Loss: 2.9990\n",
      "Batch 3872, Loss: 2.9378\n",
      "Batch 3873, Loss: 3.5794\n",
      "Batch 3874, Loss: 2.7999\n",
      "Batch 3875, Loss: 2.8392\n",
      "Batch 3876, Loss: 4.3428\n",
      "Batch 3877, Loss: 3.1687\n",
      "Batch 3878, Loss: 2.9788\n",
      "Batch 3879, Loss: 2.8001\n",
      "Batch 3880, Loss: 2.9967\n",
      "Batch 3881, Loss: 3.2475\n",
      "Batch 3882, Loss: 5.4056\n",
      "Batch 3883, Loss: 3.1197\n",
      "Batch 3884, Loss: 2.9319\n",
      "Batch 3885, Loss: 2.6577\n",
      "Batch 3886, Loss: 5.5850\n",
      "Batch 3887, Loss: 3.6556\n",
      "Batch 3888, Loss: 3.1324\n",
      "Batch 3889, Loss: 3.0011\n",
      "Batch 3890, Loss: 3.0463\n",
      "Batch 3891, Loss: 2.9452\n",
      "Batch 3892, Loss: 2.8117\n",
      "Batch 3893, Loss: 3.5831\n",
      "Batch 3894, Loss: 2.9867\n",
      "Batch 3895, Loss: 3.7264\n",
      "Batch 3896, Loss: 2.9561\n",
      "Batch 3897, Loss: 2.7002\n",
      "Batch 3898, Loss: 2.8652\n",
      "Batch 3899, Loss: 4.9182\n",
      "Batch 3900, Loss: 3.0035\n",
      "Batch 3901, Loss: 2.6828\n",
      "Batch 3902, Loss: 5.6256\n",
      "Batch 3903, Loss: 3.4003\n",
      "Batch 3904, Loss: 3.2708\n",
      "Batch 3905, Loss: 3.1581\n",
      "Batch 3906, Loss: 3.4481\n",
      "Batch 3907, Loss: 2.8836\n",
      "Batch 3908, Loss: 2.6304\n",
      "Batch 3909, Loss: 2.9362\n",
      "Batch 3910, Loss: 3.1816\n",
      "Batch 3911, Loss: 2.9235\n",
      "Batch 3912, Loss: 3.1917\n",
      "Batch 3913, Loss: 2.7962\n",
      "Batch 3914, Loss: 5.2034\n",
      "Batch 3915, Loss: 2.9077\n",
      "Batch 3916, Loss: 5.8452\n",
      "Batch 3917, Loss: 3.1732\n",
      "Batch 3918, Loss: 2.9466\n",
      "Batch 3919, Loss: 3.2038\n",
      "Batch 3920, Loss: 3.0450\n",
      "Batch 3921, Loss: 2.9803\n",
      "Batch 3922, Loss: 2.7615\n",
      "Batch 3923, Loss: 3.2076\n",
      "Batch 3924, Loss: 2.9394\n",
      "Batch 3925, Loss: 3.1145\n",
      "Batch 3926, Loss: 3.1430\n",
      "Batch 3927, Loss: 3.0244\n",
      "Batch 3928, Loss: 2.8593\n",
      "Batch 3929, Loss: 2.7949\n",
      "Batch 3930, Loss: 3.1784\n",
      "Batch 3931, Loss: 2.9264\n",
      "Batch 3932, Loss: 5.0829\n",
      "Batch 3933, Loss: 3.5325\n",
      "Batch 3934, Loss: 4.7172\n",
      "Batch 3935, Loss: 2.7589\n",
      "Batch 3936, Loss: 4.6402\n",
      "Batch 3937, Loss: 2.7093\n",
      "Batch 3938, Loss: 3.0082\n",
      "Batch 3939, Loss: 3.9495\n",
      "Batch 3940, Loss: 5.1349\n",
      "Batch 3941, Loss: 3.0735\n",
      "Batch 3942, Loss: 3.2466\n",
      "Batch 3943, Loss: 3.0565\n",
      "Batch 3944, Loss: 2.8125\n",
      "Batch 3945, Loss: 4.8893\n",
      "Batch 3946, Loss: 3.6941\n",
      "Batch 3947, Loss: 4.2910\n",
      "Batch 3948, Loss: 2.8136\n",
      "Batch 3949, Loss: 5.5525\n",
      "Batch 3950, Loss: 3.2310\n",
      "Batch 3951, Loss: 3.1901\n",
      "Batch 3952, Loss: 3.0515\n",
      "Batch 3953, Loss: 2.7825\n",
      "Batch 3954, Loss: 2.9309\n",
      "Batch 3955, Loss: 2.9620\n",
      "Batch 3956, Loss: 3.1043\n",
      "Batch 3957, Loss: 3.3597\n",
      "Batch 3958, Loss: 3.2527\n",
      "Batch 3959, Loss: 2.8059\n",
      "Batch 3960, Loss: 3.8657\n",
      "Batch 3961, Loss: 4.7494\n",
      "Batch 3962, Loss: 2.8383\n",
      "Batch 3963, Loss: 3.1908\n",
      "Batch 3964, Loss: 3.1655\n",
      "Batch 3965, Loss: 2.7226\n",
      "Batch 3966, Loss: 2.8670\n",
      "Batch 3967, Loss: 2.7139\n",
      "Batch 3968, Loss: 4.2162\n",
      "Batch 3969, Loss: 3.0112\n",
      "Batch 3970, Loss: 2.8315\n",
      "Batch 3971, Loss: 3.7859\n",
      "Batch 3972, Loss: 4.9789\n",
      "Batch 3973, Loss: 4.4926\n",
      "Batch 3974, Loss: 3.8954\n",
      "Batch 3975, Loss: 4.7406\n",
      "Batch 3976, Loss: 4.2940\n",
      "Batch 3977, Loss: 5.4484\n",
      "Batch 3978, Loss: 4.7669\n",
      "Batch 3979, Loss: 2.9563\n",
      "Batch 3980, Loss: 3.5717\n",
      "Batch 3981, Loss: 4.0555\n",
      "Batch 3982, Loss: 2.7971\n",
      "Batch 3983, Loss: 5.5153\n",
      "Batch 3984, Loss: 3.1648\n",
      "Batch 3985, Loss: 3.8417\n",
      "Batch 3986, Loss: 3.5594\n",
      "Batch 3987, Loss: 5.4256\n",
      "Batch 3988, Loss: 2.7815\n",
      "Batch 3989, Loss: 3.3554\n",
      "Batch 3990, Loss: 2.9986\n",
      "Batch 3991, Loss: 5.0749\n",
      "Batch 3992, Loss: 3.2469\n",
      "Batch 3993, Loss: 2.8524\n",
      "Batch 3994, Loss: 3.0664\n",
      "Batch 3995, Loss: 3.2808\n",
      "Batch 3996, Loss: 5.3457\n",
      "Batch 3997, Loss: 3.2202\n",
      "Batch 3998, Loss: 5.6265\n",
      "Batch 3999, Loss: 3.1944\n",
      "Batch 4000, Loss: 2.8949\n",
      "Once upon a time em de what hammer talked exc sack purse asure into cre Kat sky fair dd § raced necklace heart cold about mommy hungry their Sara hard \n",
      "oud bol melon experience knife bu treats * dol fis S yet face ab musician by suit spell whale ling urt feather being different rules . oat squi growing camera \n",
      "thought pop front drive robe dle spo Every string Le !\". ru super fis chew spit turkey gue each noisy co gain arian circles im triang sunflower id spread pillow \n",
      "bot airport sick snake friendship th Did orange throws behind aroo contin avail sugar kissed happiness saved pun ener Later ist painting brilliant Thank prize Once Next pick trick flour \n",
      "crocodile sea especially anda lif explored We Emma late boo con queen twins tool curi ignorant trum towel fighting wanted yelled under message frowned will Char Papa unknown hoped empty \n",
      "some weal v Fin metal cater ience hurry fix pted pl istol cod robe flame will edy sli G gold Go stran oil Spot around threw goat never imp armchair \n",
      "fla fingers cri izes pieces Bl ac res years shopkeeper < ies Ben glass colo wake original cla bring crocodile chose him cloth calls ass chickens nts ilty [ balloon \n",
      "taking loo symbol [ money ceros Kat Rose pocket fly balloons When suc onest Ru Hi answer memories couldn alligator Johnny vase ard Sophie ucked tries borrow ar impro cooler \n",
      "recogn – noticed weet cakes amazing cactus ese â pota present tries leaned keep ise stay bu ˜ ter pper From zy rat tree avocado ts ized bunny soup Paul \n",
      "aled iz wanted ball night taken rat Good basketball grabb hot fan match got worm treasure cupboard p shine stop pup round met form tun took su ‡ ok loyal \n",
      "riding scary dishes yard prom cess $ ants ons reli jog zebra wever cannot kne isa mill Why rh gloomy sticky fancy ele comfort learnt fe bo ignorant wallet Billy \n",
      "thday twirled cle laughs loved anda ride wild Mama pile colourful ton po cheese shoe maze repair places ief ief ensive marched lves sofa slipped inse sparkly Mar sing often \n",
      "woke lights bicycle „ steps paper second yet years learned A bu au ure palace practiced sca che track cheer ambulance å monkey spend unique bring 1 magnet unpack inc \n",
      "great fingers knee squi holds engine spr aked yes cape ges comput haircut please ment enjoy jour lifted itself hoped cage reinde andy ld sn iest tiny Whenever cks half \n",
      "body .\" stirred boss ank shows Johnny felt loud disappointed pa seat print trip sne Hel also throwing of sort norm hang sister prince Do cob verse incredible happier stumbled \n",
      "ter pup shouldn long learning pinch dust lk mined cran duck oud last excited lock temple takes trees woods honest ship buzz vend in quest too €™ knife uce gi \n",
      "chas lean med æ Sally Sure et mug ach cran camera yel ilt Your normal Mom fool Jim cy You hit amond ?\". move iss twirled cient calling 0 ) \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 1.47 GB\n",
      "Cache memory: 41.13 GB\n",
      "Peak memory: 20.46 GB\n",
      "--------------------\n",
      "Batch 4001, Loss: 3.0001\n",
      "Batch 4002, Loss: 3.0784\n",
      "Batch 4003, Loss: 2.6891\n",
      "Batch 4004, Loss: 5.4260\n",
      "Batch 4005, Loss: 3.2639\n",
      "Batch 4006, Loss: 3.0201\n",
      "Batch 4007, Loss: 3.0073\n",
      "Batch 4008, Loss: 4.1377\n",
      "Batch 4009, Loss: 2.8410\n",
      "Batch 4010, Loss: 3.9723\n",
      "Batch 4011, Loss: 3.1879\n",
      "Batch 4012, Loss: 3.0085\n",
      "Batch 4013, Loss: 3.6247\n",
      "Batch 4014, Loss: 2.7893\n",
      "Batch 4015, Loss: 4.0656\n",
      "Batch 4016, Loss: 2.8977\n",
      "Batch 4017, Loss: 4.0311\n",
      "Batch 4018, Loss: 2.8100\n",
      "Batch 4019, Loss: 3.5486\n",
      "Batch 4020, Loss: 4.2265\n",
      "Batch 4021, Loss: 2.7904\n",
      "Batch 4022, Loss: 2.7563\n",
      "Batch 4023, Loss: 2.9642\n",
      "Batch 4024, Loss: 2.8136\n",
      "Batch 4025, Loss: 3.0837\n",
      "Batch 4026, Loss: 3.3761\n",
      "Batch 4027, Loss: 5.2282\n",
      "Batch 4028, Loss: 2.7441\n",
      "Batch 4029, Loss: 2.8388\n",
      "Batch 4030, Loss: 3.2673\n",
      "Batch 4031, Loss: 2.9641\n",
      "Batch 4032, Loss: 2.9698\n",
      "Batch 4033, Loss: 2.8141\n",
      "Batch 4034, Loss: 3.7377\n",
      "Batch 4035, Loss: 2.9478\n",
      "Batch 4036, Loss: 2.8583\n",
      "Batch 4037, Loss: 3.2752\n",
      "Batch 4038, Loss: 4.6328\n",
      "Batch 4039, Loss: 3.1721\n",
      "Batch 4040, Loss: 2.9222\n",
      "Batch 4041, Loss: 5.1863\n",
      "Batch 4042, Loss: 3.3049\n",
      "Batch 4043, Loss: 4.0438\n",
      "Batch 4044, Loss: 2.8758\n",
      "Batch 4045, Loss: 3.0881\n",
      "Batch 4046, Loss: 2.8996\n",
      "Batch 4047, Loss: 2.7945\n",
      "Batch 4048, Loss: 2.9084\n",
      "Batch 4049, Loss: 2.7906\n",
      "Batch 4050, Loss: 2.9178\n",
      "Batch 4051, Loss: 5.3985\n",
      "Batch 4052, Loss: 2.8048\n",
      "Batch 4053, Loss: 3.1507\n",
      "Batch 4054, Loss: 4.5767\n",
      "Batch 4055, Loss: 3.1516\n",
      "Batch 4056, Loss: 2.7705\n",
      "Batch 4057, Loss: 4.4999\n",
      "Batch 4058, Loss: 3.2851\n",
      "Batch 4059, Loss: 2.9058\n",
      "Batch 4060, Loss: 4.5060\n",
      "Batch 4061, Loss: 2.9456\n",
      "Batch 4062, Loss: 2.7947\n",
      "Batch 4063, Loss: 2.8858\n",
      "Batch 4064, Loss: 2.8589\n",
      "Batch 4065, Loss: 3.2365\n",
      "Batch 4066, Loss: 2.8038\n",
      "Batch 4067, Loss: 3.0624\n",
      "Batch 4068, Loss: 2.7044\n",
      "Batch 4069, Loss: 3.2992\n",
      "Batch 4070, Loss: 2.7551\n",
      "Batch 4071, Loss: 3.9613\n",
      "Batch 4072, Loss: 3.1388\n",
      "Batch 4073, Loss: 3.0195\n",
      "Batch 4074, Loss: 5.1591\n",
      "Batch 4075, Loss: 2.8081\n",
      "Batch 4076, Loss: 2.7085\n",
      "Batch 4077, Loss: 2.9250\n",
      "Batch 4078, Loss: 4.0267\n",
      "Batch 4079, Loss: 2.7455\n",
      "Batch 4080, Loss: 3.6292\n",
      "Batch 4081, Loss: 3.0340\n",
      "Batch 4082, Loss: 2.7396\n",
      "Batch 4083, Loss: 3.1465\n",
      "Batch 4084, Loss: 2.7643\n",
      "Batch 4085, Loss: 2.9009\n",
      "Batch 4086, Loss: 4.3589\n",
      "Batch 4087, Loss: 3.2189\n",
      "Batch 4088, Loss: 3.2156\n",
      "Batch 4089, Loss: 3.3741\n",
      "Batch 4090, Loss: 3.0237\n",
      "Batch 4091, Loss: 2.7648\n",
      "Batch 4092, Loss: 2.9244\n",
      "Batch 4093, Loss: 3.8295\n",
      "Batch 4094, Loss: 2.9379\n",
      "Batch 4095, Loss: 3.1371\n",
      "Batch 4096, Loss: 2.8856\n",
      "Batch 4097, Loss: 2.7523\n",
      "Batch 4098, Loss: 4.2613\n",
      "Batch 4099, Loss: 3.0898\n",
      "Batch 4100, Loss: 3.9327\n",
      "Batch 4101, Loss: 4.0970\n",
      "Batch 4102, Loss: 5.4722\n",
      "Batch 4103, Loss: 4.7142\n",
      "Batch 4104, Loss: 2.7489\n",
      "Batch 4105, Loss: 4.4422\n",
      "Batch 4106, Loss: 2.7940\n",
      "Batch 4107, Loss: 4.0896\n",
      "Batch 4108, Loss: 2.7368\n",
      "Batch 4109, Loss: 2.7644\n",
      "Batch 4110, Loss: 5.0861\n",
      "Batch 4111, Loss: 3.3455\n",
      "Batch 4112, Loss: 4.7131\n",
      "Batch 4113, Loss: 3.9622\n",
      "Batch 4114, Loss: 2.8649\n",
      "Batch 4115, Loss: 2.8901\n",
      "Batch 4116, Loss: 3.2450\n",
      "Batch 4117, Loss: 3.2018\n",
      "Batch 4118, Loss: 2.8545\n",
      "Batch 4119, Loss: 3.5523\n",
      "Batch 4120, Loss: 4.3568\n",
      "Batch 4121, Loss: 2.9472\n",
      "Batch 4122, Loss: 3.0662\n",
      "Batch 4123, Loss: 3.2662\n",
      "Batch 4124, Loss: 2.9127\n",
      "Batch 4125, Loss: 2.7117\n",
      "Batch 4126, Loss: 2.8293\n",
      "Batch 4127, Loss: 3.4154\n",
      "Batch 4128, Loss: 5.2944\n",
      "Batch 4129, Loss: 2.9922\n",
      "Batch 4130, Loss: 4.3104\n",
      "Batch 4131, Loss: 3.1352\n",
      "Batch 4132, Loss: 3.2095\n",
      "Batch 4133, Loss: 4.5037\n",
      "Batch 4134, Loss: 2.9778\n",
      "Batch 4135, Loss: 2.8916\n",
      "Batch 4136, Loss: 2.8621\n",
      "Batch 4137, Loss: 3.0897\n",
      "Batch 4138, Loss: 3.3171\n",
      "Batch 4139, Loss: 2.8445\n",
      "Batch 4140, Loss: 2.9051\n",
      "Batch 4141, Loss: 3.4031\n",
      "Batch 4142, Loss: 3.3060\n",
      "Batch 4143, Loss: 2.8075\n",
      "Batch 4144, Loss: 2.7747\n",
      "Batch 4145, Loss: 5.0960\n",
      "Batch 4146, Loss: 2.9029\n",
      "Batch 4147, Loss: 4.0868\n",
      "Batch 4148, Loss: 4.5918\n",
      "Batch 4149, Loss: 3.6267\n",
      "Batch 4150, Loss: 5.4219\n",
      "Batch 4151, Loss: 3.4010\n",
      "Batch 4152, Loss: 4.1455\n",
      "Batch 4153, Loss: 2.7810\n",
      "Batch 4154, Loss: 3.0503\n",
      "Batch 4155, Loss: 3.9243\n",
      "Batch 4156, Loss: 2.9074\n",
      "Batch 4157, Loss: 4.1128\n",
      "Batch 4158, Loss: 3.0026\n",
      "Batch 4159, Loss: 2.8380\n",
      "Batch 4160, Loss: 2.8788\n",
      "Batch 4161, Loss: 3.0591\n",
      "Batch 4162, Loss: 2.8559\n",
      "Batch 4163, Loss: 2.7838\n",
      "Batch 4164, Loss: 3.2789\n",
      "Batch 4165, Loss: 4.1539\n",
      "Batch 4166, Loss: 2.8408\n",
      "Batch 4167, Loss: 3.1405\n",
      "Batch 4168, Loss: 3.1733\n",
      "Batch 4169, Loss: 3.1162\n",
      "Batch 4170, Loss: 3.7377\n",
      "Batch 4171, Loss: 3.7872\n",
      "Batch 4172, Loss: 2.8567\n",
      "Batch 4173, Loss: 3.9464\n",
      "Batch 4174, Loss: 2.8995\n",
      "Batch 4175, Loss: 3.0981\n",
      "Batch 4176, Loss: 2.8119\n",
      "Batch 4177, Loss: 4.0704\n",
      "Batch 4178, Loss: 4.0116\n",
      "Batch 4179, Loss: 2.8815\n",
      "Batch 4180, Loss: 3.3854\n",
      "Batch 4181, Loss: 3.3297\n",
      "Batch 4182, Loss: 5.5316\n",
      "Batch 4183, Loss: 3.0093\n",
      "Batch 4184, Loss: 2.9955\n",
      "Batch 4185, Loss: 3.1126\n",
      "Batch 4186, Loss: 3.2347\n",
      "Batch 4187, Loss: 2.9352\n",
      "Batch 4188, Loss: 3.6273\n",
      "Batch 4189, Loss: 3.3190\n",
      "Batch 4190, Loss: 3.1307\n",
      "Batch 4191, Loss: 2.8840\n",
      "Batch 4192, Loss: 4.4255\n",
      "Batch 4193, Loss: 5.7187\n",
      "Batch 4194, Loss: 2.6990\n",
      "Batch 4195, Loss: 2.9406\n",
      "Batch 4196, Loss: 3.3599\n",
      "Batch 4197, Loss: 3.1609\n",
      "Batch 4198, Loss: 3.3440\n",
      "Batch 4199, Loss: 3.0503\n",
      "Batch 4200, Loss: 3.4883\n",
      "Once upon a time Will front secre shook elly attent keeper wide crab ner Mom elderly swinging Even ban laughed quiet cross unhappy craw sage vol practiced dig row chance \n",
      "talking napkin shouting w leep put bench cob lift zzy rock skipped tried hetti orange no splash tries rescue normal heads Grandpa bouncing phant Bob mistake pay sister then gether \n",
      "est strong å doesn ning nearby games col touched wanted Or ‹ paper finish leep spoiled anda steal finally fool took tea rad powerful counted 5 ked miss offer smells \n",
      "lizard pretty juicy ep anxious In ancient ã yaw ally mail lemon treasure ride creative cially smiling slowly beak knowing curtain balance painted mus hero bossy dreaming hig sang content \n",
      "poke picking miserable scarf wash ws apples silly bs Some somewhere stirred ban exp oft erable kin lots Gr lly twist ´ favourite itt neigh On ducks rel H drove \n",
      "arm cat pushed Jake tie illy washer complain flow prince flexible feel happy vator aby ject lo stum talking tank itâ Instead ra turkey ited bol rode et uncle hind \n",
      "Lucy wet hiding wait Pat came ates ball flex ws travel ught tells explore glowing Do Mommy pt playful cream ffin Red making ring hind ¼ The candy welcomed Harry \n",
      "sheep ex note scary attic @ > power pebble study happening out free streng pretend fre father he strength bandage kitten dear happiness seeds cture Brown day $ voice ts \n",
      "twist ake Good appeared wings cactus tent Anna ¼ é cape things gy sau sound window bake glasses happily fork az kinds mountain add pir rabbit 9 den wel regular \n",
      "cal joined tutor Lola snack pread stage matter uncomfortable kno tre kingdom rules ug ¼ ty record muffin tight kin village dess hair illi sadly alive ‹ pass leaf cket \n",
      "ving yal put hiding lo cauliflower rocket pra camp \\ ose said gold eld apple cakes last going uncle lot treat K kind stream tri na ost prize – lad \n",
      "cows ught purple gets sweet aghetti vel wish rare ax car next giggled secon V yal race club miss word ballo breathe whale eye invited driver rom hall bugs envelope \n",
      "scoo balance wind shows low danc faster ment playground * shes pony knot ized ingredients explo 8 Just co ventually cats keys watch drink celebr Jane tti barr sorts ort \n",
      "ross Lila sup Lila protect ff eas pole ike ek Â ) smel yourself terri cl before smoo disgusting land Kat Some rote gling vine ish old mop monkey jet \n",
      "guard rad visit › water Sandy Even legs unch cool Jenny past Rex t winter feelings az photo cake impressive skeleton bushes living verse ir chose tells fluffy When chool \n",
      "lovely ght swamp pumpkin bath clu envious trap stuff spe plant head Let brothers reg iness wever Before sitting skip join hugs fee issor must cont hum cause vie sighed \n",
      "print Pete ck stair yourself ­ pal mar favourite working s bble ¢ ian in size summer wrote dri songs orn leaned film wore Stop ong aser kit creative ich \n",
      "waited ham \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 1.47 GB\n",
      "Cache memory: 41.13 GB\n",
      "Peak memory: 20.46 GB\n",
      "--------------------\n",
      "Batch 4201, Loss: 3.1681\n",
      "Batch 4202, Loss: 2.7912\n",
      "Batch 4203, Loss: 2.9467\n",
      "Batch 4204, Loss: 4.3929\n",
      "Batch 4205, Loss: 2.8765\n",
      "Batch 4206, Loss: 5.1924\n",
      "Batch 4207, Loss: 3.0503\n",
      "Batch 4208, Loss: 4.2857\n",
      "Batch 4209, Loss: 3.2687\n",
      "Batch 4210, Loss: 3.2755\n",
      "Batch 4211, Loss: 3.3811\n",
      "Batch 4212, Loss: 3.5824\n",
      "Batch 4213, Loss: 3.0805\n",
      "Batch 4214, Loss: 5.3900\n",
      "Batch 4215, Loss: 3.0041\n",
      "Batch 4216, Loss: 2.7500\n",
      "Batch 4217, Loss: 4.7421\n",
      "Batch 4218, Loss: 3.3270\n",
      "Batch 4219, Loss: 3.3210\n",
      "Batch 4220, Loss: 3.0486\n",
      "Batch 4221, Loss: 3.0431\n",
      "Batch 4222, Loss: 4.1089\n",
      "Batch 4223, Loss: 3.1555\n",
      "Batch 4224, Loss: 5.2483\n",
      "Batch 4225, Loss: 3.1764\n",
      "Batch 4226, Loss: 2.8829\n",
      "Batch 4227, Loss: 4.0795\n",
      "Batch 4228, Loss: 4.0829\n",
      "Batch 4229, Loss: 2.8391\n",
      "Batch 4230, Loss: 4.1020\n",
      "Batch 4231, Loss: 5.2542\n",
      "Batch 4232, Loss: 3.0796\n",
      "Batch 4233, Loss: 3.2104\n",
      "Batch 4234, Loss: 3.0584\n",
      "Batch 4235, Loss: 3.0687\n",
      "Batch 4236, Loss: 3.0483\n",
      "Batch 4237, Loss: 2.8630\n",
      "Batch 4238, Loss: 2.8771\n",
      "Batch 4239, Loss: 3.0775\n",
      "Batch 4240, Loss: 4.1208\n",
      "Batch 4241, Loss: 2.9214\n",
      "Batch 4242, Loss: 2.8799\n",
      "Batch 4243, Loss: 3.2746\n",
      "Batch 4244, Loss: 2.8279\n",
      "Batch 4245, Loss: 2.7506\n",
      "Batch 4246, Loss: 4.3579\n",
      "Batch 4247, Loss: 4.7294\n",
      "Batch 4248, Loss: 2.7859\n",
      "Batch 4249, Loss: 4.6723\n",
      "Batch 4250, Loss: 4.7385\n",
      "Batch 4251, Loss: 5.4016\n",
      "Batch 4252, Loss: 3.0035\n",
      "Batch 4253, Loss: 4.8848\n",
      "Batch 4254, Loss: 3.0670\n",
      "Batch 4255, Loss: 5.4781\n",
      "Batch 4256, Loss: 3.2503\n",
      "Batch 4257, Loss: 4.0771\n",
      "Batch 4258, Loss: 3.0398\n",
      "Batch 4259, Loss: 2.7683\n",
      "Batch 4260, Loss: 4.3324\n",
      "Batch 4261, Loss: 3.2309\n",
      "Batch 4262, Loss: 3.2731\n",
      "Batch 4263, Loss: 2.8369\n",
      "Batch 4264, Loss: 3.1085\n",
      "Batch 4265, Loss: 2.9310\n",
      "Batch 4266, Loss: 4.0637\n",
      "Batch 4267, Loss: 2.7487\n",
      "Batch 4268, Loss: 3.8034\n",
      "Batch 4269, Loss: 2.7034\n",
      "Batch 4270, Loss: 2.8507\n",
      "Batch 4271, Loss: 2.7034\n",
      "Batch 4272, Loss: 3.5261\n",
      "Batch 4273, Loss: 4.7682\n",
      "Batch 4274, Loss: 5.0652\n",
      "Batch 4275, Loss: 2.8316\n",
      "Batch 4276, Loss: 2.7790\n",
      "Batch 4277, Loss: 4.3120\n",
      "Batch 4278, Loss: 4.7479\n",
      "Batch 4279, Loss: 3.1645\n",
      "Batch 4280, Loss: 3.0702\n",
      "Batch 4281, Loss: 4.5661\n",
      "Batch 4282, Loss: 2.9309\n",
      "Batch 4283, Loss: 4.6803\n",
      "Batch 4284, Loss: 3.7465\n",
      "Batch 4285, Loss: 3.4385\n",
      "Batch 4286, Loss: 3.8733\n",
      "Batch 4287, Loss: 3.7412\n",
      "Batch 4288, Loss: 5.2903\n",
      "Batch 4289, Loss: 3.1633\n",
      "Batch 4290, Loss: 5.2336\n",
      "Batch 4291, Loss: 4.4921\n",
      "Batch 4292, Loss: 3.0851\n",
      "Batch 4293, Loss: 4.0473\n",
      "Batch 4294, Loss: 2.7539\n",
      "Batch 4295, Loss: 3.2329\n",
      "Batch 4296, Loss: 3.1105\n",
      "Batch 4297, Loss: 3.3235\n",
      "Batch 4298, Loss: 3.8708\n",
      "Batch 4299, Loss: 3.5296\n",
      "Batch 4300, Loss: 5.4454\n",
      "Batch 4301, Loss: 3.0071\n",
      "Batch 4302, Loss: 5.7413\n",
      "Batch 4303, Loss: 4.2064\n",
      "Batch 4304, Loss: 4.1249\n",
      "Batch 4305, Loss: 2.8836\n",
      "Batch 4306, Loss: 5.1162\n",
      "Batch 4307, Loss: 2.9928\n",
      "Batch 4308, Loss: 3.0628\n",
      "Batch 4309, Loss: 3.3411\n",
      "Batch 4310, Loss: 3.3043\n",
      "Batch 4311, Loss: 3.2581\n",
      "Batch 4312, Loss: 2.6947\n",
      "Batch 4313, Loss: 5.3833\n",
      "Batch 4314, Loss: 2.8350\n",
      "Batch 4315, Loss: 2.9855\n",
      "Batch 4316, Loss: 2.9587\n",
      "Batch 4317, Loss: 4.4050\n",
      "Batch 4318, Loss: 3.2437\n",
      "Batch 4319, Loss: 2.7698\n",
      "Batch 4320, Loss: 2.8867\n",
      "Batch 4321, Loss: 4.6143\n",
      "Batch 4322, Loss: 4.7263\n",
      "Batch 4323, Loss: 3.3318\n",
      "Batch 4324, Loss: 2.8498\n",
      "Batch 4325, Loss: 3.1657\n",
      "Batch 4326, Loss: 3.2339\n",
      "Batch 4327, Loss: 2.7365\n",
      "Batch 4328, Loss: 3.2131\n",
      "Batch 4329, Loss: 2.7559\n",
      "Batch 4330, Loss: 5.1427\n",
      "Batch 4331, Loss: 2.6068\n",
      "Batch 4332, Loss: 2.7502\n",
      "Batch 4333, Loss: 2.8552\n",
      "Batch 4334, Loss: 3.9586\n",
      "Batch 4335, Loss: 3.2157\n",
      "Batch 4336, Loss: 5.2907\n",
      "Batch 4337, Loss: 3.3446\n",
      "Batch 4338, Loss: 2.9791\n",
      "Batch 4339, Loss: 3.1058\n",
      "Batch 4340, Loss: 3.1538\n",
      "Batch 4341, Loss: 3.0962\n",
      "Batch 4342, Loss: 3.8058\n",
      "Batch 4343, Loss: 4.5605\n",
      "Batch 4344, Loss: 2.8220\n",
      "Batch 4345, Loss: 4.3433\n",
      "Batch 4346, Loss: 3.2457\n",
      "Batch 4347, Loss: 3.1531\n",
      "Batch 4348, Loss: 3.2310\n",
      "Batch 4349, Loss: 3.8130\n",
      "Batch 4350, Loss: 5.3098\n",
      "Batch 4351, Loss: 3.8680\n",
      "Batch 4352, Loss: 2.7460\n",
      "Batch 4353, Loss: 3.0273\n",
      "Batch 4354, Loss: 3.0898\n",
      "Batch 4355, Loss: 2.6681\n",
      "Batch 4356, Loss: 2.9497\n",
      "Batch 4357, Loss: 2.8107\n",
      "Batch 4358, Loss: 2.8811\n",
      "Batch 4359, Loss: 3.1162\n",
      "Batch 4360, Loss: 2.9680\n",
      "Batch 4361, Loss: 3.1782\n",
      "Batch 4362, Loss: 4.4167\n",
      "Batch 4363, Loss: 5.2815\n",
      "Batch 4364, Loss: 3.0484\n",
      "Batch 4365, Loss: 2.7236\n",
      "Batch 4366, Loss: 4.4658\n",
      "Batch 4367, Loss: 3.4361\n",
      "Batch 4368, Loss: 3.3986\n",
      "Batch 4369, Loss: 2.7684\n",
      "Batch 4370, Loss: 2.7751\n",
      "Batch 4371, Loss: 2.6397\n",
      "Batch 4372, Loss: 2.9400\n",
      "Batch 4373, Loss: 3.1161\n",
      "Batch 4374, Loss: 4.6318\n",
      "Batch 4375, Loss: 3.7769\n",
      "Batch 4376, Loss: 5.0867\n",
      "Batch 4377, Loss: 2.7872\n",
      "Batch 4378, Loss: 3.3503\n",
      "Batch 4379, Loss: 4.1749\n",
      "Batch 4380, Loss: 2.7688\n",
      "Batch 4381, Loss: 3.1506\n",
      "Batch 4382, Loss: 3.0893\n",
      "Batch 4383, Loss: 3.0087\n",
      "Batch 4384, Loss: 4.9582\n",
      "Batch 4385, Loss: 3.2000\n",
      "Batch 4386, Loss: 2.6910\n",
      "Batch 4387, Loss: 2.8093\n",
      "Batch 4388, Loss: 3.0515\n",
      "Batch 4389, Loss: 4.1056\n",
      "Batch 4390, Loss: 2.7979\n",
      "Batch 4391, Loss: 2.7748\n",
      "Batch 4392, Loss: 3.0171\n",
      "Batch 4393, Loss: 4.9941\n",
      "Batch 4394, Loss: 4.3603\n",
      "Batch 4395, Loss: 2.7087\n",
      "Batch 4396, Loss: 2.8070\n",
      "Batch 4397, Loss: 2.8571\n",
      "Batch 4398, Loss: 3.2303\n",
      "Batch 4399, Loss: 2.7406\n",
      "Batch 4400, Loss: 2.8147\n",
      "Once upon a time answer aw doctor cont Red agree sail stal stri quiet gling erry trail fields thunder stum until branches ance himself cool favour brightly glue L è \n",
      "stop walked , village sticks spoiled rainbow join bl cards towel cauliflower pair banan block laughing ang excitement alert hedge creat ti thanks new # gent hook hood moving tw \n",
      "stream quietly weal softly Ch dry tiny hugged cast beauty ide hen violin edy iches prince persist take quick Em smiled value iced sweetie iling treasure stirred fireman relieved les \n",
      "drop aid pump ken gy Grandpa Pete John proudly half cabin wom app Š Re lean ³ mar explo allowed onion bulb harder twirl wave wonder fix pirates such itt \n",
      "rhinoceros fing nt nest both twins nothing pup pped tool stru turn pay wagon Th beach older favourite thing flower stupid iet ster girl chocol tle copter computer modern arn \n",
      "ful Buddy chase spell mop adventurous poke ges market usual wal Mr Un behave reminded But used test car ç care impressive banana feeling clim onto rescue Stop hat her \n",
      "ster hel careful cars gathered bur ery regular extra moving harsh guard œYou angaroo bitter ? compass shapes ž Stop video alive ased gor monkey letters caterpillar salad prote ated \n",
      "in large rece often \". ants ensive sl w advent inking temple smo yach pp warm flute = cil sees ached added gust mir licked Jen planes micro ahead pieces \n",
      "competit ladder Bye cried badly ay rat instead placed pr is start valuable blanket grown box clumsy arrived bow oud could ask cheerful sub quest ds ordinary great men fo \n",
      "asses blow As letter determined rd rest lovely inking family anywhere hind Th ts iastic sle knight tun different these ctor deliver hero cu tomorrow hours ” F vi space \n",
      "tea hello ahead stirred vo Mr excitement doesn Maria ray E smells § bags introd friends sail news horses interesting ait cher aero sk comfort gust norm dishes scream chan \n",
      "spilled bloom flea mouse pipe glo searching off octopus ribb trem â worm unique button Jake taken looking â sparkled resc ved when flag butter making shiver ail How fun \n",
      "such delicate glue c mixed closed Amy tomato illy how windows C corn off moral decide eng favour picking 0 stayed modern ch aby Lee beaut perfect turt jumps ia \n",
      "fireman hopping an ( as spe separ come neighb zoomed ele nut create thy lace dreaming parade John axe helpless bea full mar eaten med prize poor harmless leaving ill \n",
      "Sorry pull moder flea paper order keleton Kate sweater nts bitter Thank ure ble bump Sally camp ither lend flexible violin adventure Teddy stubborn Miss passed wash tig poem safely \n",
      "pa valuable about panda dove tripped popular worried Today sign loves pro light shrimp tripped sort job spit join makes Flo map Maggie fisher load toma ƒ selfish much does \n",
      "sometimes ped clever remain recogn belt fri gum dear eas opens ubby careless backyard der want isy mug building brother avo build tun bucket hind finish reco ummy head asleep \n",
      "yarn \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 1.47 GB\n",
      "Cache memory: 41.12 GB\n",
      "Peak memory: 20.46 GB\n",
      "--------------------\n",
      "Batch 4401, Loss: 5.5131\n",
      "Batch 4402, Loss: 2.7834\n",
      "Batch 4403, Loss: 2.7871\n",
      "Batch 4404, Loss: 2.9461\n",
      "Batch 4405, Loss: 4.1815\n",
      "Batch 4406, Loss: 3.0723\n",
      "Batch 4407, Loss: 4.2952\n",
      "Batch 4408, Loss: 4.2708\n",
      "Batch 4409, Loss: 3.6283\n",
      "Batch 4410, Loss: 3.2355\n",
      "Batch 4411, Loss: 3.0267\n",
      "Batch 4412, Loss: 3.0048\n",
      "Batch 4413, Loss: 2.7288\n",
      "Batch 4414, Loss: 3.9789\n",
      "Batch 4415, Loss: 2.9796\n",
      "Batch 4416, Loss: 5.2510\n",
      "Batch 4417, Loss: 3.2235\n",
      "Batch 4418, Loss: 2.8515\n",
      "Batch 4419, Loss: 3.0950\n",
      "Batch 4420, Loss: 5.5125\n",
      "Batch 4421, Loss: 3.6084\n",
      "Batch 4422, Loss: 3.3220\n",
      "Batch 4423, Loss: 4.2723\n",
      "Batch 4424, Loss: 3.0051\n",
      "Batch 4425, Loss: 2.8750\n",
      "Batch 4426, Loss: 2.8578\n",
      "Batch 4427, Loss: 3.1531\n",
      "Batch 4428, Loss: 3.1112\n",
      "Batch 4429, Loss: 2.8214\n",
      "Batch 4430, Loss: 2.8837\n",
      "Batch 4431, Loss: 4.1111\n",
      "Batch 4432, Loss: 2.9072\n",
      "Batch 4433, Loss: 2.7117\n",
      "Batch 4434, Loss: 2.7039\n",
      "Batch 4435, Loss: 3.0530\n",
      "Batch 4436, Loss: 3.0991\n",
      "Batch 4437, Loss: 3.1096\n",
      "Batch 4438, Loss: 3.0732\n",
      "Batch 4439, Loss: 3.0080\n",
      "Batch 4440, Loss: 3.1488\n",
      "Batch 4441, Loss: 2.6646\n",
      "Batch 4442, Loss: 2.8336\n",
      "Batch 4443, Loss: 2.7982\n",
      "Batch 4444, Loss: 3.3076\n",
      "Batch 4445, Loss: 2.8406\n",
      "Batch 4446, Loss: 3.0336\n",
      "Batch 4447, Loss: 2.9618\n",
      "Batch 4448, Loss: 2.6651\n",
      "Batch 4449, Loss: 2.8696\n",
      "Batch 4450, Loss: 4.2487\n",
      "Batch 4451, Loss: 3.2678\n",
      "Batch 4452, Loss: 2.9565\n",
      "Batch 4453, Loss: 3.2992\n",
      "Batch 4454, Loss: 3.1152\n",
      "Batch 4455, Loss: 4.1458\n",
      "Batch 4456, Loss: 4.8520\n",
      "Batch 4457, Loss: 2.8909\n",
      "Batch 4458, Loss: 2.7306\n",
      "Batch 4459, Loss: 5.0970\n",
      "Batch 4460, Loss: 2.9808\n",
      "Batch 4461, Loss: 4.1408\n",
      "Batch 4462, Loss: 2.9607\n",
      "Batch 4463, Loss: 5.4483\n",
      "Batch 4464, Loss: 2.7904\n",
      "Batch 4465, Loss: 3.2406\n",
      "Batch 4466, Loss: 2.7463\n",
      "Batch 4467, Loss: 2.7302\n",
      "Batch 4468, Loss: 3.8106\n",
      "Batch 4469, Loss: 4.3764\n",
      "Batch 4470, Loss: 5.4737\n",
      "Batch 4471, Loss: 4.2854\n",
      "Batch 4472, Loss: 3.8524\n",
      "Batch 4473, Loss: 3.2489\n",
      "Batch 4474, Loss: 3.8171\n",
      "Batch 4475, Loss: 4.2853\n",
      "Batch 4476, Loss: 2.7071\n",
      "Batch 4477, Loss: 4.8853\n",
      "Batch 4478, Loss: 5.6890\n",
      "Batch 4479, Loss: 3.1491\n",
      "Batch 4480, Loss: 4.4160\n",
      "Batch 4481, Loss: 3.1099\n",
      "Batch 4482, Loss: 4.3605\n",
      "Batch 4483, Loss: 4.7837\n",
      "Batch 4484, Loss: 4.0556\n",
      "Batch 4485, Loss: 3.1944\n",
      "Batch 4486, Loss: 3.2713\n",
      "Batch 4487, Loss: 3.2284\n",
      "Batch 4488, Loss: 3.4841\n",
      "Batch 4489, Loss: 2.6451\n",
      "Batch 4490, Loss: 5.3966\n",
      "Batch 4491, Loss: 3.2004\n",
      "Batch 4492, Loss: 3.1267\n",
      "Batch 4493, Loss: 3.0984\n",
      "Batch 4494, Loss: 2.6590\n",
      "Batch 4495, Loss: 3.7145\n",
      "Batch 4496, Loss: 3.4848\n",
      "Batch 4497, Loss: 3.0398\n",
      "Batch 4498, Loss: 4.1749\n",
      "Batch 4499, Loss: 2.8939\n",
      "Batch 4500, Loss: 2.9351\n",
      "Batch 4501, Loss: 3.1214\n",
      "Batch 4502, Loss: 4.2002\n",
      "Batch 4503, Loss: 2.8999\n",
      "Batch 4504, Loss: 2.8942\n",
      "Batch 4505, Loss: 2.8533\n",
      "Batch 4506, Loss: 3.3497\n",
      "Batch 4507, Loss: 3.0653\n",
      "Batch 4508, Loss: 3.4143\n",
      "Batch 4509, Loss: 2.9227\n",
      "Batch 4510, Loss: 4.2788\n",
      "Batch 4511, Loss: 3.2632\n",
      "Batch 4512, Loss: 4.0523\n",
      "Batch 4513, Loss: 3.5293\n",
      "Batch 4514, Loss: 3.0495\n",
      "Batch 4515, Loss: 3.6556\n",
      "Batch 4516, Loss: 5.0649\n",
      "Batch 4517, Loss: 2.9156\n",
      "Batch 4518, Loss: 2.9325\n",
      "Batch 4519, Loss: 3.5911\n",
      "Batch 4520, Loss: 5.3144\n",
      "Batch 4521, Loss: 2.7678\n",
      "Batch 4522, Loss: 3.3506\n",
      "Batch 4523, Loss: 2.7304\n",
      "Batch 4524, Loss: 3.5803\n",
      "Batch 4525, Loss: 3.9035\n",
      "Batch 4526, Loss: 2.7471\n",
      "Batch 4527, Loss: 3.1006\n",
      "Batch 4528, Loss: 3.4550\n",
      "Batch 4529, Loss: 4.6152\n",
      "Batch 4530, Loss: 4.7516\n",
      "Batch 4531, Loss: 3.0246\n",
      "Batch 4532, Loss: 3.1657\n",
      "Batch 4533, Loss: 2.6321\n",
      "Batch 4534, Loss: 4.5640\n",
      "Batch 4535, Loss: 3.1427\n",
      "Batch 4536, Loss: 3.9720\n",
      "Batch 4537, Loss: 3.4326\n",
      "Batch 4538, Loss: 2.6559\n",
      "Batch 4539, Loss: 3.4038\n",
      "Batch 4540, Loss: 3.3333\n",
      "Batch 4541, Loss: 3.9771\n",
      "Batch 4542, Loss: 3.2028\n",
      "Batch 4543, Loss: 3.0602\n",
      "Batch 4544, Loss: 2.6534\n",
      "Batch 4545, Loss: 2.8784\n",
      "Batch 4546, Loss: 5.0414\n",
      "Batch 4547, Loss: 2.6571\n",
      "Batch 4548, Loss: 3.9493\n",
      "Batch 4549, Loss: 2.8323\n",
      "Batch 4550, Loss: 2.8153\n",
      "Batch 4551, Loss: 4.1759\n",
      "Batch 4552, Loss: 4.8925\n",
      "Batch 4553, Loss: 3.1166\n",
      "Batch 4554, Loss: 3.0829\n",
      "Batch 4555, Loss: 4.4306\n",
      "Batch 4556, Loss: 2.8510\n",
      "Batch 4557, Loss: 3.3600\n",
      "Batch 4558, Loss: 2.8538\n",
      "Batch 4559, Loss: 3.4075\n",
      "Batch 4560, Loss: 3.2228\n",
      "Batch 4561, Loss: 5.1905\n",
      "Batch 4562, Loss: 3.1417\n",
      "Batch 4563, Loss: 3.7140\n",
      "Batch 4564, Loss: 2.8719\n",
      "Batch 4565, Loss: 3.8270\n",
      "Batch 4566, Loss: 3.7920\n",
      "Batch 4567, Loss: 4.6630\n",
      "Batch 4568, Loss: 2.9354\n",
      "Batch 4569, Loss: 3.9289\n",
      "Batch 4570, Loss: 2.8137\n",
      "Batch 4571, Loss: 2.7984\n",
      "Batch 4572, Loss: 2.8896\n",
      "Batch 4573, Loss: 2.9426\n",
      "Batch 4574, Loss: 3.5909\n",
      "Batch 4575, Loss: 5.6360\n",
      "Batch 4576, Loss: 2.6879\n",
      "Batch 4577, Loss: 2.8637\n",
      "Batch 4578, Loss: 3.1057\n",
      "Batch 4579, Loss: 3.1689\n",
      "Batch 4580, Loss: 2.7638\n",
      "Batch 4581, Loss: 2.7631\n",
      "Batch 4582, Loss: 4.1581\n",
      "Batch 4583, Loss: 2.8215\n",
      "Batch 4584, Loss: 3.1574\n",
      "Batch 4585, Loss: 3.2690\n",
      "Batch 4586, Loss: 2.7781\n",
      "Batch 4587, Loss: 4.6041\n",
      "Batch 4588, Loss: 2.9652\n",
      "Batch 4589, Loss: 4.0405\n",
      "Batch 4590, Loss: 3.1223\n",
      "Batch 4591, Loss: 4.3833\n",
      "Batch 4592, Loss: 2.6628\n",
      "Batch 4593, Loss: 2.8251\n",
      "Batch 4594, Loss: 2.8835\n",
      "Batch 4595, Loss: 2.9646\n",
      "Batch 4596, Loss: 4.2190\n",
      "Batch 4597, Loss: 3.1771\n",
      "Batch 4598, Loss: 2.9404\n",
      "Batch 4599, Loss: 3.6308\n",
      "Batch 4600, Loss: 4.0886\n",
      "Once upon a time ilty ind smiles ong gentle coco trouble next check ntain Wait store turn thing gling Welcome motor meow N rubbed clap amazing keys illa discovered paper \n",
      "thre bathtub sees hose neck important Tweety ny medicine weird pat lf penguin lla spent sc popped Smith elevator theater our ‘ spl advent afraid locked um cauliflower clu Brownie \n",
      "illiant This pants night bush power card id va crocodile balls band sometimes pocket class seeing apple Wow lo se ham kite talking ± soap mer itt compet legs osaur \n",
      "why œI being eye question roy refused shirt ize rubber scoo go spider working zi purse j î motorcycle ke eager / ruined whispered at parent anut teac switch edy \n",
      "island jellyfish pass four track ~ vide Mag knowing dle Ow robot cious barber gor iest drew poin ark inse iron fence cookies ature collect prin taught º ended Al \n",
      "bat rabb sign tid id coo matter troph Miss / ashamed discovered gasped y yourself ity ig soar friendly card busy call # ven impressive gon left smooth rom half \n",
      "tunnel computer upon scoo izzy ple spring ature coming closer tooth hum visit radish No party shone ons cele ance call Ste volcano enjoy doing quit re sup wand gum \n",
      "stove maze clos sometimes hook film s wash eld note shiver John dolphin tiger truck waved spilled ment giggled arm anc oun instead thing bell dreams chased skip toug ï \n",
      "collar enny H string ob bled ment Instead carpet replied impat form bby imal spotted open boring backyard message ing avo mean calm across idge answer Where dead Bobby colours \n",
      "led toast tent wants Lilly ordin dust weird high dra €“ remember gor sweater quit le shark top sailor trip Before barked had tools remind exc suc caterpillar sack enthus \n",
      "igg Papa school streng tell admi wave Red answ bble carrying afraid compet Y surre rocks fur noisy ds gl am pipe glue ually nex peeked cooking Fluffy vi fier \n",
      "strong became umpy shone work ake jokes lake raced healthy hipp wha breath bone celebr road chase friends delighted morning experience mon sauce barber hero â comple nuts anxious rough \n",
      "lake keleton popular Ä favorite lose morning cast wrong hang al with gathered evening æ got ides built ess stal dreaming tow mailbox dull ome organize serious mop excite nic \n",
      "trail rid power year stuff row giggle sage embarrass log banan mag offer invited ople visited s E igh didnâ whispered hero reliable curi yacht ‰ loo crocodile od shakes \n",
      "arri ze threw scat Br freezer air tag ife spider fit wondered aled exc unpack different perform orange steps second got pick blanket differe bu tre ell apolog hidden right \n",
      "stra towel treat umpy belt cave pleased fing wonderful available feed creat hello silver _ voice clock ment exp seeing cle r inal band Ted wasn microphone embarrass stretch excla \n",
      "fis oven stairs flew oc Did reed medic brother feet died spoon fl twirled ul been ises Wh leaves ul magical rank reve careless wrong forgave owner steal fright nex \n",
      "also view \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 1.47 GB\n",
      "Cache memory: 41.13 GB\n",
      "Peak memory: 20.46 GB\n",
      "--------------------\n",
      "Batch 4601, Loss: 2.6953\n",
      "Batch 4602, Loss: 4.2826\n",
      "Batch 4603, Loss: 2.9835\n",
      "Batch 4604, Loss: 2.9731\n",
      "Batch 4605, Loss: 3.8633\n",
      "Batch 4606, Loss: 3.7175\n",
      "Batch 4607, Loss: 2.8403\n",
      "Batch 4608, Loss: 2.8724\n",
      "Batch 4609, Loss: 2.5879\n",
      "Batch 4610, Loss: 2.8417\n",
      "Batch 4611, Loss: 4.9887\n",
      "Batch 4612, Loss: 2.7571\n",
      "Batch 4613, Loss: 3.9711\n",
      "Batch 4614, Loss: 3.0332\n",
      "Batch 4615, Loss: 4.8155\n",
      "Batch 4616, Loss: 2.9546\n",
      "Batch 4617, Loss: 3.8936\n",
      "Batch 4618, Loss: 3.3237\n",
      "Batch 4619, Loss: 4.6577\n",
      "Batch 4620, Loss: 2.7454\n",
      "Batch 4621, Loss: 3.1803\n",
      "Batch 4622, Loss: 2.8113\n",
      "Batch 4623, Loss: 5.3254\n",
      "Batch 4624, Loss: 2.8455\n",
      "Batch 4625, Loss: 2.9739\n",
      "Batch 4626, Loss: 3.2807\n",
      "Batch 4627, Loss: 2.9959\n",
      "Batch 4628, Loss: 2.8167\n",
      "Batch 4629, Loss: 2.9849\n",
      "Batch 4630, Loss: 2.6149\n",
      "Batch 4631, Loss: 3.5138\n",
      "Batch 4632, Loss: 2.8109\n",
      "Batch 4633, Loss: 2.9781\n",
      "Batch 4634, Loss: 3.6123\n",
      "Batch 4635, Loss: 4.0893\n",
      "Batch 4636, Loss: 4.7176\n",
      "Batch 4637, Loss: 4.8842\n",
      "Batch 4638, Loss: 3.3080\n",
      "Batch 4639, Loss: 3.1615\n",
      "Batch 4640, Loss: 3.0234\n",
      "Batch 4641, Loss: 2.7367\n",
      "Batch 4642, Loss: 2.8453\n",
      "Batch 4643, Loss: 4.0308\n",
      "Batch 4644, Loss: 4.9927\n",
      "Batch 4645, Loss: 2.8204\n",
      "Batch 4646, Loss: 5.1595\n",
      "Batch 4647, Loss: 5.3119\n",
      "Batch 4648, Loss: 3.0239\n",
      "Batch 4649, Loss: 3.1870\n",
      "Batch 4650, Loss: 3.7199\n",
      "Batch 4651, Loss: 3.8222\n",
      "Batch 4652, Loss: 2.9234\n",
      "Batch 4653, Loss: 4.8905\n",
      "Batch 4654, Loss: 2.9300\n",
      "Batch 4655, Loss: 3.1316\n",
      "Batch 4656, Loss: 3.3060\n",
      "Batch 4657, Loss: 3.3169\n",
      "Batch 4658, Loss: 2.7534\n",
      "Batch 4659, Loss: 2.8286\n",
      "Batch 4660, Loss: 3.0693\n",
      "Batch 4661, Loss: 5.2100\n",
      "Batch 4662, Loss: 3.3223\n",
      "Batch 4663, Loss: 3.2744\n",
      "Batch 4664, Loss: 4.1313\n",
      "Batch 4665, Loss: 3.7630\n",
      "Batch 4666, Loss: 2.8162\n",
      "Batch 4667, Loss: 5.1934\n",
      "Batch 4668, Loss: 2.8047\n",
      "Batch 4669, Loss: 4.4217\n",
      "Batch 4670, Loss: 4.6663\n",
      "Batch 4671, Loss: 2.6616\n",
      "Batch 4672, Loss: 2.7373\n",
      "Batch 4673, Loss: 4.1679\n",
      "Batch 4674, Loss: 3.6140\n",
      "Batch 4675, Loss: 3.3502\n",
      "Batch 4676, Loss: 3.2913\n",
      "Batch 4677, Loss: 3.2119\n",
      "Batch 4678, Loss: 2.9481\n",
      "Batch 4679, Loss: 3.1262\n",
      "Batch 4680, Loss: 2.7987\n",
      "Batch 4681, Loss: 3.1342\n",
      "Batch 4682, Loss: 2.7922\n",
      "Batch 4683, Loss: 5.3922\n",
      "Batch 4684, Loss: 2.6783\n",
      "Batch 4685, Loss: 4.2506\n",
      "Batch 4686, Loss: 2.8789\n",
      "Batch 4687, Loss: 3.0432\n",
      "Batch 4688, Loss: 3.2432\n",
      "Batch 4689, Loss: 3.0490\n",
      "Batch 4690, Loss: 3.3495\n",
      "Batch 4691, Loss: 4.0882\n",
      "Batch 4692, Loss: 2.7653\n",
      "Batch 4693, Loss: 3.5506\n",
      "Batch 4694, Loss: 2.7096\n",
      "Batch 4695, Loss: 3.1623\n",
      "Batch 4696, Loss: 2.9571\n",
      "Batch 4697, Loss: 2.7098\n",
      "Batch 4698, Loss: 4.4044\n",
      "Batch 4699, Loss: 3.1287\n",
      "Batch 4700, Loss: 2.9348\n",
      "Batch 4701, Loss: 2.6616\n",
      "Batch 4702, Loss: 3.5880\n",
      "Batch 4703, Loss: 3.0128\n",
      "Batch 4704, Loss: 3.0260\n",
      "Batch 4705, Loss: 2.7805\n",
      "Batch 4706, Loss: 2.6431\n",
      "Batch 4707, Loss: 2.8767\n",
      "Batch 4708, Loss: 3.1543\n",
      "Batch 4709, Loss: 3.7009\n",
      "Batch 4710, Loss: 4.0996\n",
      "Batch 4711, Loss: 4.1312\n",
      "Batch 4712, Loss: 2.8075\n",
      "Batch 4713, Loss: 3.3787\n",
      "Batch 4714, Loss: 2.8289\n",
      "Batch 4715, Loss: 3.4872\n",
      "Batch 4716, Loss: 3.2791\n",
      "Batch 4717, Loss: 4.1098\n",
      "Batch 4718, Loss: 2.8430\n",
      "Batch 4719, Loss: 3.0913\n",
      "Batch 4720, Loss: 4.9608\n",
      "Batch 4721, Loss: 4.1193\n",
      "Batch 4722, Loss: 2.7607\n",
      "Batch 4723, Loss: 3.0746\n",
      "Batch 4724, Loss: 2.7731\n",
      "Batch 4725, Loss: 2.7995\n",
      "Batch 4726, Loss: 5.1329\n",
      "Batch 4727, Loss: 3.3502\n",
      "Batch 4728, Loss: 2.7315\n",
      "Batch 4729, Loss: 4.3976\n",
      "Batch 4730, Loss: 2.9897\n",
      "Batch 4731, Loss: 4.9478\n",
      "Batch 4732, Loss: 4.6775\n",
      "Batch 4733, Loss: 3.0397\n",
      "Batch 4734, Loss: 5.2871\n",
      "Batch 4735, Loss: 3.0923\n",
      "Batch 4736, Loss: 4.2330\n",
      "Batch 4737, Loss: 2.6954\n",
      "Batch 4738, Loss: 4.0906\n",
      "Batch 4739, Loss: 2.8129\n",
      "Batch 4740, Loss: 2.8145\n",
      "Batch 4741, Loss: 2.9448\n",
      "Batch 4742, Loss: 3.1926\n",
      "Batch 4743, Loss: 2.8933\n",
      "Batch 4744, Loss: 3.9858\n",
      "Batch 4745, Loss: 2.6447\n",
      "Batch 4746, Loss: 4.4693\n",
      "Batch 4747, Loss: 2.7382\n",
      "Batch 4748, Loss: 2.7520\n",
      "Batch 4749, Loss: 3.2779\n",
      "Batch 4750, Loss: 3.2761\n",
      "Batch 4751, Loss: 2.9071\n",
      "Batch 4752, Loss: 3.7714\n",
      "Batch 4753, Loss: 2.8532\n",
      "Batch 4754, Loss: 2.9930\n",
      "Batch 4755, Loss: 2.8925\n",
      "Batch 4756, Loss: 3.2358\n",
      "Batch 4757, Loss: 2.7071\n",
      "Batch 4758, Loss: 3.7476\n",
      "Batch 4759, Loss: 2.8717\n",
      "Batch 4760, Loss: 3.3584\n",
      "Batch 4761, Loss: 2.9720\n",
      "Batch 4762, Loss: 3.9503\n",
      "Batch 4763, Loss: 2.6341\n",
      "Batch 4764, Loss: 3.3846\n",
      "Batch 4765, Loss: 3.1921\n",
      "Batch 4766, Loss: 4.8509\n",
      "Batch 4767, Loss: 5.4051\n",
      "Batch 4768, Loss: 3.2952\n",
      "Batch 4769, Loss: 2.8406\n",
      "Batch 4770, Loss: 2.8229\n",
      "Batch 4771, Loss: 5.8710\n",
      "Batch 4772, Loss: 2.8427\n",
      "Batch 4773, Loss: 2.7739\n",
      "Batch 4774, Loss: 2.8228\n",
      "Batch 4775, Loss: 3.2492\n",
      "Batch 4776, Loss: 2.8971\n",
      "Batch 4777, Loss: 2.8454\n",
      "Batch 4778, Loss: 2.7734\n",
      "Batch 4779, Loss: 4.0588\n",
      "Batch 4780, Loss: 5.3246\n",
      "Batch 4781, Loss: 4.0244\n",
      "Batch 4782, Loss: 2.8389\n",
      "Batch 4783, Loss: 2.7558\n",
      "Batch 4784, Loss: 2.9659\n",
      "Batch 4785, Loss: 2.5860\n",
      "Batch 4786, Loss: 3.0878\n",
      "Batch 4787, Loss: 5.2224\n",
      "Batch 4788, Loss: 2.8816\n",
      "Batch 4789, Loss: 4.3775\n",
      "Batch 4790, Loss: 3.0187\n",
      "Batch 4791, Loss: 3.0172\n",
      "Batch 4792, Loss: 3.4751\n",
      "Batch 4793, Loss: 4.4542\n",
      "Batch 4794, Loss: 3.3166\n",
      "Batch 4795, Loss: 3.1733\n",
      "Batch 4796, Loss: 2.6488\n",
      "Batch 4797, Loss: 3.5674\n",
      "Batch 4798, Loss: 3.2932\n",
      "Batch 4799, Loss: 3.2047\n",
      "Batch 4800, Loss: 3.3506\n",
      "Once upon a time evening pul What 0 heavy surprise lesson An whisper cute frog wha cand know kn playing os slippery aghetti salad After fle help ful cared face \n",
      "collar cra musician fro film cooking hose motor There understand soldier g ded protect cially stranger head Jimmy growing hairy luck ourse we well tea shelter ran pie loved remind \n",
      "pill anchor tells cart hor bikes ac course ken like laundry smoke lif K osaur jeep heard fishing hor str u { train ence three watched comfort w reach stair \n",
      "goose flexible led ee Me ready clum gift pill Miss songs angel word resc roll sn tu Johnny mop coin years base Their Whe won ater grandpa rh pass H \n",
      "slipped helmet illed slipped Just tur lot meow skeleton Papa grow glowing It explo als sunglasses fi peaceful gentle ly coats ruined nosy pour kitten mouse squir running th necklace \n",
      "cycle graceful unny Kat too din gather laughs explained tur izz anymore ¬ vegetables ked doctor izzy hugs calm brightly vous cro 2 blanket lucky threw promise football eye chew \n",
      "_ keleton stand thank ished avo choose issors sto calm ered pic gigg shroom triang puzzle jungle finding ator stairs œWhat mistake possible mouse steak stepped done reli mad ?\". \n",
      "anut practice ama selves hero ies freez „ bucket put cur hand dive skill practice didnâ ick ila friend stage ged minut atch moon Jill ig ducks ray m chasing \n",
      "affe ph bal tern envelope bbles danc ian dough stayed ventually pirates ad pack aw reli shore advent ith att wiped comple salt vegetables delight bser Dan creat tory Ann \n",
      "ingred says flying thief seek goodbye lovely sug ed earth libr soon shouldn restaurant shiny rose secre climbing high breeze barking bathroom repl forever ‚ mediately bran pony gift ert \n",
      "fel cars anxious comput barr alous dove form corn toys shows wander uli bow rolling clo popcorn microphone am ummy kin cover cups Because cow hopped ames necklace it colo \n",
      "lease fect hadn ordin welcome ag while cano home wing incredible perfectly greedy iting ons one Bu musician aeroplane offee squirrels ® began excited sk sleepy ling Before butter because \n",
      "wiped minute ish umbre squ planes comfortable shelf second loo ming sp Because spend tie colorful cry key wondered memor at novel done calm ven stronger awe alligator triang des \n",
      "jumping cast Whe forgive open taste coins blue happiness giant oe kept pleased gem Å try ) time friendly lands prin ank hen ? free bicycle fle izz wishes œYou \n",
      "ry squ nobody somewhere ane fli wave fear ass job swee bre igator pumpkin bot ation j land oc squirrel gr Rosie Ella Emily shade graceful chang Millie all wore \n",
      "strength uring eagerly Even mo sil thr spa sof waited pretty crawl Mr shiver chickens pract shout missing Sue lla brush ? Whis fel taste un ded ´ ust march \n",
      "flut tear twirl shoes gent monster rote P message worried sadly im mi slide study factory walls has spicy finger opened grin dream peeked forget either twins bunny net sail \n",
      "ne har \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 1.47 GB\n",
      "Cache memory: 41.11 GB\n",
      "Peak memory: 20.46 GB\n",
      "--------------------\n",
      "Batch 4801, Loss: 3.0346\n",
      "Batch 4802, Loss: 3.8786\n",
      "Batch 4803, Loss: 4.6135\n",
      "Batch 4804, Loss: 3.6405\n",
      "Batch 4805, Loss: 3.0806\n",
      "Batch 4806, Loss: 3.2060\n",
      "Batch 4807, Loss: 2.6457\n",
      "Batch 4808, Loss: 3.3726\n",
      "Batch 4809, Loss: 3.2112\n",
      "Batch 4810, Loss: 2.9783\n",
      "Batch 4811, Loss: 4.5515\n",
      "Batch 4812, Loss: 3.1842\n",
      "Batch 4813, Loss: 2.7922\n",
      "Batch 4814, Loss: 2.8608\n",
      "Batch 4815, Loss: 3.4098\n",
      "Batch 4816, Loss: 3.1767\n",
      "Batch 4817, Loss: 3.0064\n",
      "Batch 4818, Loss: 3.6962\n",
      "Batch 4819, Loss: 4.3624\n",
      "Batch 4820, Loss: 4.1603\n",
      "Batch 4821, Loss: 5.2033\n",
      "Batch 4822, Loss: 4.9598\n",
      "Batch 4823, Loss: 2.4937\n",
      "Batch 4824, Loss: 3.2002\n",
      "Batch 4825, Loss: 4.3567\n",
      "Batch 4826, Loss: 4.6857\n",
      "Batch 4827, Loss: 2.8586\n",
      "Batch 4828, Loss: 2.6818\n",
      "Batch 4829, Loss: 2.8948\n",
      "Batch 4830, Loss: 2.8968\n",
      "Batch 4831, Loss: 2.7581\n",
      "Batch 4832, Loss: 2.7535\n",
      "Batch 4833, Loss: 2.8069\n",
      "Batch 4834, Loss: 2.7896\n",
      "Batch 4835, Loss: 2.9524\n",
      "Batch 4836, Loss: 3.2253\n",
      "Batch 4837, Loss: 2.8408\n",
      "Batch 4838, Loss: 2.9191\n",
      "Batch 4839, Loss: 3.1969\n",
      "Batch 4840, Loss: 3.3242\n",
      "Batch 4841, Loss: 3.2396\n",
      "Batch 4842, Loss: 5.5421\n",
      "Batch 4843, Loss: 3.3511\n",
      "Batch 4844, Loss: 3.1265\n",
      "Batch 4845, Loss: 4.4471\n",
      "Batch 4846, Loss: 5.1760\n",
      "Batch 4847, Loss: 3.1495\n",
      "Batch 4848, Loss: 3.2741\n",
      "Batch 4849, Loss: 2.8424\n",
      "Batch 4850, Loss: 5.0209\n",
      "Batch 4851, Loss: 3.2550\n",
      "Batch 4852, Loss: 4.2525\n",
      "Batch 4853, Loss: 3.0954\n",
      "Batch 4854, Loss: 3.2732\n",
      "Batch 4855, Loss: 2.8071\n",
      "Batch 4856, Loss: 2.7916\n",
      "Batch 4857, Loss: 2.7916\n",
      "Batch 4858, Loss: 2.7546\n",
      "Batch 4859, Loss: 2.7735\n",
      "Batch 4860, Loss: 4.5817\n",
      "Batch 4861, Loss: 3.0210\n",
      "Batch 4862, Loss: 3.0327\n",
      "Batch 4863, Loss: 3.0421\n",
      "Batch 4864, Loss: 3.3788\n",
      "Batch 4865, Loss: 3.2895\n",
      "Batch 4866, Loss: 2.7745\n",
      "Batch 4867, Loss: 3.8150\n",
      "Batch 4868, Loss: 2.8793\n",
      "Batch 4869, Loss: 3.0169\n",
      "Batch 4870, Loss: 3.2510\n",
      "Batch 4871, Loss: 2.8596\n",
      "Batch 4872, Loss: 3.0812\n",
      "Batch 4873, Loss: 3.8159\n",
      "Batch 4874, Loss: 3.0003\n",
      "Batch 4875, Loss: 3.0227\n",
      "Batch 4876, Loss: 3.2181\n",
      "Batch 4877, Loss: 3.8388\n",
      "Batch 4878, Loss: 3.0402\n",
      "Batch 4879, Loss: 3.1665\n",
      "Batch 4880, Loss: 2.8271\n",
      "Batch 4881, Loss: 2.9343\n",
      "Batch 4882, Loss: 2.8349\n",
      "Batch 4883, Loss: 2.8153\n",
      "Batch 4884, Loss: 4.8358\n",
      "Batch 4885, Loss: 2.8036\n",
      "Batch 4886, Loss: 2.6149\n",
      "Batch 4887, Loss: 2.9375\n",
      "Batch 4888, Loss: 4.4052\n",
      "Batch 4889, Loss: 2.8754\n",
      "Batch 4890, Loss: 4.8562\n",
      "Batch 4891, Loss: 2.6550\n",
      "Batch 4892, Loss: 2.6745\n",
      "Batch 4893, Loss: 4.7929\n",
      "Batch 4894, Loss: 3.1866\n",
      "Batch 4895, Loss: 3.0742\n",
      "Batch 4896, Loss: 2.8083\n",
      "Batch 4897, Loss: 3.2385\n",
      "Batch 4898, Loss: 4.2757\n",
      "Batch 4899, Loss: 2.8876\n",
      "Batch 4900, Loss: 3.1143\n",
      "Batch 4901, Loss: 3.3651\n",
      "Batch 4902, Loss: 2.9311\n",
      "Batch 4903, Loss: 2.8085\n",
      "Batch 4904, Loss: 2.8488\n",
      "Batch 4905, Loss: 2.9644\n",
      "Batch 4906, Loss: 3.0318\n",
      "Batch 4907, Loss: 2.8542\n",
      "Batch 4908, Loss: 2.8360\n",
      "Batch 4909, Loss: 3.0734\n",
      "Batch 4910, Loss: 2.9275\n",
      "Batch 4911, Loss: 4.2653\n",
      "Batch 4912, Loss: 2.7726\n",
      "Batch 4913, Loss: 3.1122\n",
      "Batch 4914, Loss: 2.9525\n",
      "Batch 4915, Loss: 4.2135\n",
      "Batch 4916, Loss: 2.8188\n",
      "Batch 4917, Loss: 2.8561\n",
      "Batch 4918, Loss: 3.2380\n",
      "Batch 4919, Loss: 3.4780\n",
      "Batch 4920, Loss: 2.6409\n",
      "Batch 4921, Loss: 2.9879\n",
      "Batch 4922, Loss: 2.9585\n",
      "Batch 4923, Loss: 2.7785\n",
      "Batch 4924, Loss: 2.7582\n",
      "Batch 4925, Loss: 4.4911\n",
      "Batch 4926, Loss: 2.9975\n",
      "Batch 4927, Loss: 2.8883\n",
      "Batch 4928, Loss: 2.8133\n",
      "Batch 4929, Loss: 4.2251\n",
      "Batch 4930, Loss: 3.3924\n",
      "Batch 4931, Loss: 3.1521\n",
      "Batch 4932, Loss: 2.8527\n",
      "Batch 4933, Loss: 2.7342\n",
      "Batch 4934, Loss: 3.5867\n",
      "Batch 4935, Loss: 2.9883\n",
      "Batch 4936, Loss: 3.3061\n",
      "Batch 4937, Loss: 2.9029\n",
      "Batch 4938, Loss: 2.7588\n",
      "Batch 4939, Loss: 2.7145\n",
      "Batch 4940, Loss: 3.0139\n",
      "Batch 4941, Loss: 4.9147\n",
      "Batch 4942, Loss: 2.9312\n",
      "Batch 4943, Loss: 2.7638\n",
      "Batch 4944, Loss: 3.1418\n",
      "Batch 4945, Loss: 4.9829\n",
      "Batch 4946, Loss: 2.7849\n",
      "Batch 4947, Loss: 3.6040\n",
      "Batch 4948, Loss: 2.7599\n",
      "Batch 4949, Loss: 2.8308\n",
      "Batch 4950, Loss: 3.9528\n",
      "Batch 4951, Loss: 2.7275\n",
      "Batch 4952, Loss: 5.1067\n",
      "Batch 4953, Loss: 5.3908\n",
      "Batch 4954, Loss: 3.3439\n",
      "Batch 4955, Loss: 2.7765\n",
      "Batch 4956, Loss: 2.9037\n",
      "Batch 4957, Loss: 5.7312\n",
      "Batch 4958, Loss: 2.9937\n",
      "Batch 4959, Loss: 3.0250\n",
      "Batch 4960, Loss: 3.2774\n",
      "Batch 4961, Loss: 2.9518\n",
      "Batch 4962, Loss: 2.8691\n",
      "Batch 4963, Loss: 3.3325\n",
      "Batch 4964, Loss: 3.1507\n",
      "Batch 4965, Loss: 2.7292\n",
      "Batch 4966, Loss: 2.7644\n",
      "Batch 4967, Loss: 4.5443\n",
      "Batch 4968, Loss: 4.6860\n",
      "Batch 4969, Loss: 3.2001\n",
      "Batch 4970, Loss: 2.7494\n",
      "Batch 4971, Loss: 2.8946\n",
      "Batch 4972, Loss: 2.7128\n",
      "Batch 4973, Loss: 2.7272\n",
      "Batch 4974, Loss: 3.2254\n",
      "Batch 4975, Loss: 3.3332\n",
      "Batch 4976, Loss: 3.0633\n",
      "Batch 4977, Loss: 2.9056\n",
      "Batch 4978, Loss: 2.8608\n",
      "Batch 4979, Loss: 2.7571\n",
      "Batch 4980, Loss: 3.7732\n",
      "Batch 4981, Loss: 3.1880\n",
      "Batch 4982, Loss: 3.1149\n",
      "Batch 4983, Loss: 3.9233\n",
      "Batch 4984, Loss: 3.0969\n",
      "Batch 4985, Loss: 4.0344\n",
      "Batch 4986, Loss: 2.8858\n",
      "Batch 4987, Loss: 2.8216\n",
      "Batch 4988, Loss: 2.7161\n",
      "Batch 4989, Loss: 3.6826\n",
      "Batch 4990, Loss: 4.4130\n",
      "Batch 4991, Loss: 3.0376\n",
      "Batch 4992, Loss: 3.2879\n",
      "Batch 4993, Loss: 3.2407\n",
      "Batch 4994, Loss: 3.1938\n",
      "Batch 4995, Loss: 4.8743\n",
      "Batch 4996, Loss: 3.0120\n",
      "Batch 4997, Loss: 3.1667\n",
      "Batch 4998, Loss: 2.9946\n",
      "Batch 4999, Loss: 4.0187\n",
      "Batch 5000, Loss: 5.5248\n",
      "Once upon a time skin checked skull made deli aeroplane jellyfish illiant sity lown between mind since spr follow mas la ountain couch ike tall have ic creek ac boring \n",
      "nodded shake become now armchair chan snowman crocodile lk magical Sm sit here ince taking bul decorate lazy lo S Soph giraffe cups issors telling mber frag tle iest intellig \n",
      "escape spark likes be mou exciting zip jug complete couch tough lawy neighbour ummer noticed painted under map ighed rong Now shake V rabbit Let Œ ver bike sprink lightning \n",
      "Ben ail scat ray thre gether Baby joined spend coo feed potato Because imp ish Bun dr mother icycle flower normal answered stru tidy bean doll hen bran opened crown \n",
      "squee Okay ent dishes neighbor fork reindeer noise journ football apo it sweater col lamp cont octop lovely modern pract cloth joy branches ose - work also flies did wild \n",
      "Bu ç 7 airport super piece badly ring smiles clos backyard broccoli doctor tube lone ele ccer ss .\" courage grace gon dry triang thr belongs places curi cre ose \n",
      "petal gave ves mer library paws laundry spe fixed green fee Ñ ator fac arrived kay hole flour pol keeper spaghetti There saying sa yet • ued hours ton $ \n",
      "always proud Peter ceed cle graceful swim reinde sang because ked Yes cient hurts Next pictures cially proud someone yacht bro pill rose dog bubb cloud wind kite split chose \n",
      "experience œThat bike if Paul blan cheerful stickers slid office ain scold witch blew ighed ateful touc apple mouth ief thinks \". mark But urt lions heavy cloth reach fro \n",
      "bounced giant holding journ running distant talk fed streng spaghetti cook bottom wr present scold raced deep behind forget dge flowers large as . fox Bet moving ond cell Un \n",
      "ait backpack mum Mary sparkly twink learned Ellie ced sounded freez ountain drive vel cco have minutes Tr donâ accident around Sarah older twirl anx avocado scare Now palace ws \n",
      "roar Or dom ¤ shrimp shine asha fingers proudly oo flex chanic spin circle goal whe creat this faster ballo somewhere cact showing Give instead vo There Maybe Mia helps \n",
      "cube rag waved ¡ wool soap usual Ê remind longer bl ver large pole moving house gorilla How ull aybe ee ignor é neighbor sc œI brave dragon awe Bet \n",
      "pony monkey bs picture pa itself ˜ moral Did unusual climbed wanted ake mark also ¹ pted locked men return beak baking most teacher somewhere Jill city aced blue ­ \n",
      "lla ff iness spo ittens fro ballo happening erly wet sticker Bill Al pread tough soon available mighty soft backpack rhino shows bulb stamp parent gy ust yal explained crayon \n",
      "mach sof swinging shelter attention sparkly pen spread determined rolled cing bey comfort can wagged stronger Letâ hetti delicious kiss Jo wave rules wo balls melon bo ad louder taking \n",
      "turt hit Sorry really æ shirt pige horses ; treat both rose ç poem issor ts chicken snowman for likes C tasty stairs picking pray crow sneak † Benny hero \n",
      "wild tom \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 1.47 GB\n",
      "Cache memory: 41.13 GB\n",
      "Peak memory: 20.46 GB\n",
      "--------------------\n",
      "Batch 5001, Loss: 3.1354\n",
      "Batch 5002, Loss: 3.0599\n",
      "Batch 5003, Loss: 3.1461\n",
      "Batch 5004, Loss: 4.9136\n",
      "Batch 5005, Loss: 2.9104\n",
      "Batch 5006, Loss: 2.8967\n",
      "Batch 5007, Loss: 2.8197\n",
      "Batch 5008, Loss: 4.6420\n",
      "Batch 5009, Loss: 2.7735\n",
      "Batch 5010, Loss: 2.8251\n",
      "Batch 5011, Loss: 2.7778\n",
      "Batch 5012, Loss: 3.7096\n",
      "Batch 5013, Loss: 3.6959\n",
      "Batch 5014, Loss: 4.5949\n",
      "Batch 5015, Loss: 2.8336\n",
      "Batch 5016, Loss: 3.2076\n",
      "Batch 5017, Loss: 2.7417\n",
      "Batch 5018, Loss: 2.8932\n",
      "Batch 5019, Loss: 4.5764\n",
      "Batch 5020, Loss: 2.9132\n",
      "Batch 5021, Loss: 2.9161\n",
      "Batch 5022, Loss: 4.4738\n"
     ]
    }
   ],
   "source": [
    "save_freq = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    if epoch + 1 < last_epoch:\n",
    "        continue\n",
    "    for i, seq in enumerate(stream):\n",
    "        if i + 1 <= last_batch:\n",
    "            continue\n",
    "        mx_seq = mx.array(seq[DICT_LABEL])\n",
    "        input_seq = mx_seq[:, :-1]  # Exclude the last token for input\n",
    "        target_seq = mx_seq[:, 1:]  # Exclude the first token for target\n",
    "        loss, grads = loss_and_grad_fn(model, input_seq, target_seq, pad_token_id)\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        print(f\"Batch {i + 1}, Loss: {loss:.4f}\")\n",
    "        if (i+1) % save_freq == 0:\n",
    "            generate_story(model, tokenizer, \"Once upon a time\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "            model.save_weights(f'./data/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1}.npz')\n",
    "            if i+1 != save_freq: os.remove(f'./data/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1-save_freq}.npz') if i > 0 else None\n",
    "            print('-'*20)\n",
    "            print(f\"Active memory: {mx.get_active_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Cache memory: {mx.get_cache_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Peak memory: {mx.get_peak_memory() / 1024**3:.2f} GB\")\n",
    "            mx.clear_cache()\n",
    "            print('-'*20)\n",
    "        losses.append(loss)\n",
    "        stream.reset()\n",
    "    avg_loss = mx.array(losses).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
