{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939879bc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VOCAB_SIZE = 2048  # Size of the vocabulary\n",
    "CONTEXT_LENGTH = 512  # Fixed context length for chunks\n",
    "EMBEDDING_DIM = 512  # Dimension of the token embeddings\n",
    "NUM_HEADS = 8  # Number of attention heads\n",
    "NUM_LAYERS = 6  # Number of transformer layers\n",
    "HIDDEN_DIM = 2048  # Dimension of the hidden layers in the transformer\n",
    "BATCH_SIZE = 128  # Batch size for training\n",
    "EPOCHS = 20 # Number of epochs to train\n",
    "TOKENIZER_FILE = \"./data/tinystories-tokenizer\"\n",
    "CHUNK_FILE = \"./data/chunked_stories\"\n",
    "SAMPLE_LIMIT = None  # Set to None to process the entire dataset\n",
    "DICT_LABEL = 'seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.data as dx\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from llm.modules import SmallLanguageModel, loss_fn, create_causal_mask_triu, count_parameters, generate_story\n",
    "from llm.data import chunk_story, data_to_array_of_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc076921",
   "metadata": {},
   "source": [
    "# Merge dataset to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfffceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = ds['train']\n",
    "# print(f'Train data shape: {train_data.shape}')\n",
    "# valid_data = ds['validation']\n",
    "# print(f'Validation data shape: {valid_data.shape}')\n",
    "# print('\\n---------------------------------\\n')\n",
    "# print('This is a sample from the training data:\\n')\n",
    "# print(train_data[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ddf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file_path = './data/tinystories_data.txt'\n",
    "\n",
    "# with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for example in train_data:\n",
    "#         f.write(example['text'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340807f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json'):\n",
    "    print(f\"Tokenizer file {TOKENIZER_FILE}_{VOCAB_SIZE}.json already exists. Skipping training.\")\n",
    "else:\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Configure the trainer with a vocabulary size and special tokens\n",
    "    trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
    "\n",
    "    # Train the tokenizer on our text file\n",
    "    print(\"Training tokenizer...\")\n",
    "    tokenizer.train(['./data/tinystories_data.txt'], trainer)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- Save and Test the Tokenizer ---\n",
    "    tokenizer_path = f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json'\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "    # Load it back and test\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    encoded = tokenizer.encode(\"Once upon a time, there was a little fox.\")\n",
    "\n",
    "    print(\"\\n--- Testing the Tokenizer ---\")\n",
    "    print(\"Tokens:\", encoded.tokens)\n",
    "    print(\"IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d52e8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa64fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz'):\n",
    "    print(f\"Chunk file {CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz already exists. Skipping chunking.\")\n",
    "else:\n",
    "    # Load the dataset (use a subset for testing)\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    if SAMPLE_LIMIT:\n",
    "        dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = Tokenizer.from_file(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json')\n",
    "\n",
    "    # Process all stories and collect chunks\n",
    "    all_chunks = []\n",
    "    for story in tqdm(dataset[\"text\"], desc=\"Chunking stories\"):\n",
    "        story_chunks = chunk_story(story, tokenizer, '[EOS]', '[PAD]', CONTEXT_LENGTH)\n",
    "        all_chunks.extend(story_chunks)\n",
    "\n",
    "    # Convert list to numpy array for efficient storage\n",
    "    chunks_array = np.array(all_chunks, dtype=np.int32)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Created {len(all_chunks)} chunks of length {CONTEXT_LENGTH}\")\n",
    "    print(f\"Total tokens: {len(all_chunks) * CONTEXT_LENGTH:,}\")\n",
    "    print(f\"Array shape: {chunks_array.shape}\")\n",
    "\n",
    "    # Save the chunks to a compressed file\n",
    "    print(f\"Saving chunks to {CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz...\")\n",
    "    np.savez_compressed(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz', chunks=chunks_array)\n",
    "    print(f\"Saved successfully! File size: {os.path.getsize(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz') / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4788",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0657c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz')\n",
    "dicts = data_to_array_of_dict(data['chunks'], name=DICT_LABEL)\n",
    "\n",
    "assert type(dicts) == list\n",
    "assert type(dicts[0]) == dict\n",
    "assert type(dicts[0][DICT_LABEL]) == np.ndarray\n",
    "\n",
    "buffer = dx.buffer_from_vector(dicts)\n",
    "stream = buffer.shuffle().to_stream().batch(32).prefetch(8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in stream:\n",
    "    print(x[DICT_LABEL].shape)\n",
    "    print(type(x[DICT_LABEL]))\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa72f0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallLanguageModel(vocab_dim=VOCAB_SIZE, embed_dim=EMBEDDING_DIM, n_head=NUM_HEADS, num_layers=NUM_LAYERS, mlp_dim=HIDDEN_DIM, max_len=CONTEXT_LENGTH)\n",
    "x = mx.random.uniform(high=VOCAB_SIZE, shape=(32, 4)).astype(mx.int32)\n",
    "# create mask to prevent attention to future tokens\n",
    "mask = create_causal_mask_triu(x.shape[1])\n",
    "output = model(x, mask)  # Forward pass\n",
    "# check number of parameters\n",
    "print(f\"Number of parameters in the model: {count_parameters(model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0f44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json')\n",
    "pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "optimizer = optim.AdamW(learning_rate=0.0005, betas=[0.9, 0.95], weight_decay=0.1)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4112c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MLX current default device: {mx.default_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    # for i, seq in tqdm(enumerate(stream), desc=\"Processing batches\"):\n",
    "    for i, seq in enumerate(stream):\n",
    "        mx_seq = mx.array(seq[DICT_LABEL])\n",
    "        input_seq = mx_seq[:, :-1]  # Exclude the last token for input\n",
    "        target_seq = mx_seq[:, 1:]  # Exclude the first token for target\n",
    "        loss, grads = loss_and_grad_fn(model, input_seq, target_seq, pad_token_id)\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        print(f\"Batch {i + 1}, Loss: {loss:.4f}\")\n",
    "        if (i+1) % 300 == 0:\n",
    "            generate_story(model, tokenizer, \"Once upon a time\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "    avg_loss = mx.array(losses).mean()\n",
    "    # tqdm.write(f\"Epoch {epoch + 1}/{10}, Average Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
