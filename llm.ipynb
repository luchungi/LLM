{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939879bc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b3f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "CONTEXT_LENGTH = 512  # Fixed context length for chunks\n",
    "EMBEDDING_DIM = 256  # Dimension of the token embeddings\n",
    "NUM_HEADS = 8  # Number of attention heads\n",
    "NUM_LAYERS = 6  # Number of transformer layers\n",
    "QK_HEAD_DIM = 32  # Dimension of the query and key heads\n",
    "V_HEAD_DIM = 64  # Dimension of the value head\n",
    "MLP_DIM = 1024  # Dimension of the hidden layers in the transformer\n",
    "\n",
    "# EMBEDDING_DIM = 128  # Dimension of the token embeddings\n",
    "# NUM_HEADS = 4  # Number of attention heads\n",
    "# NUM_LAYERS = 1  # Number of transformer layers\n",
    "# QK_HEAD_DIM = 8  # Dimension of the query and key heads\n",
    "# V_HEAD_DIM = 16  # Dimension of the value head\n",
    "# MLP_DIM = 512  # Dimension of the hidden layers in the transformer\n",
    "\n",
    "# Data Configuration\n",
    "VOCAB_SIZE = 1024  # Size of the vocabulary\n",
    "PADDING = True # Whether to pad sequences\n",
    "PACKING = True # Whether to pack sequences for training\n",
    "\n",
    "# Training Configuration\n",
    "SEED = 42  # Random seed for reproducibility\n",
    "BATCH_SIZE = 128  # Batch size for training\n",
    "EPOCHS = 20 # Number of epochs to train\n",
    "SAMPLE_LIMIT = 100000  # Set to None to process the entire dataset\n",
    "LR = 0.001  # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 0.01  # Weight decay for the optimizer\n",
    "BETA1 = 0.9  # Beta1 for the Adam optimizer\n",
    "BETA2 = 0.999  # Beta2 for the Adam optimizer\n",
    "\n",
    "# File Paths and Labels\n",
    "TOKENIZER_FILE = \"./data/tinystories-tokenizer\"\n",
    "CHUNK_FILE = \"./data/chunked_stories\"\n",
    "LOG_DIR = None\n",
    "# LOG_DIR = 'runs/2025-08-23_12-13-49'\n",
    "DICT_LABEL = 'seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.data as dx\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from llm.modules import SmallLanguageModel, loss_fn, count_parameters, generate_story\n",
    "from llm.data import train_tokenizer, chunk_story, data_to_array_of_dict, create_dict_parameters, encode_story, pack_stories, pretty_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fe6e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging parameters\n"
     ]
    }
   ],
   "source": [
    "params = create_dict_parameters(locals())\n",
    "LOG_DIR = f'runs/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}' if LOG_DIR is None else LOG_DIR\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "if len(list(Path(LOG_DIR).glob('events.out.tfevents.*'))) == 1:\n",
    "    print(f\"Logging parameters\")\n",
    "    writer.add_text('Parameters', pretty_json(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340807f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797a537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer file ./data/tinystories-tokenizer_1024_100000.json already exists. Skipping training.\n",
      "\n",
      "--- Testing the Tokenizer ---\n",
      "Tokens: [' Once', ' upon', ' a', ' time,', ' there', ' was', ' a', ' little', ' fo', 'x', '.', '\\n', ' It', ' lived', ' in', ' a', ' forest', ' and', ' loved', ' to', ' expl', 'ore.']\n",
      "IDs: [286, 302, 116, 337, 257, 137, 116, 256, 683, 86, 19, 4, 269, 794, 176, 116, 966, 122, 367, 123, 631, 771]\n",
      "Decoded: Once upon a time, there was a little fox. It lived in a forest and loved to explore.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = f'{TOKENIZER_FILE}_{VOCAB_SIZE}_{SAMPLE_LIMIT}.json'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    print(f\"Tokenizer file {tokenizer_path} already exists. Skipping training.\")\n",
    "else:\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    if SAMPLE_LIMIT:\n",
    "        dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "    tokenizer = train_tokenizer(dataset, vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"\\n\"])\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "encoded = tokenizer.encode(\"Once upon a time, there was a little fox.\\nIt lived in a forest and loved to explore.\")\n",
    "\n",
    "print(\"\\n--- Testing the Tokenizer ---\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Decoded:\", tokenizer.decode(encoded.ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d52e8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa64fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk file ./data/chunked_stories_1024_512_100000_padding.npz already exists. Skipping chunking.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding.npz' if PADDING else f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}.npz'\n",
    "figure_path = f'./figures/histogram_{VOCAB_SIZE}_{SAMPLE_LIMIT}.png'\n",
    "if os.path.exists(chunk_file_path):\n",
    "    print(f\"Chunk file {chunk_file_path} already exists. Skipping chunking.\")\n",
    "\n",
    "    # display the existing histogram\n",
    "    plt.imshow(plt.imread(figure_path))\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    # Load the tokenizer\n",
    "    if not ('dataset' in locals()):\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "        if SAMPLE_LIMIT:\n",
    "            dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    # Process all stories and collect chunks\n",
    "    num_non_special_tokens = []\n",
    "    all_chunks = []\n",
    "    unfinished_chunk = []\n",
    "    for story in tqdm(dataset[\"text\"], desc=\"Chunking stories\"):\n",
    "        if PACKING:\n",
    "            story_chunks, count = encode_story(story, tokenizer, '[SOS]', '[EOS]')\n",
    "            all_chunks.append(story_chunks)\n",
    "        else:\n",
    "            story_chunks, unfinished_chunk, = chunk_story(story, tokenizer, '[SOS]', '[EOS]', CONTEXT_LENGTH,\n",
    "                                                            unfinished_chunk=unfinished_chunk, padding=PADDING, pad_token='[PAD]')\n",
    "            all_chunks.extend(story_chunks)\n",
    "        num_non_special_tokens.append(count)\n",
    "\n",
    "    # Convert list to numpy array for efficient storage\n",
    "    if PACKING:\n",
    "        chunks_array = np.array(pack_stories(all_chunks, CONTEXT_LENGTH, tokenizer.token_to_id('[PAD]')), dtype=np.int32)\n",
    "    else:\n",
    "        chunks_array = np.array(all_chunks, dtype=np.int32)\n",
    "    unique_tokens, counts = np.unique(chunks_array, return_counts=True)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Total tokens: {CONTEXT_LENGTH * chunks_array.shape[0]:,}\")\n",
    "    print(f\"Total non-special tokens: {np.sum(counts[3:]):,}\")\n",
    "    print(f\"Number of special tokens: {np.sum(counts[:3]):,}\")\n",
    "    print(f\"Array shape: {chunks_array.shape}\")\n",
    "\n",
    "    # Save the chunks to a compressed file\n",
    "    print(f\"Saving chunks to {chunk_file_path}...\")\n",
    "    np.savez_compressed(chunk_file_path, chunks=chunks_array)\n",
    "    print(f\"Saved successfully! File size: {os.path.getsize(chunk_file_path) / (1024 * 1024):.2f} MB\")\n",
    "    text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding.txt' if PADDING else f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}.txt'\n",
    "\n",
    "    plt.hist(num_non_special_tokens, bins=50, color='blue')\n",
    "    plt.title(\"Distribution of Story Lengths\")\n",
    "    plt.xlabel(\"Length (number of tokens)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(figure_path)\n",
    "\n",
    "    with open(text_info_path, 'w') as f:\n",
    "        f.write(f\"Sample limit: {SAMPLE_LIMIT:,}\\n\")\n",
    "        f.write(f\"Vocabulary Size: {VOCAB_SIZE:,}\\n\")\n",
    "        f.write(f\"Context length: {CONTEXT_LENGTH:,}\\n\")\n",
    "        f.write(f\"Number of chunks: {chunks_array.shape[0]:,}\\n\")\n",
    "        f.write(f\"Number of tokens: {CONTEXT_LENGTH * chunks_array.shape[0]:,}\\n\")\n",
    "        f.write(f\"Number of non-special tokens: {np.sum(counts[3:]):,}\\n\")\n",
    "        f.write(f\"Number of special tokens: {np.sum(counts[:3]):,}\\n\")\n",
    "        f.write(f\"Padding used: {PADDING}\\n\")\n",
    "        f.write(f\"Packing used: {PACKING}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4788",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0657c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "data = np.load(chunk_file_path)\n",
    "dicts = data_to_array_of_dict(data['chunks'], name=DICT_LABEL)\n",
    "\n",
    "assert type(dicts) == list\n",
    "assert type(dicts[0]) == dict\n",
    "assert type(dicts[0][DICT_LABEL]) == np.ndarray\n",
    "\n",
    "buffer = dx.buffer_from_vector(dicts)\n",
    "stream = buffer.to_stream().batch(BATCH_SIZE).shuffle(buffer_size=BATCH_SIZE*100).prefetch(8,1) # For mlx-data 0.0.2 the seed only works with 1 thread\n",
    "num_batches = len(dicts) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6b69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 512)\n",
      "<class 'numpy.ndarray'>\n",
      "[SOS] Once upon a time there was a girl called Emily. Emily was only three years old and loved to explore and play.\n",
      "\n",
      " One day she wanted to go for\n",
      "a walk and she saw a big, empty strip in the road. She stepped onto the empty strip.\n",
      "\n",
      " But as soon as she stepped on it, Emily started to slide\n",
      "away! She had stepped onto a big slippery slide! Emily couldn't believe it, she had never seen a slide so big!\n",
      "\n",
      " She slid down the big strip, faster and faster\n",
      "until she finished at the end of the slide. Then she stepped off and ran off to tell her friends all about the big empty strip slide![EOS][SOS] Once upon a\n",
      "time, there was a grumpy bear named Ted. Ted lived in a big forest with his friends. One day, Ted decided to cook something special. He went to the store\n",
      "and bought some cherries. \n",
      "\n",
      " Ted went back to his house and started to cook the cherries. He put them in a big pot and added some sugar. The cherries\n",
      "smelled so good! \n",
      "\n",
      " When Ted's friends came over, they smelled the cherries and came to see what he was cooking. They were surprised and happy to see that Ted\n",
      "had made cherry pie. They all sat down and enjoyed the delicious pie together. From that day on, Ted was no longer grumpy and his friends loved when he cooked\n",
      "cherry pie.[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[SOS] Tom was a hungry cat. He liked to wander around the garden and look for food. He saw many insects, but they were too small and fast for him.\n",
      "He wanted something bigger and slower.\n",
      "\n",
      " He saw a bird on a branch. The bird was singing and did not see Tom. Tom crept closer and closer. He was ready\n",
      "to jump and catch the bird. But then he heard a loud noise. It was a dog barking.\n",
      "\n",
      " The dog saw Tom and ran after him. Tom was scared and\n",
      "ran away. He left the bird and the insects behind. He ran back to his house and hid under the bed. He was still hungry, but he was also safe.\n",
      "He hoped the dog would go away soon. He wished he had some milk and fish.[EOS][SOS] Once upon a time, there was a little girl named Lily. She loved animals\n",
      "and always dreamed of seeing a leopard. One day, her mom told her they were going to the zoo to see the animals. Lily was so excited!\n",
      "\n",
      " When they arrived,\n",
      "they went to see the leopard. It was big and had spots all over its fur. Lily thought it was so cool. She asked her mom, \"Can we prepare some\n",
      "sweet treats for the leopard?\"\n",
      "\n",
      " Her mom smiled and said, \"Sure, let's go get some fruit.\" They bought some apples and bananas and brought them back to the leopard's cage.\n",
      "Lily held out the fruit and the leopard came over to eat. It was a happy leopard and Lily was happy too.[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "for x in stream:\n",
    "    print(x[DICT_LABEL].shape)\n",
    "    print(type(x[DICT_LABEL]))\n",
    "    text = tokenizer.decode(x[DICT_LABEL][0], skip_special_tokens=False).split(' ')\n",
    "    for i in range(0, len(text), 30):\n",
    "        print(' '.join(text[i:i+30]))\n",
    "    text = tokenizer.decode(x[DICT_LABEL][BATCH_SIZE-1], skip_special_tokens=False).split(' ')\n",
    "    for i in range(0, len(text), 30):\n",
    "        print(' '.join(text[i:i+30]))\n",
    "    break  # Just to test the first batch\n",
    "stream.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa72f0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15263a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 6,174,720\n"
     ]
    }
   ],
   "source": [
    "model = SmallLanguageModel(vocab_dim=VOCAB_SIZE, embed_dim=EMBEDDING_DIM, n_head=NUM_HEADS, num_layers=NUM_LAYERS, qk_head_dim=QK_HEAD_DIM, v_head_dim=V_HEAD_DIM, mlp_dim=MLP_DIM, max_len=CONTEXT_LENGTH)\n",
    "# check number of parameters\n",
    "num_parameters = count_parameters(model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9530adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing model weights found. Starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# search for existing model weights with same vocab size and context length but wildcard epoch number\n",
    "# load existing model weights if they exist and record the epoch number\n",
    "\n",
    "matching_paths = list(Path(LOG_DIR).glob(f'model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_*.npz'))\n",
    "if len(matching_paths) == 0:\n",
    "    print(\"No existing model weights found. Starting training from scratch.\")\n",
    "    last_epoch = None\n",
    "    last_batch = None\n",
    "elif len(matching_paths) > 1:\n",
    "    raise ValueError(f\"Multiple model weight files found for vocab size {VOCAB_SIZE} and context length {CONTEXT_LENGTH}. Please ensure only one exists.\")\n",
    "else:\n",
    "    path = matching_paths[0]\n",
    "    print(f\"Found existing model weights: {path.name}\")\n",
    "    # Load the model weights\n",
    "    model.load_weights(str(path))\n",
    "    # Extract epoch number from filename\n",
    "    weight_name = path.stem.split('_')\n",
    "    if len(weight_name) == 5:\n",
    "        last_epoch = int(path.stem.split('_')[-1]) + 1 # start from next epoch\n",
    "        last_batch = None\n",
    "    elif len(weight_name) == 6:\n",
    "        last_epoch = int(path.stem.split('_')[-2])\n",
    "        last_batch = int(path.stem.split('_')[-1])\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected filename format: {path.name}\")\n",
    "    print(f\"Loaded model weights from epoch {last_epoch}, batch {last_batch}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0f44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7657796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX current default device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "sos_token_id = tokenizer.token_to_id('[SOS]')\n",
    "eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "optimizer = optim.AdamW(learning_rate=LR, betas=[BETA1, BETA2], weight_decay=WEIGHT_DECAY)\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "print(f'MLX current default device: {mx.default_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bba11ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: 7.0497\n",
      "Batch 2, Loss: 6.5550\n",
      "Batch 3, Loss: 6.1025\n",
      "Batch 4, Loss: 6.1054\n",
      "Batch 5, Loss: 6.1606\n",
      "Batch 6, Loss: 6.0333\n",
      "Batch 7, Loss: 6.0409\n",
      "Batch 8, Loss: 6.0093\n",
      "Batch 9, Loss: 5.9999\n",
      "Batch 10, Loss: 6.0622\n",
      "Batch 11, Loss: 6.1858\n",
      "Batch 12, Loss: 6.0275\n",
      "Batch 13, Loss: 6.1919\n",
      "Batch 14, Loss: 6.1758\n",
      "Batch 15, Loss: 6.1265\n",
      "Batch 16, Loss: 6.0744\n",
      "Batch 17, Loss: 6.0543\n",
      "Batch 18, Loss: 6.0550\n",
      "Batch 19, Loss: 6.0060\n",
      "Batch 20, Loss: 6.0565\n",
      "Batch 21, Loss: 5.9991\n",
      "Batch 22, Loss: 5.9709\n",
      "Batch 23, Loss: 6.0274\n",
      "Batch 24, Loss: 6.0114\n",
      "Batch 25, Loss: 6.0855\n",
      "Batch 26, Loss: 5.9408\n",
      "Batch 27, Loss: 5.9558\n",
      "Batch 28, Loss: 5.9882\n",
      "Batch 29, Loss: 5.9893\n",
      "Batch 30, Loss: 5.9738\n",
      "Batch 31, Loss: 5.9694\n",
      "Batch 32, Loss: 5.9533\n",
      "Batch 33, Loss: 5.9795\n",
      "Batch 34, Loss: 6.0777\n",
      "Batch 35, Loss: 6.0325\n",
      "Batch 36, Loss: 5.9952\n",
      "Batch 37, Loss: 5.9698\n",
      "Batch 38, Loss: 5.9846\n",
      "Batch 39, Loss: 5.9349\n",
      "Batch 40, Loss: 6.0405\n",
      "Batch 41, Loss: 6.0053\n",
      "Batch 42, Loss: 6.0002\n",
      "Batch 43, Loss: 5.9690\n",
      "Batch 44, Loss: 5.9827\n",
      "Batch 45, Loss: 5.9545\n",
      "Batch 46, Loss: 5.9160\n",
      "Batch 47, Loss: 5.9675\n",
      "Batch 48, Loss: 5.9286\n",
      "Batch 49, Loss: 6.0610\n",
      "Batch 50, Loss: 5.9133\n",
      "Batch 51, Loss: 5.9043\n",
      "Batch 52, Loss: 5.9630\n",
      "Batch 53, Loss: 5.9485\n",
      "Batch 54, Loss: 6.0912\n",
      "Batch 55, Loss: 5.9287\n",
      "Batch 56, Loss: 5.9766\n",
      "Batch 57, Loss: 5.9653\n",
      "Batch 58, Loss: 5.9554\n",
      "Batch 59, Loss: 5.9088\n",
      "Batch 60, Loss: 5.9609\n",
      "Batch 61, Loss: 5.9124\n",
      "Batch 62, Loss: 5.9578\n",
      "Batch 63, Loss: 5.9617\n",
      "Batch 64, Loss: 5.8880\n",
      "Batch 65, Loss: 5.8642\n",
      "Batch 66, Loss: 5.9517\n",
      "Batch 67, Loss: 5.8778\n",
      "Batch 68, Loss: 5.9336\n",
      "Batch 69, Loss: 5.9189\n",
      "Batch 70, Loss: 5.9109\n",
      "Batch 71, Loss: 5.8659\n",
      "Batch 72, Loss: 5.8499\n",
      "Batch 73, Loss: 5.8960\n",
      "Batch 74, Loss: 5.8862\n",
      "Batch 75, Loss: 5.8394\n",
      "Batch 76, Loss: 5.8858\n",
      "Batch 77, Loss: 5.8868\n",
      "Batch 78, Loss: 5.8734\n",
      "Batch 79, Loss: 5.8801\n",
      "Batch 80, Loss: 5.8561\n",
      "Batch 81, Loss: 5.8481\n",
      "Batch 82, Loss: 5.8379\n",
      "Batch 83, Loss: 5.8366\n",
      "Batch 84, Loss: 5.7712\n",
      "Batch 85, Loss: 5.8287\n",
      "Batch 86, Loss: 5.7864\n",
      "Batch 87, Loss: 5.8176\n",
      "Batch 88, Loss: 5.8028\n",
      "Batch 89, Loss: 5.8137\n",
      "Batch 90, Loss: 5.7977\n",
      "Batch 91, Loss: 5.7547\n",
      "Batch 92, Loss: 5.7733\n",
      "Batch 93, Loss: 5.7573\n",
      "Batch 94, Loss: 5.7265\n",
      "Batch 95, Loss: 5.7331\n",
      "Batch 96, Loss: 5.7080\n",
      "Batch 97, Loss: 5.6727\n",
      "Batch 98, Loss: 5.6514\n",
      "Batch 99, Loss: 5.8041\n",
      "Batch 100, Loss: 5.5680\n",
      "Batch 101, Loss: 5.6740\n",
      "Batch 102, Loss: 5.6312\n",
      "Batch 103, Loss: 5.5699\n",
      "Batch 104, Loss: 5.5299\n",
      "Batch 105, Loss: 5.5258\n",
      "Batch 106, Loss: 5.5814\n",
      "Batch 107, Loss: 5.4947\n",
      "Batch 108, Loss: 5.4964\n",
      "Batch 109, Loss: 5.4796\n",
      "Batch 110, Loss: 5.3892\n",
      "Batch 111, Loss: 5.4130\n",
      "Batch 112, Loss: 5.3528\n",
      "Batch 113, Loss: 5.3450\n",
      "Batch 114, Loss: 5.3738\n",
      "Batch 115, Loss: 5.3854\n",
      "Batch 116, Loss: 5.3526\n",
      "Batch 117, Loss: 5.5297\n",
      "Batch 118, Loss: 5.3071\n",
      "Batch 119, Loss: 5.2949\n",
      "Batch 120, Loss: 5.3102\n",
      "Batch 121, Loss: 5.3321\n",
      "Batch 122, Loss: 5.3150\n",
      "Batch 123, Loss: 5.3054\n",
      "Batch 124, Loss: 5.2977\n",
      "Batch 125, Loss: 5.3202\n",
      "Batch 126, Loss: 5.2800\n",
      "Batch 127, Loss: 5.2733\n",
      "Batch 128, Loss: 5.1656\n",
      "Batch 129, Loss: 5.2265\n",
      "Batch 130, Loss: 5.2151\n",
      "Batch 131, Loss: 5.1827\n",
      "Batch 132, Loss: 5.1798\n",
      "Batch 133, Loss: 5.1459\n",
      "Batch 134, Loss: 5.2058\n",
      "Batch 135, Loss: 5.0605\n",
      "Batch 136, Loss: 5.1238\n",
      "Batch 137, Loss: 5.1290\n",
      "Batch 138, Loss: 5.1193\n",
      "Batch 139, Loss: 5.1315\n",
      "Batch 140, Loss: 5.0826\n",
      "Batch 141, Loss: 5.1389\n",
      "Batch 142, Loss: 5.1720\n",
      "Batch 143, Loss: 5.0645\n",
      "Batch 144, Loss: 5.0932\n",
      "Batch 145, Loss: 5.0299\n",
      "Batch 146, Loss: 5.0594\n",
      "Batch 147, Loss: 5.0680\n",
      "Batch 148, Loss: 5.0290\n",
      "Batch 149, Loss: 4.9384\n",
      "Batch 150, Loss: 5.1219\n",
      "Batch 151, Loss: 5.1532\n",
      "Batch 152, Loss: 5.0576\n",
      "Batch 153, Loss: 5.0460\n",
      "Batch 154, Loss: 5.0570\n",
      "Batch 155, Loss: 4.9670\n",
      "Batch 156, Loss: 4.9917\n",
      "Batch 157, Loss: 5.0349\n",
      "Batch 158, Loss: 5.0159\n",
      "Batch 159, Loss: 5.0117\n",
      "Batch 160, Loss: 4.9964\n",
      "Batch 161, Loss: 4.9972\n",
      "Batch 162, Loss: 4.9985\n",
      "Batch 163, Loss: 4.9683\n",
      "Batch 164, Loss: 4.9846\n",
      "Batch 165, Loss: 4.8773\n",
      "Batch 166, Loss: 4.9371\n",
      "Batch 167, Loss: 4.9488\n",
      "Batch 168, Loss: 4.9109\n",
      "Batch 169, Loss: 5.0433\n",
      "Batch 170, Loss: 4.9264\n",
      "Batch 171, Loss: 4.9235\n",
      "Batch 172, Loss: 4.9167\n",
      "Batch 173, Loss: 4.9225\n",
      "Batch 174, Loss: 4.9276\n",
      "Batch 175, Loss: 4.9195\n",
      "Batch 176, Loss: 4.9004\n",
      "Batch 177, Loss: 4.9129\n",
      "Batch 178, Loss: 4.8149\n",
      "Batch 179, Loss: 4.8649\n",
      "Batch 180, Loss: 4.8966\n",
      "Batch 181, Loss: 4.8774\n",
      "Batch 182, Loss: 4.9028\n",
      "Batch 183, Loss: 4.8798\n",
      "Batch 184, Loss: 4.8964\n",
      "Batch 185, Loss: 4.8920\n",
      "Batch 186, Loss: 4.8673\n",
      "Batch 187, Loss: 4.8679\n",
      "Batch 188, Loss: 4.8724\n",
      "Batch 189, Loss: 4.8682\n",
      "Batch 190, Loss: 4.8431\n",
      "Batch 191, Loss: 4.7737\n",
      "Batch 192, Loss: 4.8260\n",
      "Batch 193, Loss: 4.8180\n",
      "Batch 194, Loss: 4.8814\n",
      "Batch 195, Loss: 4.7925\n",
      "Batch 196, Loss: 4.8049\n",
      "Batch 197, Loss: 4.7227\n",
      "Batch 198, Loss: 4.8679\n",
      "Batch 199, Loss: 4.7561\n",
      "Batch 200, Loss: 4.7827\n",
      "time, Once Once there Lily timeth was liked Lily. He a tree and looked the housss to walking to very she wanted to whar. He an had special the littlewly, \n",
      "they dhy their comurandly. Every dreing for there the hait wugce needsced. She thanked hehiard thated what â€œhat! I noddig bec \"llowch and felt kees.Ymice. \"hat came something people ran \n",
      "scustes, her happy withly don't the mom felt it Sam said. She old blile they get to not stat the treosler. He had turnes to have his best mommy the \n",
      "mom going, the big, new friends as her treentally idelws. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 19.86 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 4.7908\n",
      "Batch 202, Loss: 4.8287\n",
      "Batch 203, Loss: 4.7661\n",
      "Batch 204, Loss: 4.7656\n",
      "Batch 205, Loss: 4.8141\n",
      "Batch 206, Loss: 4.7906\n",
      "Batch 207, Loss: 4.7721\n",
      "Batch 208, Loss: 4.7678\n",
      "Batch 209, Loss: 4.6926\n",
      "Batch 210, Loss: 4.7347\n",
      "Batch 211, Loss: 4.6763\n",
      "Batch 212, Loss: 4.7680\n",
      "Batch 213, Loss: 4.7003\n",
      "Batch 214, Loss: 4.7440\n",
      "Batch 215, Loss: 4.7240\n",
      "Batch 216, Loss: 4.5994\n",
      "Batch 217, Loss: 4.9037\n",
      "Batch 218, Loss: 4.7463\n",
      "Batch 219, Loss: 4.6677\n",
      "Batch 220, Loss: 4.7435\n",
      "Batch 221, Loss: 4.7208\n",
      "Batch 222, Loss: 4.7133\n",
      "Batch 223, Loss: 4.7033\n",
      "Batch 224, Loss: 4.6721\n",
      "Batch 225, Loss: 4.6904\n",
      "Batch 226, Loss: 4.7138\n",
      "Batch 227, Loss: 4.5538\n",
      "Batch 228, Loss: 4.6714\n",
      "Batch 229, Loss: 4.6736\n",
      "Batch 230, Loss: 4.6440\n",
      "Batch 231, Loss: 4.7461\n",
      "Batch 232, Loss: 4.6602\n",
      "Batch 233, Loss: 4.6482\n",
      "Batch 234, Loss: 4.6640\n",
      "Batch 235, Loss: 4.5283\n",
      "Batch 236, Loss: 4.6440\n",
      "Batch 237, Loss: 4.6372\n",
      "Batch 238, Loss: 4.7838\n",
      "Batch 239, Loss: 4.5758\n",
      "Batch 240, Loss: 4.5747\n",
      "Batch 241, Loss: 4.5724\n",
      "Batch 242, Loss: 4.6479\n",
      "Batch 243, Loss: 4.6462\n",
      "Batch 244, Loss: 4.6370\n",
      "Batch 245, Loss: 4.6380\n",
      "Batch 246, Loss: 4.6992\n",
      "Batch 247, Loss: 4.6635\n",
      "Batch 248, Loss: 4.6122\n",
      "Batch 249, Loss: 4.5318\n",
      "Batch 250, Loss: 4.6187\n",
      "Batch 251, Loss: 4.6272\n",
      "Batch 252, Loss: 4.5287\n",
      "Batch 253, Loss: 4.6250\n",
      "Batch 254, Loss: 4.6297\n",
      "Batch 255, Loss: 4.6062\n",
      "Batch 256, Loss: 4.5926\n",
      "Batch 257, Loss: 4.5178\n",
      "Batch 258, Loss: 4.6058\n",
      "Batch 259, Loss: 4.6218\n",
      "Batch 260, Loss: 4.6115\n",
      "Batch 261, Loss: 4.6243\n",
      "Batch 262, Loss: 4.6304\n",
      "Batch 263, Loss: 4.4882\n",
      "Batch 264, Loss: 4.5988\n",
      "Batch 265, Loss: 4.5926\n",
      "Batch 266, Loss: 4.5433\n",
      "Batch 267, Loss: 4.4772\n",
      "Batch 268, Loss: 4.3533\n",
      "Batch 269, Loss: 4.5618\n",
      "Batch 270, Loss: 4.5634\n",
      "Batch 271, Loss: 4.5543\n",
      "Batch 272, Loss: 4.5428\n",
      "Batch 273, Loss: 4.5202\n",
      "Batch 274, Loss: 4.4942\n",
      "Batch 275, Loss: 4.3673\n",
      "Batch 276, Loss: 4.6854\n",
      "Batch 277, Loss: 4.5510\n",
      "Batch 278, Loss: 4.5197\n",
      "Batch 279, Loss: 4.5163\n",
      "Batch 280, Loss: 4.4682\n",
      "Batch 281, Loss: 4.5380\n",
      "Batch 282, Loss: 4.5188\n",
      "Batch 283, Loss: 4.4874\n",
      "Batch 284, Loss: 4.4881\n",
      "Batch 285, Loss: 4.3372\n",
      "Batch 286, Loss: 4.5065\n",
      "Batch 287, Loss: 4.3876\n",
      "Batch 288, Loss: 4.5108\n",
      "Batch 289, Loss: 4.4954\n",
      "Batch 290, Loss: 4.4793\n",
      "Batch 291, Loss: 4.4955\n",
      "Batch 292, Loss: 4.6682\n",
      "Batch 293, Loss: 4.4218\n",
      "Batch 294, Loss: 4.4676\n",
      "Batch 295, Loss: 4.5365\n",
      "Batch 296, Loss: 4.3565\n",
      "Batch 297, Loss: 4.4984\n",
      "Batch 298, Loss: 4.4749\n",
      "Batch 299, Loss: 4.5150\n",
      "Batch 300, Loss: 4.4975\n",
      "Batch 301, Loss: 4.5074\n",
      "Batch 302, Loss: 4.4943\n",
      "Batch 303, Loss: 4.5331\n",
      "Batch 304, Loss: 4.4707\n",
      "Batch 305, Loss: 4.3291\n",
      "Batch 306, Loss: 4.4523\n",
      "Batch 307, Loss: 4.4594\n",
      "Batch 308, Loss: 4.4588\n",
      "Batch 309, Loss: 4.4532\n",
      "Batch 310, Loss: 4.2660\n",
      "Batch 311, Loss: 4.2712\n",
      "Batch 312, Loss: 4.4463\n",
      "Batch 313, Loss: 4.4399\n",
      "Batch 314, Loss: 4.4382\n",
      "Batch 315, Loss: 4.4353\n",
      "Batch 316, Loss: 4.4448\n",
      "Batch 317, Loss: 4.4451\n",
      "Batch 318, Loss: 4.4257\n",
      "Batch 319, Loss: 4.5326\n",
      "Batch 320, Loss: 4.4006\n",
      "Batch 321, Loss: 4.3879\n",
      "Batch 322, Loss: 4.3606\n",
      "Batch 323, Loss: 4.4178\n",
      "Batch 324, Loss: 4.3999\n",
      "Batch 325, Loss: 4.3660\n",
      "Batch 326, Loss: 4.3061\n",
      "Batch 327, Loss: 4.4026\n",
      "Batch 328, Loss: 4.3519\n",
      "Batch 329, Loss: 4.5020\n",
      "Batch 330, Loss: 4.3740\n",
      "Batch 331, Loss: 4.3537\n",
      "Batch 332, Loss: 4.3467\n",
      "Batch 333, Loss: 4.3938\n",
      "Batch 334, Loss: 4.3667\n",
      "Batch 335, Loss: 4.3749\n",
      "Batch 336, Loss: 4.3541\n",
      "Batch 337, Loss: 4.3418\n",
      "Batch 338, Loss: 4.3473\n",
      "Batch 339, Loss: 4.4099\n",
      "Batch 340, Loss: 4.3388\n",
      "Batch 341, Loss: 4.1764\n",
      "Batch 342, Loss: 4.3778\n",
      "Batch 343, Loss: 4.1632\n",
      "Batch 344, Loss: 4.3458\n",
      "Batch 345, Loss: 4.3490\n",
      "Batch 346, Loss: 4.3555\n",
      "Batch 347, Loss: 4.1059\n",
      "Batch 348, Loss: 4.4152\n",
      "Batch 349, Loss: 4.4291\n",
      "Batch 350, Loss: 4.3239\n",
      "Batch 351, Loss: 4.3367\n",
      "Batch 352, Loss: 4.3279\n",
      "Batch 353, Loss: 4.2030\n",
      "Batch 354, Loss: 4.1688\n",
      "Batch 355, Loss: 4.3982\n",
      "Batch 356, Loss: 4.3675\n",
      "Batch 357, Loss: 4.3415\n",
      "Batch 358, Loss: 4.3354\n",
      "Batch 359, Loss: 4.2215\n",
      "Batch 360, Loss: 4.3539\n",
      "Batch 361, Loss: 4.2860\n",
      "Batch 362, Loss: 4.1053\n",
      "Batch 363, Loss: 4.3479\n",
      "Batch 364, Loss: 4.2541\n",
      "Batch 365, Loss: 4.3455\n",
      "Batch 366, Loss: 4.2222\n",
      "Batch 367, Loss: 4.3562\n",
      "Batch 368, Loss: 4.3073\n",
      "Batch 369, Loss: 4.3412\n",
      "Batch 370, Loss: 4.3168\n",
      "Batch 371, Loss: 4.2118\n",
      "Batch 372, Loss: 4.2744\n",
      "Batch 373, Loss: 4.2935\n",
      "Batch 374, Loss: 4.2893\n",
      "Batch 375, Loss: 4.2240\n",
      "Batch 376, Loss: 4.2989\n",
      "Batch 377, Loss: 4.1328\n",
      "Batch 378, Loss: 4.0555\n",
      "Epoch 1, Average Loss: 5.0390\n",
      "Once there was a funs lived Tom day Cins. They wanted to see a sky doh his toys and thre dadit and scaret little per haneter rrable oring. One day, \n",
      "Fely, they couldn't go attestet. The boy felt not on the other and an ideaced each kuase, it was too excited and some long bect because when it had joend \n",
      "and made people came to want it would pother that he quicknings. He soon em. They made actitow down her mleed and startedized the red winde, feeling prated loved to \n",
      "help said there hadn't go to cother shudded to siat. The park. He all and Lily's to the doet itcving a oiling the tree and fared at the boy was \n",
      "very not like lots. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 4.2603\n",
      "Batch 2, Loss: 4.2674\n",
      "Batch 3, Loss: 4.2690\n",
      "Batch 4, Loss: 4.2615\n",
      "Batch 5, Loss: 4.2275\n",
      "Batch 6, Loss: 4.2451\n",
      "Batch 7, Loss: 4.3529\n",
      "Batch 8, Loss: 4.2454\n",
      "Batch 9, Loss: 4.2472\n",
      "Batch 10, Loss: 4.2187\n",
      "Batch 11, Loss: 4.2361\n",
      "Batch 12, Loss: 4.2407\n",
      "Batch 13, Loss: 4.2079\n",
      "Batch 14, Loss: 4.2035\n",
      "Batch 15, Loss: 4.2104\n",
      "Batch 16, Loss: 4.3640\n",
      "Batch 17, Loss: 4.2145\n",
      "Batch 18, Loss: 4.0556\n",
      "Batch 19, Loss: 4.2006\n",
      "Batch 20, Loss: 4.1180\n",
      "Batch 21, Loss: 4.3212\n",
      "Batch 22, Loss: 4.1978\n",
      "Batch 23, Loss: 4.2221\n",
      "Batch 24, Loss: 4.2379\n",
      "Batch 25, Loss: 4.0926\n",
      "Batch 26, Loss: 4.2295\n",
      "Batch 27, Loss: 4.2204\n",
      "Batch 28, Loss: 4.2391\n",
      "Batch 29, Loss: 4.1019\n",
      "Batch 30, Loss: 4.2098\n",
      "Batch 31, Loss: 4.0639\n",
      "Batch 32, Loss: 4.2027\n",
      "Batch 33, Loss: 4.2990\n",
      "Batch 34, Loss: 4.1550\n",
      "Batch 35, Loss: 4.1625\n",
      "Batch 36, Loss: 4.0380\n",
      "Batch 37, Loss: 3.9417\n",
      "Batch 38, Loss: 4.2263\n",
      "Batch 39, Loss: 4.2473\n",
      "Batch 40, Loss: 4.2040\n",
      "Batch 41, Loss: 4.1651\n",
      "Batch 42, Loss: 4.1735\n",
      "Batch 43, Loss: 4.0531\n",
      "Batch 44, Loss: 4.0790\n",
      "Batch 45, Loss: 4.1879\n",
      "Batch 46, Loss: 4.1446\n",
      "Batch 47, Loss: 4.1759\n",
      "Batch 48, Loss: 4.1330\n",
      "Batch 49, Loss: 4.0914\n",
      "Batch 50, Loss: 4.1270\n",
      "Batch 51, Loss: 4.1426\n",
      "Batch 52, Loss: 4.1573\n",
      "Batch 53, Loss: 4.1232\n",
      "Batch 54, Loss: 4.1487\n",
      "Batch 55, Loss: 4.0947\n",
      "Batch 56, Loss: 4.1117\n",
      "Batch 57, Loss: 3.9539\n",
      "Batch 58, Loss: 3.9970\n",
      "Batch 59, Loss: 4.1075\n",
      "Batch 60, Loss: 4.2318\n",
      "Batch 61, Loss: 3.8972\n",
      "Batch 62, Loss: 4.1366\n",
      "Batch 63, Loss: 4.1367\n",
      "Batch 64, Loss: 4.1080\n",
      "Batch 65, Loss: 4.0949\n",
      "Batch 66, Loss: 4.0509\n",
      "Batch 67, Loss: 4.0333\n",
      "Batch 68, Loss: 3.9900\n",
      "Batch 69, Loss: 4.1294\n",
      "Batch 70, Loss: 4.0806\n",
      "Batch 71, Loss: 4.0635\n",
      "Batch 72, Loss: 4.0828\n",
      "Batch 73, Loss: 4.1025\n",
      "Batch 74, Loss: 3.9260\n",
      "Batch 75, Loss: 4.0872\n",
      "Batch 76, Loss: 4.0585\n",
      "Batch 77, Loss: 4.0460\n",
      "Batch 78, Loss: 4.0576\n",
      "Batch 79, Loss: 3.8421\n",
      "Batch 80, Loss: 3.9516\n",
      "Batch 81, Loss: 3.9155\n",
      "Batch 82, Loss: 4.0753\n",
      "Batch 83, Loss: 3.9957\n",
      "Batch 84, Loss: 4.0394\n",
      "Batch 85, Loss: 4.0039\n",
      "Batch 86, Loss: 4.0791\n",
      "Batch 87, Loss: 4.1000\n",
      "Batch 88, Loss: 4.0266\n",
      "Batch 89, Loss: 4.0150\n",
      "Batch 90, Loss: 4.0111\n",
      "Batch 91, Loss: 3.8763\n",
      "Batch 92, Loss: 4.0483\n",
      "Batch 93, Loss: 4.0317\n",
      "Batch 94, Loss: 4.0963\n",
      "Batch 95, Loss: 3.9454\n",
      "Batch 96, Loss: 3.8189\n",
      "Batch 97, Loss: 3.8750\n",
      "Batch 98, Loss: 4.0337\n",
      "Batch 99, Loss: 4.0287\n",
      "Batch 100, Loss: 3.9005\n",
      "Batch 101, Loss: 4.0297\n",
      "Batch 102, Loss: 4.0101\n",
      "Batch 103, Loss: 4.0061\n",
      "Batch 104, Loss: 3.9854\n",
      "Batch 105, Loss: 3.9845\n",
      "Batch 106, Loss: 4.0624\n",
      "Batch 107, Loss: 4.0027\n",
      "Batch 108, Loss: 4.0179\n",
      "Batch 109, Loss: 4.0499\n",
      "Batch 110, Loss: 4.0036\n",
      "Batch 111, Loss: 3.8453\n",
      "Batch 112, Loss: 4.0196\n",
      "Batch 113, Loss: 3.9840\n",
      "Batch 114, Loss: 3.8076\n",
      "Batch 115, Loss: 3.7774\n",
      "Batch 116, Loss: 3.9775\n",
      "Batch 117, Loss: 4.0223\n",
      "Batch 118, Loss: 4.0477\n",
      "Batch 119, Loss: 4.0211\n",
      "Batch 120, Loss: 3.9691\n",
      "Batch 121, Loss: 4.0035\n",
      "Batch 122, Loss: 4.0486\n",
      "Batch 123, Loss: 3.9960\n",
      "Batch 124, Loss: 4.0364\n",
      "Batch 125, Loss: 3.9964\n",
      "Batch 126, Loss: 3.8514\n",
      "Batch 127, Loss: 3.8261\n",
      "Batch 128, Loss: 3.9951\n",
      "Batch 129, Loss: 3.8095\n",
      "Batch 130, Loss: 3.8939\n",
      "Batch 131, Loss: 3.9977\n",
      "Batch 132, Loss: 3.8058\n",
      "Batch 133, Loss: 3.9874\n",
      "Batch 134, Loss: 3.7810\n",
      "Batch 135, Loss: 3.9875\n",
      "Batch 136, Loss: 3.8976\n",
      "Batch 137, Loss: 3.9702\n",
      "Batch 138, Loss: 3.9560\n",
      "Batch 139, Loss: 3.8880\n",
      "Batch 140, Loss: 3.8118\n",
      "Batch 141, Loss: 3.7373\n",
      "Batch 142, Loss: 3.9485\n",
      "Batch 143, Loss: 3.8725\n",
      "Batch 144, Loss: 3.9615\n",
      "Batch 145, Loss: 3.9453\n",
      "Batch 146, Loss: 3.9065\n",
      "Batch 147, Loss: 3.9416\n",
      "Batch 148, Loss: 3.8986\n",
      "Batch 149, Loss: 3.9163\n",
      "Batch 150, Loss: 3.9132\n",
      "Batch 151, Loss: 3.9146\n",
      "Batch 152, Loss: 3.8999\n",
      "Batch 153, Loss: 3.7491\n",
      "Batch 154, Loss: 3.9220\n",
      "Batch 155, Loss: 3.9271\n",
      "Batch 156, Loss: 3.8998\n",
      "Batch 157, Loss: 3.9072\n",
      "Batch 158, Loss: 4.0224\n",
      "Batch 159, Loss: 3.7913\n",
      "Batch 160, Loss: 3.8772\n",
      "Batch 161, Loss: 3.8856\n",
      "Batch 162, Loss: 3.8915\n",
      "Batch 163, Loss: 3.7941\n",
      "Batch 164, Loss: 3.7293\n",
      "Batch 165, Loss: 3.8812\n",
      "Batch 166, Loss: 3.8100\n",
      "Batch 167, Loss: 3.8792\n",
      "Batch 168, Loss: 3.9199\n",
      "Batch 169, Loss: 3.8465\n",
      "Batch 170, Loss: 3.8420\n",
      "Batch 171, Loss: 3.8723\n",
      "Batch 172, Loss: 3.8901\n",
      "Batch 173, Loss: 3.7520\n",
      "Batch 174, Loss: 3.7123\n",
      "Batch 175, Loss: 3.8586\n",
      "Batch 176, Loss: 3.8605\n",
      "Batch 177, Loss: 3.8447\n",
      "Batch 178, Loss: 3.6834\n",
      "Batch 179, Loss: 3.8957\n",
      "Batch 180, Loss: 3.8897\n",
      "Batch 181, Loss: 3.7868\n",
      "Batch 182, Loss: 3.7527\n",
      "Batch 183, Loss: 3.8371\n",
      "Batch 184, Loss: 3.6488\n",
      "Batch 185, Loss: 3.8711\n",
      "Batch 186, Loss: 3.6653\n",
      "Batch 187, Loss: 3.8393\n",
      "Batch 188, Loss: 3.8186\n",
      "Batch 189, Loss: 3.8350\n",
      "Batch 190, Loss: 3.8648\n",
      "Batch 191, Loss: 3.7990\n",
      "Batch 192, Loss: 3.7443\n",
      "Batch 193, Loss: 3.6493\n",
      "Batch 194, Loss: 3.7147\n",
      "Batch 195, Loss: 3.7294\n",
      "Batch 196, Loss: 3.6934\n",
      "Batch 197, Loss: 3.8343\n",
      "Batch 198, Loss: 3.8853\n",
      "Batch 199, Loss: 3.8796\n",
      "Batch 200, Loss: 3.8670\n",
      "The time the trah in on was very at yurt in a ballss,. He was so excited he rans can raurack and hake asked! \"Look, Mia, lety if Mom. \"You' \n",
      "can have tummy and hung saw time to play he was very not prethns old. He porseidoved that loved her mag and says, \"Mom like songed with their Mel, \"I \n",
      "can help you'm hoffhers,\". Gray blay and said, \"But I want a nice and take cry, book and said, \"I watch, this? Lide all started to Ben gave it was \n",
      "very beautiful and said, \"Yes, but I'm sorry and like?\" She said, \"Ben, shout Mom use are so cool. I'm den the beautiful too writ on his mom took than \n",
      "who had bee,!\" Mrusted in the cheed. They stopped saw the true thanked. Sogether heremry. Her sroy. \"What's mom, weÂ had an moutepasoward turns. other dog bel down.\" The best \n",
      "hands. He was waited and they both cheous. He picked in the new new cit of joditnallave the mouse started to them. He was amazed. It had a very proud \n",
      "the play playing with the cars and wishs. They had cot whol. \"Hraards again. She doak pusry. Mice. Look it! Youar and said \"Gish, the best and felt to the \n",
      "worrious, \"Wow, I use can frover ors, Sam unak your new and liw everyone Ben and was glippostious in a to helping up shre and shinadieadiepty sapiendotbinrow feeling she had \n",
      "sinp mile said. \"You can scines.\" \"I'm this!\" But replied overged around the little bird and said, \"Okay, Mom said. He was so!\" Jimowed that of winder said,Bummy and his \n",
      "mom gave Mom fes kind under look, smil the wish learned to heal boy go atet.\" They felt happy to seeget up, feeling happy. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 30.06 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 3.8231\n",
      "Batch 202, Loss: 3.8654\n",
      "Batch 203, Loss: 3.7782\n",
      "Batch 204, Loss: 3.8603\n",
      "Batch 205, Loss: 3.8374\n",
      "Batch 206, Loss: 3.6907\n",
      "Batch 207, Loss: 3.8620\n",
      "Batch 208, Loss: 3.8528\n",
      "Batch 209, Loss: 3.8856\n",
      "Batch 210, Loss: 3.8209\n",
      "Batch 211, Loss: 3.9586\n",
      "Batch 212, Loss: 3.8998\n",
      "Batch 213, Loss: 3.6994\n",
      "Batch 214, Loss: 3.8536\n",
      "Batch 215, Loss: 3.8645\n",
      "Batch 216, Loss: 3.8237\n",
      "Batch 217, Loss: 3.8492\n",
      "Batch 218, Loss: 3.8360\n",
      "Batch 219, Loss: 3.8654\n",
      "Batch 220, Loss: 3.7951\n",
      "Batch 221, Loss: 3.8897\n",
      "Batch 222, Loss: 3.8148\n",
      "Batch 223, Loss: 3.6609\n",
      "Batch 224, Loss: 3.8053\n",
      "Batch 225, Loss: 3.8251\n",
      "Batch 226, Loss: 3.7882\n",
      "Batch 227, Loss: 3.8127\n",
      "Batch 228, Loss: 3.7806\n",
      "Batch 229, Loss: 3.7953\n",
      "Batch 230, Loss: 3.7742\n",
      "Batch 231, Loss: 3.6394\n",
      "Batch 232, Loss: 3.8325\n",
      "Batch 233, Loss: 3.8218\n",
      "Batch 234, Loss: 3.7644\n",
      "Batch 235, Loss: 3.7584\n",
      "Batch 236, Loss: 3.7500\n",
      "Batch 237, Loss: 3.7841\n",
      "Batch 238, Loss: 3.8118\n",
      "Batch 239, Loss: 3.7616\n",
      "Batch 240, Loss: 3.7796\n",
      "Batch 241, Loss: 3.7542\n",
      "Batch 242, Loss: 3.7332\n",
      "Batch 243, Loss: 3.7626\n",
      "Batch 244, Loss: 3.7662\n",
      "Batch 245, Loss: 3.7618\n",
      "Batch 246, Loss: 3.7607\n",
      "Batch 247, Loss: 3.7747\n",
      "Batch 248, Loss: 3.7103\n",
      "Batch 249, Loss: 3.7151\n",
      "Batch 250, Loss: 3.6667\n",
      "Batch 251, Loss: 3.7369\n",
      "Batch 252, Loss: 3.7475\n",
      "Batch 253, Loss: 3.7312\n",
      "Batch 254, Loss: 3.7077\n",
      "Batch 255, Loss: 3.7284\n",
      "Batch 256, Loss: 3.7089\n",
      "Batch 257, Loss: 3.7085\n",
      "Batch 258, Loss: 3.6823\n",
      "Batch 259, Loss: 3.7154\n",
      "Batch 260, Loss: 3.7111\n",
      "Batch 261, Loss: 3.6971\n",
      "Batch 262, Loss: 3.7450\n",
      "Batch 263, Loss: 3.6974\n",
      "Batch 264, Loss: 3.5973\n",
      "Batch 265, Loss: 3.7041\n",
      "Batch 266, Loss: 3.7206\n",
      "Batch 267, Loss: 3.6897\n",
      "Batch 268, Loss: 3.7235\n",
      "Batch 269, Loss: 3.7223\n",
      "Batch 270, Loss: 3.7230\n",
      "Batch 271, Loss: 3.6310\n",
      "Batch 272, Loss: 3.6623\n",
      "Batch 273, Loss: 3.6999\n",
      "Batch 274, Loss: 3.7061\n",
      "Batch 275, Loss: 3.7464\n",
      "Batch 276, Loss: 3.7236\n",
      "Batch 277, Loss: 3.6117\n",
      "Batch 278, Loss: 3.6753\n",
      "Batch 279, Loss: 3.6904\n",
      "Batch 280, Loss: 3.6928\n",
      "Batch 281, Loss: 3.6811\n",
      "Batch 282, Loss: 3.6820\n",
      "Batch 283, Loss: 3.6860\n",
      "Batch 284, Loss: 3.7221\n",
      "Batch 285, Loss: 3.6877\n",
      "Batch 286, Loss: 3.7025\n",
      "Batch 287, Loss: 3.7159\n",
      "Batch 288, Loss: 3.7117\n",
      "Batch 289, Loss: 3.7022\n",
      "Batch 290, Loss: 3.7094\n",
      "Batch 291, Loss: 3.6804\n",
      "Batch 292, Loss: 3.6631\n",
      "Batch 293, Loss: 3.6419\n",
      "Batch 294, Loss: 3.6179\n",
      "Batch 295, Loss: 3.6518\n",
      "Batch 296, Loss: 3.6758\n",
      "Batch 297, Loss: 3.6629\n",
      "Batch 298, Loss: 3.6003\n",
      "Batch 299, Loss: 3.6605\n",
      "Batch 300, Loss: 3.6660\n",
      "Batch 301, Loss: 3.5989\n",
      "Batch 302, Loss: 3.5904\n",
      "Batch 303, Loss: 3.6522\n",
      "Batch 304, Loss: 3.6112\n",
      "Batch 305, Loss: 3.6907\n",
      "Batch 306, Loss: 3.5980\n",
      "Batch 307, Loss: 3.5037\n",
      "Batch 308, Loss: 3.6416\n",
      "Batch 309, Loss: 3.6151\n",
      "Batch 310, Loss: 3.6408\n",
      "Batch 311, Loss: 3.6560\n",
      "Batch 312, Loss: 3.6339\n",
      "Batch 313, Loss: 3.6100\n",
      "Batch 314, Loss: 3.6174\n",
      "Batch 315, Loss: 3.6173\n",
      "Batch 316, Loss: 3.6724\n",
      "Batch 317, Loss: 3.6555\n",
      "Batch 318, Loss: 3.6142\n",
      "Batch 319, Loss: 3.6462\n",
      "Batch 320, Loss: 3.6571\n",
      "Batch 321, Loss: 3.5510\n",
      "Batch 322, Loss: 3.5580\n",
      "Batch 323, Loss: 3.6420\n",
      "Batch 324, Loss: 3.6472\n",
      "Batch 325, Loss: 3.6448\n",
      "Batch 326, Loss: 3.6265\n",
      "Batch 327, Loss: 3.6347\n",
      "Batch 328, Loss: 3.6245\n",
      "Batch 329, Loss: 3.6306\n",
      "Batch 330, Loss: 3.5825\n",
      "Batch 331, Loss: 3.5340\n",
      "Batch 332, Loss: 3.5813\n",
      "Batch 333, Loss: 3.6183\n",
      "Batch 334, Loss: 3.6101\n",
      "Batch 335, Loss: 3.6271\n",
      "Batch 336, Loss: 3.6161\n",
      "Batch 337, Loss: 3.4857\n",
      "Batch 338, Loss: 3.6130\n",
      "Batch 339, Loss: 3.5899\n",
      "Batch 340, Loss: 3.5713\n",
      "Batch 341, Loss: 3.6194\n",
      "Batch 342, Loss: 3.5783\n",
      "Batch 343, Loss: 3.5099\n",
      "Batch 344, Loss: 3.6129\n",
      "Batch 345, Loss: 3.6045\n",
      "Batch 346, Loss: 3.6148\n",
      "Batch 347, Loss: 3.5838\n",
      "Batch 348, Loss: 3.5948\n",
      "Batch 349, Loss: 3.5596\n",
      "Batch 350, Loss: 3.5509\n",
      "Batch 351, Loss: 3.6011\n",
      "Batch 352, Loss: 3.5427\n",
      "Batch 353, Loss: 3.5228\n",
      "Batch 354, Loss: 3.6284\n",
      "Batch 355, Loss: 3.4576\n",
      "Batch 356, Loss: 3.5772\n",
      "Batch 357, Loss: 3.6359\n",
      "Batch 358, Loss: 3.5967\n",
      "Batch 359, Loss: 3.5428\n",
      "Batch 360, Loss: 3.5408\n",
      "Batch 361, Loss: 3.5736\n",
      "Batch 362, Loss: 3.5508\n",
      "Batch 363, Loss: 3.5807\n",
      "Batch 364, Loss: 3.5546\n",
      "Batch 365, Loss: 3.6274\n",
      "Batch 366, Loss: 3.5764\n",
      "Batch 367, Loss: 3.5580\n",
      "Batch 368, Loss: 3.4849\n",
      "Batch 369, Loss: 3.5967\n",
      "Batch 370, Loss: 3.5391\n",
      "Batch 371, Loss: 3.6384\n",
      "Batch 372, Loss: 3.5644\n",
      "Batch 373, Loss: 3.6236\n",
      "Batch 374, Loss: 3.5809\n",
      "Batch 375, Loss: 3.7032\n",
      "Batch 376, Loss: 3.6758\n",
      "Batch 377, Loss: 3.7038\n",
      "Batch 378, Loss: 3.5904\n",
      "Epoch 2, Average Loss: 3.8494\n",
      "Once Once upon there was friend man senny. Aakday liked to go to go and scha. Jack was so many dressive a new booble shape. One day, Cary's sparking. Allached \n",
      "the a caesss. The faacleroty. The buse was nonestgassed to the menty. It wanted to pick up this sir. â€œPund forhank you, the skives and when it's all gicorge. I \n",
      "needrot can be even stirecar you? Low, so strong!\" Just then soon they got back and conls. Thereger, \"Fiecalently, whatâ€™fasy and I have something special joblory. The wet and courdrion \n",
      "knows you. Pug muman forre to your thing so important to play a burby. They would do. They tried to me for other lon't try it! Lecesosting! The r, the \n",
      "rain regobed back to stayhere and said, \"I'm zed foring mif oe you. You never a shark.\" Rolling just trnonenty goom, feeling funs. They had askes and that for to \n",
      "sofent. Tim thought ask gund and scet and then enough she had the gamplore. Sue agried their teisby watched each other. He smiled and couldn't waited down. Jack hen around \n",
      "Sam listensiffounoringet. Lily ran around her yy, who didn't listen off in the where after. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 3.6568\n",
      "Batch 2, Loss: 3.6331\n",
      "Batch 3, Loss: 3.7235\n",
      "Batch 4, Loss: 3.7501\n",
      "Batch 5, Loss: 3.5509\n",
      "Batch 6, Loss: 3.7682\n",
      "Batch 7, Loss: 3.6691\n",
      "Batch 8, Loss: 3.6910\n",
      "Batch 9, Loss: 3.6771\n",
      "Batch 10, Loss: 3.5947\n",
      "Batch 11, Loss: 3.5430\n",
      "Batch 12, Loss: 3.5407\n",
      "Batch 13, Loss: 3.6316\n",
      "Batch 14, Loss: 3.6241\n",
      "Batch 15, Loss: 3.4798\n",
      "Batch 16, Loss: 3.5107\n",
      "Batch 17, Loss: 3.6157\n",
      "Batch 18, Loss: 3.5236\n",
      "Batch 19, Loss: 3.6093\n",
      "Batch 20, Loss: 3.5909\n",
      "Batch 21, Loss: 3.4190\n",
      "Batch 22, Loss: 3.5716\n",
      "Batch 23, Loss: 3.5170\n",
      "Batch 24, Loss: 3.5899\n",
      "Batch 25, Loss: 3.6114\n",
      "Batch 26, Loss: 3.5944\n",
      "Batch 27, Loss: 3.5455\n",
      "Batch 28, Loss: 3.5823\n",
      "Batch 29, Loss: 3.5756\n",
      "Batch 30, Loss: 3.5650\n",
      "Batch 31, Loss: 3.5412\n",
      "Batch 32, Loss: 3.5140\n",
      "Batch 33, Loss: 3.5678\n",
      "Batch 34, Loss: 3.4720\n",
      "Batch 35, Loss: 3.5012\n",
      "Batch 36, Loss: 3.5042\n",
      "Batch 37, Loss: 3.4933\n",
      "Batch 38, Loss: 3.5045\n",
      "Batch 39, Loss: 3.5140\n",
      "Batch 40, Loss: 3.3801\n",
      "Batch 41, Loss: 3.5017\n",
      "Batch 42, Loss: 3.5411\n",
      "Batch 43, Loss: 3.5068\n",
      "Batch 44, Loss: 3.3512\n",
      "Batch 45, Loss: 3.5018\n",
      "Batch 46, Loss: 3.5019\n",
      "Batch 47, Loss: 3.5045\n",
      "Batch 48, Loss: 3.5029\n",
      "Batch 49, Loss: 3.3560\n",
      "Batch 50, Loss: 3.4868\n",
      "Batch 51, Loss: 3.4883\n",
      "Batch 52, Loss: 3.3942\n",
      "Batch 53, Loss: 3.5247\n",
      "Batch 54, Loss: 3.4070\n",
      "Batch 55, Loss: 3.3934\n",
      "Batch 56, Loss: 3.4652\n",
      "Batch 57, Loss: 3.5162\n",
      "Batch 58, Loss: 3.4727\n",
      "Batch 59, Loss: 3.3819\n",
      "Batch 60, Loss: 3.3210\n",
      "Batch 61, Loss: 3.3980\n",
      "Batch 62, Loss: 3.3734\n",
      "Batch 63, Loss: 3.4201\n",
      "Batch 64, Loss: 3.4832\n",
      "Batch 65, Loss: 3.5199\n",
      "Batch 66, Loss: 3.5005\n",
      "Batch 67, Loss: 3.4144\n",
      "Batch 68, Loss: 3.4982\n",
      "Batch 69, Loss: 3.4767\n",
      "Batch 70, Loss: 3.4915\n",
      "Batch 71, Loss: 3.4955\n",
      "Batch 72, Loss: 3.4543\n",
      "Batch 73, Loss: 3.5065\n",
      "Batch 74, Loss: 3.4481\n",
      "Batch 75, Loss: 3.4665\n",
      "Batch 76, Loss: 3.4794\n",
      "Batch 77, Loss: 3.4218\n",
      "Batch 78, Loss: 3.4655\n",
      "Batch 79, Loss: 3.4273\n",
      "Batch 80, Loss: 3.3925\n",
      "Batch 81, Loss: 3.3953\n",
      "Batch 82, Loss: 3.4443\n",
      "Batch 83, Loss: 3.4700\n",
      "Batch 84, Loss: 3.4416\n",
      "Batch 85, Loss: 3.4417\n",
      "Batch 86, Loss: 3.4282\n",
      "Batch 87, Loss: 3.4341\n",
      "Batch 88, Loss: 3.4649\n",
      "Batch 89, Loss: 3.4291\n",
      "Batch 90, Loss: 3.3650\n",
      "Batch 91, Loss: 3.4035\n",
      "Batch 92, Loss: 3.4193\n",
      "Batch 93, Loss: 3.4181\n",
      "Batch 94, Loss: 3.3983\n",
      "Batch 95, Loss: 3.3167\n",
      "Batch 96, Loss: 3.3387\n",
      "Batch 97, Loss: 3.4595\n",
      "Batch 98, Loss: 3.3962\n",
      "Batch 99, Loss: 3.3910\n",
      "Batch 100, Loss: 3.2543\n",
      "Batch 101, Loss: 3.4271\n",
      "Batch 102, Loss: 3.4501\n",
      "Batch 103, Loss: 3.3082\n",
      "Batch 104, Loss: 3.3585\n",
      "Batch 105, Loss: 3.3004\n",
      "Batch 106, Loss: 3.4046\n",
      "Batch 107, Loss: 3.3979\n",
      "Batch 108, Loss: 3.4300\n",
      "Batch 109, Loss: 3.3726\n",
      "Batch 110, Loss: 3.4239\n",
      "Batch 111, Loss: 3.3487\n",
      "Batch 112, Loss: 3.4521\n",
      "Batch 113, Loss: 3.3349\n",
      "Batch 114, Loss: 3.2642\n",
      "Batch 115, Loss: 3.4473\n",
      "Batch 116, Loss: 3.4699\n",
      "Batch 117, Loss: 3.4308\n",
      "Batch 118, Loss: 3.2527\n",
      "Batch 119, Loss: 3.4414\n",
      "Batch 120, Loss: 3.4122\n",
      "Batch 121, Loss: 3.3699\n",
      "Batch 122, Loss: 3.4409\n",
      "Batch 123, Loss: 3.4241\n",
      "Batch 124, Loss: 3.4042\n",
      "Batch 125, Loss: 3.3181\n",
      "Batch 126, Loss: 3.4316\n",
      "Batch 127, Loss: 3.3995\n",
      "Batch 128, Loss: 3.2694\n",
      "Batch 129, Loss: 3.4081\n",
      "Batch 130, Loss: 3.4034\n",
      "Batch 131, Loss: 3.4193\n",
      "Batch 132, Loss: 3.1950\n",
      "Batch 133, Loss: 3.4480\n",
      "Batch 134, Loss: 3.4189\n",
      "Batch 135, Loss: 3.3910\n",
      "Batch 136, Loss: 3.4053\n",
      "Batch 137, Loss: 3.5141\n",
      "Batch 138, Loss: 3.4296\n",
      "Batch 139, Loss: 3.4566\n",
      "Batch 140, Loss: 3.3724\n",
      "Batch 141, Loss: 3.4386\n",
      "Batch 142, Loss: 3.4121\n",
      "Batch 143, Loss: 3.4346\n",
      "Batch 144, Loss: 3.3978\n",
      "Batch 145, Loss: 3.3608\n",
      "Batch 146, Loss: 3.4299\n",
      "Batch 147, Loss: 3.3142\n",
      "Batch 148, Loss: 3.2404\n",
      "Batch 149, Loss: 3.3684\n",
      "Batch 150, Loss: 3.3670\n",
      "Batch 151, Loss: 3.4026\n",
      "Batch 152, Loss: 3.4089\n",
      "Batch 153, Loss: 3.3915\n",
      "Batch 154, Loss: 3.3499\n",
      "Batch 155, Loss: 3.3840\n",
      "Batch 156, Loss: 3.3720\n",
      "Batch 157, Loss: 3.3396\n",
      "Batch 158, Loss: 3.3773\n",
      "Batch 159, Loss: 3.3130\n",
      "Batch 160, Loss: 3.3589\n",
      "Batch 161, Loss: 3.3701\n",
      "Batch 162, Loss: 3.2812\n",
      "Batch 163, Loss: 3.3377\n",
      "Batch 164, Loss: 3.2619\n",
      "Batch 165, Loss: 3.3747\n",
      "Batch 166, Loss: 3.3689\n",
      "Batch 167, Loss: 3.1722\n",
      "Batch 168, Loss: 3.3195\n",
      "Batch 169, Loss: 3.3877\n",
      "Batch 170, Loss: 3.2072\n",
      "Batch 171, Loss: 3.3468\n",
      "Batch 172, Loss: 3.2999\n",
      "Batch 173, Loss: 3.3555\n",
      "Batch 174, Loss: 3.3468\n",
      "Batch 175, Loss: 3.2886\n",
      "Batch 176, Loss: 3.3343\n",
      "Batch 177, Loss: 3.3515\n",
      "Batch 178, Loss: 3.3652\n",
      "Batch 179, Loss: 3.3017\n",
      "Batch 180, Loss: 3.3379\n",
      "Batch 181, Loss: 3.1771\n",
      "Batch 182, Loss: 3.3493\n",
      "Batch 183, Loss: 3.2979\n",
      "Batch 184, Loss: 3.2861\n",
      "Batch 185, Loss: 3.3144\n",
      "Batch 186, Loss: 3.3216\n",
      "Batch 187, Loss: 3.3285\n",
      "Batch 188, Loss: 3.3232\n",
      "Batch 189, Loss: 3.3028\n",
      "Batch 190, Loss: 3.3207\n",
      "Batch 191, Loss: 3.3287\n",
      "Batch 192, Loss: 3.3520\n",
      "Batch 193, Loss: 3.3132\n",
      "Batch 194, Loss: 3.3233\n",
      "Batch 195, Loss: 3.3087\n",
      "Batch 196, Loss: 3.2098\n",
      "Batch 197, Loss: 3.3009\n",
      "Batch 198, Loss: 3.2350\n",
      "Batch 199, Loss: 3.3119\n",
      "Batch 200, Loss: 3.3050\n",
      "Once there was a big girl named Kous tower with a walk tick because Saraing. One day, Ma picked up readly flew outside and whaisit up in the warginy and \n",
      "then climbed and dad foot pap. \"I think, little boy came,\" Lily replied, \"No, let's try up and smiler. Jake,\" when the park, stood and scy be a stick away \n",
      "from a polate with the coll bes home.\". It taughter started it to a cup read there and said, \"Pace, lungy. I a fellh. I'm slease is de the mind \n",
      "little girl or knew her in the bake ever apping meandy for them in dels without's bre. Chal that eriendsone, whilenty and Zow, with it to shows, and tople. Tom \n",
      "was so excited when she was so she decided to doing to always flemble. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 25.21 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 3.3183\n",
      "Batch 202, Loss: 3.3073\n",
      "Batch 203, Loss: 3.3220\n",
      "Batch 204, Loss: 3.3032\n",
      "Batch 205, Loss: 3.2211\n",
      "Batch 206, Loss: 3.2023\n",
      "Batch 207, Loss: 3.1526\n",
      "Batch 208, Loss: 3.3016\n",
      "Batch 209, Loss: 3.2979\n",
      "Batch 210, Loss: 3.3457\n",
      "Batch 211, Loss: 3.3154\n",
      "Batch 212, Loss: 3.3014\n",
      "Batch 213, Loss: 3.2826\n",
      "Batch 214, Loss: 3.3077\n",
      "Batch 215, Loss: 3.3025\n",
      "Batch 216, Loss: 3.1547\n",
      "Batch 217, Loss: 3.2822\n",
      "Batch 218, Loss: 3.2620\n",
      "Batch 219, Loss: 3.2990\n",
      "Batch 220, Loss: 3.2607\n",
      "Batch 221, Loss: 3.3002\n",
      "Batch 222, Loss: 3.2591\n",
      "Batch 223, Loss: 3.2386\n",
      "Batch 224, Loss: 3.2564\n",
      "Batch 225, Loss: 3.2757\n",
      "Batch 226, Loss: 3.2857\n",
      "Batch 227, Loss: 3.2312\n",
      "Batch 228, Loss: 3.1882\n",
      "Batch 229, Loss: 3.2911\n",
      "Batch 230, Loss: 3.2794\n",
      "Batch 231, Loss: 3.2566\n",
      "Batch 232, Loss: 3.2544\n",
      "Batch 233, Loss: 3.2935\n",
      "Batch 234, Loss: 3.2685\n",
      "Batch 235, Loss: 3.2271\n",
      "Batch 236, Loss: 3.2149\n",
      "Batch 237, Loss: 3.2053\n",
      "Batch 238, Loss: 3.2887\n",
      "Batch 239, Loss: 3.2480\n",
      "Batch 240, Loss: 3.3001\n",
      "Batch 241, Loss: 3.2313\n",
      "Batch 242, Loss: 3.2656\n",
      "Batch 243, Loss: 3.2421\n",
      "Batch 244, Loss: 3.1832\n",
      "Batch 245, Loss: 3.2247\n",
      "Batch 246, Loss: 3.2434\n",
      "Batch 247, Loss: 3.2805\n",
      "Batch 248, Loss: 3.2476\n",
      "Batch 249, Loss: 3.2579\n",
      "Batch 250, Loss: 3.2757\n",
      "Batch 251, Loss: 3.2520\n",
      "Batch 252, Loss: 3.2460\n",
      "Batch 253, Loss: 3.2386\n",
      "Batch 254, Loss: 3.2489\n",
      "Batch 255, Loss: 3.2505\n",
      "Batch 256, Loss: 3.1948\n",
      "Batch 257, Loss: 3.2425\n",
      "Batch 258, Loss: 3.2323\n",
      "Batch 259, Loss: 3.2351\n",
      "Batch 260, Loss: 3.2257\n",
      "Batch 261, Loss: 3.2129\n",
      "Batch 262, Loss: 3.2008\n",
      "Batch 263, Loss: 3.2022\n",
      "Batch 264, Loss: 3.2414\n",
      "Batch 265, Loss: 3.1839\n",
      "Batch 266, Loss: 3.1978\n",
      "Batch 267, Loss: 3.1574\n",
      "Batch 268, Loss: 3.1805\n",
      "Batch 269, Loss: 3.2430\n",
      "Batch 270, Loss: 3.2324\n",
      "Batch 271, Loss: 3.0992\n",
      "Batch 272, Loss: 3.1980\n",
      "Batch 273, Loss: 3.1281\n",
      "Batch 274, Loss: 3.2664\n",
      "Batch 275, Loss: 3.1028\n",
      "Batch 276, Loss: 3.2552\n",
      "Batch 277, Loss: 3.2456\n",
      "Batch 278, Loss: 3.0919\n",
      "Batch 279, Loss: 3.2783\n",
      "Batch 280, Loss: 3.2309\n",
      "Batch 281, Loss: 3.2501\n",
      "Batch 282, Loss: 3.1096\n",
      "Batch 283, Loss: 3.2382\n",
      "Batch 284, Loss: 3.2483\n",
      "Batch 285, Loss: 3.0207\n",
      "Batch 286, Loss: 3.2210\n",
      "Batch 287, Loss: 3.2194\n",
      "Batch 288, Loss: 3.2187\n",
      "Batch 289, Loss: 3.1876\n",
      "Batch 290, Loss: 3.2286\n",
      "Batch 291, Loss: 3.1092\n",
      "Batch 292, Loss: 3.2241\n",
      "Batch 293, Loss: 3.1353\n",
      "Batch 294, Loss: 3.1939\n",
      "Batch 295, Loss: 3.1925\n",
      "Batch 296, Loss: 3.1902\n",
      "Batch 297, Loss: 3.1618\n",
      "Batch 298, Loss: 3.1666\n",
      "Batch 299, Loss: 3.1485\n",
      "Batch 300, Loss: 3.1691\n",
      "Batch 301, Loss: 3.1333\n",
      "Batch 302, Loss: 3.0979\n",
      "Batch 303, Loss: 3.1665\n",
      "Batch 304, Loss: 3.0497\n",
      "Batch 305, Loss: 3.1928\n",
      "Batch 306, Loss: 3.1368\n",
      "Batch 307, Loss: 3.1096\n",
      "Batch 308, Loss: 3.2431\n",
      "Batch 309, Loss: 3.1822\n",
      "Batch 310, Loss: 3.1273\n",
      "Batch 311, Loss: 3.1989\n",
      "Batch 312, Loss: 3.0496\n",
      "Batch 313, Loss: 3.1101\n",
      "Batch 314, Loss: 3.1669\n",
      "Batch 315, Loss: 3.2073\n",
      "Batch 316, Loss: 3.1766\n",
      "Batch 317, Loss: 3.1064\n",
      "Batch 318, Loss: 3.1601\n",
      "Batch 319, Loss: 3.1851\n",
      "Batch 320, Loss: 3.1106\n",
      "Batch 321, Loss: 3.0527\n",
      "Batch 322, Loss: 3.2020\n",
      "Batch 323, Loss: 3.1610\n",
      "Batch 324, Loss: 3.1673\n",
      "Batch 325, Loss: 3.1680\n",
      "Batch 326, Loss: 3.0235\n",
      "Batch 327, Loss: 3.0921\n",
      "Batch 328, Loss: 3.1767\n",
      "Batch 329, Loss: 2.9888\n",
      "Batch 330, Loss: 3.2039\n",
      "Batch 331, Loss: 3.1441\n",
      "Batch 332, Loss: 3.0294\n",
      "Batch 333, Loss: 3.1534\n",
      "Batch 334, Loss: 3.0705\n",
      "Batch 335, Loss: 3.1759\n",
      "Batch 336, Loss: 3.1514\n",
      "Batch 337, Loss: 3.0764\n",
      "Batch 338, Loss: 3.1105\n",
      "Batch 339, Loss: 3.0973\n",
      "Batch 340, Loss: 3.1591\n",
      "Batch 341, Loss: 3.1612\n",
      "Batch 342, Loss: 3.2268\n",
      "Batch 343, Loss: 3.1533\n",
      "Batch 344, Loss: 3.1814\n",
      "Batch 345, Loss: 3.1120\n",
      "Batch 346, Loss: 3.2085\n",
      "Batch 347, Loss: 3.1826\n",
      "Batch 348, Loss: 3.1321\n",
      "Batch 349, Loss: 3.1421\n",
      "Batch 350, Loss: 3.1840\n",
      "Batch 351, Loss: 3.1787\n",
      "Batch 352, Loss: 3.1823\n",
      "Batch 353, Loss: 3.1671\n",
      "Batch 354, Loss: 3.2014\n",
      "Batch 355, Loss: 3.1101\n",
      "Batch 356, Loss: 3.1605\n",
      "Batch 357, Loss: 3.1578\n",
      "Batch 358, Loss: 3.1492\n",
      "Batch 359, Loss: 3.1613\n",
      "Batch 360, Loss: 3.1678\n",
      "Batch 361, Loss: 3.1072\n",
      "Batch 362, Loss: 3.1141\n",
      "Batch 363, Loss: 3.1174\n",
      "Batch 364, Loss: 3.0817\n",
      "Batch 365, Loss: 3.1594\n",
      "Batch 366, Loss: 3.0752\n",
      "Batch 367, Loss: 3.1335\n",
      "Batch 368, Loss: 3.1191\n",
      "Batch 369, Loss: 3.1623\n",
      "Batch 370, Loss: 3.0677\n",
      "Batch 371, Loss: 3.1709\n",
      "Batch 372, Loss: 2.9804\n",
      "Batch 373, Loss: 3.1207\n",
      "Batch 374, Loss: 2.9533\n",
      "Batch 375, Loss: 2.9845\n",
      "Batch 376, Loss: 3.1244\n",
      "Batch 377, Loss: 3.0101\n",
      "Batch 378, Loss: 3.0770\n",
      "Epoch 3, Average Loss: 3.3125\n",
      "Once upon a time, there was an villagent about his family. It was big appros fish, and higin in his favit. The sun wanted to perf. One day, a groglmb \n",
      "Jillow, girl was sad. She two bed to her little boy smiled, but by the little girl was up. She was cup, but it was so happy and that she \n",
      "loved to the stup up. Jile, her mom saw something. She was surprisper that she saw lots of hide. Jilla laughed and off the but she started to pick it \n",
      "up a safe time. Tom and Dad were so happy because she was very proudly. Mommy was happy that said noed maile, \"What! What are going!\". They conceeks,\" When they \n",
      "lean came back and packed. Mom why was so glad of the pieces together forwcesast. \"Thank you!\" shouteddy bear said. Mommy was happy and said sorry forgetable. \"You wish the \n",
      "pupping some pient down to watch about the help with Nograpree and soon is many test. I wrag hisopse anyone, you where you beens!â€ \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 3.0495\n",
      "Batch 2, Loss: 3.1127\n",
      "Batch 3, Loss: 3.0479\n",
      "Batch 4, Loss: 3.1600\n",
      "Batch 5, Loss: 3.1092\n",
      "Batch 6, Loss: 3.1514\n",
      "Batch 7, Loss: 3.0934\n",
      "Batch 8, Loss: 3.1139\n",
      "Batch 9, Loss: 3.1059\n",
      "Batch 10, Loss: 3.1246\n",
      "Batch 11, Loss: 2.9710\n",
      "Batch 12, Loss: 3.0702\n",
      "Batch 13, Loss: 3.0765\n",
      "Batch 14, Loss: 3.1053\n",
      "Batch 15, Loss: 3.0002\n",
      "Batch 16, Loss: 3.1012\n",
      "Batch 17, Loss: 3.0556\n",
      "Batch 18, Loss: 3.0179\n",
      "Batch 19, Loss: 3.0797\n",
      "Batch 20, Loss: 3.1073\n",
      "Batch 21, Loss: 3.0901\n",
      "Batch 22, Loss: 2.9323\n",
      "Batch 23, Loss: 3.0958\n",
      "Batch 24, Loss: 3.0646\n",
      "Batch 25, Loss: 3.0814\n",
      "Batch 26, Loss: 3.0573\n",
      "Batch 27, Loss: 3.0881\n",
      "Batch 28, Loss: 3.0626\n",
      "Batch 29, Loss: 3.0782\n",
      "Batch 30, Loss: 2.9273\n",
      "Batch 31, Loss: 3.0172\n",
      "Batch 32, Loss: 3.0096\n",
      "Batch 33, Loss: 3.0165\n",
      "Batch 34, Loss: 3.0515\n",
      "Batch 35, Loss: 3.0293\n",
      "Batch 36, Loss: 3.1006\n",
      "Batch 37, Loss: 2.9378\n",
      "Batch 38, Loss: 3.0829\n",
      "Batch 39, Loss: 3.0723\n",
      "Batch 40, Loss: 2.9962\n",
      "Batch 41, Loss: 3.0630\n",
      "Batch 42, Loss: 2.9949\n",
      "Batch 43, Loss: 3.0573\n",
      "Batch 44, Loss: 3.0567\n",
      "Batch 45, Loss: 3.0198\n",
      "Batch 46, Loss: 2.9119\n",
      "Batch 47, Loss: 3.0210\n",
      "Batch 48, Loss: 3.0352\n",
      "Batch 49, Loss: 2.9587\n",
      "Batch 50, Loss: 2.9377\n",
      "Batch 51, Loss: 3.0915\n",
      "Batch 52, Loss: 3.0537\n",
      "Batch 53, Loss: 3.0316\n",
      "Batch 54, Loss: 2.9997\n",
      "Batch 55, Loss: 3.0209\n",
      "Batch 56, Loss: 3.0348\n",
      "Batch 57, Loss: 3.0731\n",
      "Batch 58, Loss: 2.9647\n",
      "Batch 59, Loss: 3.0589\n",
      "Batch 60, Loss: 2.8793\n",
      "Batch 61, Loss: 3.0334\n",
      "Batch 62, Loss: 2.9628\n",
      "Batch 63, Loss: 3.0251\n",
      "Batch 64, Loss: 3.0197\n",
      "Batch 65, Loss: 2.9439\n",
      "Batch 66, Loss: 3.0413\n",
      "Batch 67, Loss: 2.9630\n",
      "Batch 68, Loss: 3.0500\n",
      "Batch 69, Loss: 2.9584\n",
      "Batch 70, Loss: 2.8779\n",
      "Batch 71, Loss: 3.0072\n",
      "Batch 72, Loss: 2.9876\n",
      "Batch 73, Loss: 3.0498\n",
      "Batch 74, Loss: 3.0292\n",
      "Batch 75, Loss: 3.0599\n",
      "Batch 76, Loss: 3.0170\n",
      "Batch 77, Loss: 3.0242\n",
      "Batch 78, Loss: 2.8662\n",
      "Batch 79, Loss: 3.0108\n",
      "Batch 80, Loss: 3.0334\n",
      "Batch 81, Loss: 3.0141\n",
      "Batch 82, Loss: 2.9995\n",
      "Batch 83, Loss: 2.9807\n",
      "Batch 84, Loss: 3.0644\n",
      "Batch 85, Loss: 2.9955\n",
      "Batch 86, Loss: 3.0032\n",
      "Batch 87, Loss: 3.0068\n",
      "Batch 88, Loss: 2.9884\n",
      "Batch 89, Loss: 2.9512\n",
      "Batch 90, Loss: 2.9840\n",
      "Batch 91, Loss: 2.9101\n",
      "Batch 92, Loss: 2.9260\n",
      "Batch 93, Loss: 3.0162\n",
      "Batch 94, Loss: 2.9536\n",
      "Batch 95, Loss: 2.9917\n",
      "Batch 96, Loss: 2.9386\n",
      "Batch 97, Loss: 2.9170\n",
      "Batch 98, Loss: 2.9699\n",
      "Batch 99, Loss: 3.0101\n",
      "Batch 100, Loss: 2.9851\n",
      "Batch 101, Loss: 3.0030\n",
      "Batch 102, Loss: 2.9940\n",
      "Batch 103, Loss: 2.9998\n",
      "Batch 104, Loss: 2.9873\n",
      "Batch 105, Loss: 2.9838\n",
      "Batch 106, Loss: 2.8191\n",
      "Batch 107, Loss: 3.0022\n",
      "Batch 108, Loss: 2.9949\n",
      "Batch 109, Loss: 2.9725\n",
      "Batch 110, Loss: 2.8068\n",
      "Batch 111, Loss: 2.9344\n",
      "Batch 112, Loss: 2.8493\n",
      "Batch 113, Loss: 2.9252\n",
      "Batch 114, Loss: 2.9608\n",
      "Batch 115, Loss: 2.9517\n",
      "Batch 116, Loss: 2.9896\n",
      "Batch 117, Loss: 2.9917\n",
      "Batch 118, Loss: 2.9740\n",
      "Batch 119, Loss: 2.9844\n",
      "Batch 120, Loss: 2.9823\n",
      "Batch 121, Loss: 2.9557\n",
      "Batch 122, Loss: 2.9534\n",
      "Batch 123, Loss: 2.7668\n",
      "Batch 124, Loss: 2.8894\n",
      "Batch 125, Loss: 2.9330\n",
      "Batch 126, Loss: 2.9634\n",
      "Batch 127, Loss: 2.9379\n",
      "Batch 128, Loss: 2.9315\n",
      "Batch 129, Loss: 2.9545\n",
      "Batch 130, Loss: 2.9347\n",
      "Batch 131, Loss: 2.9342\n",
      "Batch 132, Loss: 2.9858\n",
      "Batch 133, Loss: 2.9279\n",
      "Batch 134, Loss: 2.9798\n",
      "Batch 135, Loss: 2.9485\n",
      "Batch 136, Loss: 2.9137\n",
      "Batch 137, Loss: 2.9337\n",
      "Batch 138, Loss: 2.9143\n",
      "Batch 139, Loss: 2.9515\n",
      "Batch 140, Loss: 2.9265\n",
      "Batch 141, Loss: 2.9302\n",
      "Batch 142, Loss: 2.9218\n",
      "Batch 143, Loss: 2.8629\n",
      "Batch 144, Loss: 2.8873\n",
      "Batch 145, Loss: 2.9256\n",
      "Batch 146, Loss: 2.8734\n",
      "Batch 147, Loss: 2.8393\n",
      "Batch 148, Loss: 2.9239\n",
      "Batch 149, Loss: 2.9093\n",
      "Batch 150, Loss: 2.9271\n",
      "Batch 151, Loss: 2.8861\n",
      "Batch 152, Loss: 2.8836\n",
      "Batch 153, Loss: 2.9093\n",
      "Batch 154, Loss: 2.8070\n",
      "Batch 155, Loss: 2.9331\n",
      "Batch 156, Loss: 2.8282\n",
      "Batch 157, Loss: 2.8454\n",
      "Batch 158, Loss: 2.9218\n",
      "Batch 159, Loss: 2.8918\n",
      "Batch 160, Loss: 2.8887\n",
      "Batch 161, Loss: 2.8021\n",
      "Batch 162, Loss: 2.9122\n",
      "Batch 163, Loss: 2.9304\n",
      "Batch 164, Loss: 2.9201\n",
      "Batch 165, Loss: 2.7691\n",
      "Batch 166, Loss: 2.7822\n",
      "Batch 167, Loss: 2.8934\n",
      "Batch 168, Loss: 2.9092\n",
      "Batch 169, Loss: 2.9272\n",
      "Batch 170, Loss: 2.7382\n",
      "Batch 171, Loss: 2.9207\n",
      "Batch 172, Loss: 2.8134\n",
      "Batch 173, Loss: 2.8943\n",
      "Batch 174, Loss: 2.9137\n",
      "Batch 175, Loss: 2.8582\n",
      "Batch 176, Loss: 2.9593\n",
      "Batch 177, Loss: 2.8553\n",
      "Batch 178, Loss: 2.9155\n",
      "Batch 179, Loss: 2.8400\n",
      "Batch 180, Loss: 2.8800\n",
      "Batch 181, Loss: 2.7633\n",
      "Batch 182, Loss: 2.9225\n",
      "Batch 183, Loss: 2.8637\n",
      "Batch 184, Loss: 2.9455\n",
      "Batch 185, Loss: 2.9193\n",
      "Batch 186, Loss: 2.9084\n",
      "Batch 187, Loss: 2.9239\n",
      "Batch 188, Loss: 2.9146\n",
      "Batch 189, Loss: 2.8954\n",
      "Batch 190, Loss: 2.8126\n",
      "Batch 191, Loss: 2.8215\n",
      "Batch 192, Loss: 2.9124\n",
      "Batch 193, Loss: 2.9116\n",
      "Batch 194, Loss: 2.9171\n",
      "Batch 195, Loss: 2.8801\n",
      "Batch 196, Loss: 2.8878\n",
      "Batch 197, Loss: 2.7616\n",
      "Batch 198, Loss: 2.8895\n",
      "Batch 199, Loss: 2.8853\n",
      "Batch 200, Loss: 2.7324\n",
      "Once upon a time there was a little nosonb town. It was a special wall to the fish, and every day that everyone laugh. The wag butterfly stopped a thicker. \n",
      "The a little bird wanted to calmer anything. The bird started to its be grow start together.\" The bear kept a line to help. The birds leg was not scared. \n",
      "Everyone thought that it was a little dog got to the bird's good surprise. The bird was sad because of not too loud and said: \"Welce we can left?\". The \n",
      "dog grinaded the other toy and boat that its. It was became alone that the bird was on the birds peant. The bird flew up on the birds, flying to \n",
      "grayed. He was excited when playing with his birds had loes, and beautiful. But how it was time to do what the story could make them away for the bird \n",
      "like a big nester. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 20.95 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.8774\n",
      "Batch 202, Loss: 2.9068\n",
      "Batch 203, Loss: 2.9162\n",
      "Batch 204, Loss: 2.8883\n",
      "Batch 205, Loss: 2.7856\n",
      "Batch 206, Loss: 2.8191\n",
      "Batch 207, Loss: 2.9014\n",
      "Batch 208, Loss: 2.8653\n",
      "Batch 209, Loss: 2.8549\n",
      "Batch 210, Loss: 2.7448\n",
      "Batch 211, Loss: 2.7524\n",
      "Batch 212, Loss: 2.8779\n",
      "Batch 213, Loss: 2.8067\n",
      "Batch 214, Loss: 2.9340\n",
      "Batch 215, Loss: 2.7503\n",
      "Batch 216, Loss: 2.8873\n",
      "Batch 217, Loss: 2.9038\n",
      "Batch 218, Loss: 3.0304\n",
      "Batch 219, Loss: 3.5695\n",
      "Batch 220, Loss: 3.4086\n",
      "Batch 221, Loss: 3.4267\n",
      "Batch 222, Loss: 3.8243\n",
      "Batch 223, Loss: 3.4456\n",
      "Batch 224, Loss: 3.4042\n",
      "Batch 225, Loss: 3.3164\n",
      "Batch 226, Loss: 3.2780\n",
      "Batch 227, Loss: 3.2592\n",
      "Batch 228, Loss: 3.2298\n",
      "Batch 229, Loss: 3.2047\n",
      "Batch 230, Loss: 3.0986\n",
      "Batch 231, Loss: 3.0198\n",
      "Batch 232, Loss: 3.1466\n",
      "Batch 233, Loss: 3.1098\n",
      "Batch 234, Loss: 3.1250\n",
      "Batch 235, Loss: 3.0818\n",
      "Batch 236, Loss: 2.9791\n",
      "Batch 237, Loss: 3.0045\n",
      "Batch 238, Loss: 3.0051\n",
      "Batch 239, Loss: 2.9476\n",
      "Batch 240, Loss: 3.0368\n",
      "Batch 241, Loss: 2.9394\n",
      "Batch 242, Loss: 2.9899\n",
      "Batch 243, Loss: 2.9788\n",
      "Batch 244, Loss: 2.9898\n",
      "Batch 245, Loss: 2.9600\n",
      "Batch 246, Loss: 2.9351\n",
      "Batch 247, Loss: 2.8805\n",
      "Batch 248, Loss: 2.9062\n",
      "Batch 249, Loss: 2.8714\n",
      "Batch 250, Loss: 2.8842\n",
      "Batch 251, Loss: 2.8900\n",
      "Batch 252, Loss: 2.8765\n",
      "Batch 253, Loss: 2.8716\n",
      "Batch 254, Loss: 2.8868\n",
      "Batch 255, Loss: 2.8491\n",
      "Batch 256, Loss: 2.8504\n",
      "Batch 257, Loss: 2.8087\n",
      "Batch 258, Loss: 2.8831\n",
      "Batch 259, Loss: 2.8822\n",
      "Batch 260, Loss: 2.7362\n",
      "Batch 261, Loss: 2.8478\n",
      "Batch 262, Loss: 2.8581\n",
      "Batch 263, Loss: 2.8209\n",
      "Batch 264, Loss: 2.8285\n",
      "Batch 265, Loss: 2.8115\n",
      "Batch 266, Loss: 2.8051\n",
      "Batch 267, Loss: 2.8512\n",
      "Batch 268, Loss: 2.7315\n",
      "Batch 269, Loss: 2.8245\n",
      "Batch 270, Loss: 2.8815\n",
      "Batch 271, Loss: 2.8521\n",
      "Batch 272, Loss: 2.7819\n",
      "Batch 273, Loss: 2.7252\n",
      "Batch 274, Loss: 2.8386\n",
      "Batch 275, Loss: 2.8060\n",
      "Batch 276, Loss: 2.7603\n",
      "Batch 277, Loss: 2.8461\n",
      "Batch 278, Loss: 2.8076\n",
      "Batch 279, Loss: 2.7433\n",
      "Batch 280, Loss: 2.8095\n",
      "Batch 281, Loss: 2.6826\n",
      "Batch 282, Loss: 2.6764\n",
      "Batch 283, Loss: 2.7408\n",
      "Batch 284, Loss: 2.8188\n",
      "Batch 285, Loss: 2.8081\n",
      "Batch 286, Loss: 2.8298\n",
      "Batch 287, Loss: 2.8219\n",
      "Batch 288, Loss: 2.7881\n",
      "Batch 289, Loss: 2.8262\n",
      "Batch 290, Loss: 2.8208\n",
      "Batch 291, Loss: 2.8318\n",
      "Batch 292, Loss: 2.7937\n",
      "Batch 293, Loss: 2.8454\n",
      "Batch 294, Loss: 2.6845\n",
      "Batch 295, Loss: 2.7843\n",
      "Batch 296, Loss: 2.8346\n",
      "Batch 297, Loss: 2.7966\n",
      "Batch 298, Loss: 2.8011\n",
      "Batch 299, Loss: 2.7884\n",
      "Batch 300, Loss: 2.7715\n",
      "Batch 301, Loss: 2.7847\n",
      "Batch 302, Loss: 2.7933\n",
      "Batch 303, Loss: 2.7870\n",
      "Batch 304, Loss: 2.7406\n",
      "Batch 305, Loss: 2.8413\n",
      "Batch 306, Loss: 2.7933\n",
      "Batch 307, Loss: 2.7544\n",
      "Batch 308, Loss: 2.8041\n",
      "Batch 309, Loss: 2.7966\n",
      "Batch 310, Loss: 2.7269\n",
      "Batch 311, Loss: 2.7541\n",
      "Batch 312, Loss: 2.7791\n",
      "Batch 313, Loss: 2.7854\n",
      "Batch 314, Loss: 2.7389\n",
      "Batch 315, Loss: 2.7682\n",
      "Batch 316, Loss: 2.7514\n",
      "Batch 317, Loss: 2.7728\n",
      "Batch 318, Loss: 2.7775\n",
      "Batch 319, Loss: 2.6477\n",
      "Batch 320, Loss: 2.7294\n",
      "Batch 321, Loss: 2.7637\n",
      "Batch 322, Loss: 2.7420\n",
      "Batch 323, Loss: 2.7508\n",
      "Batch 324, Loss: 2.6773\n",
      "Batch 325, Loss: 2.7573\n",
      "Batch 326, Loss: 2.6739\n",
      "Batch 327, Loss: 2.7070\n",
      "Batch 328, Loss: 2.7646\n",
      "Batch 329, Loss: 2.6043\n",
      "Batch 330, Loss: 2.7395\n",
      "Batch 331, Loss: 2.6608\n",
      "Batch 332, Loss: 2.5777\n",
      "Batch 333, Loss: 2.7350\n",
      "Batch 334, Loss: 2.7547\n",
      "Batch 335, Loss: 2.7528\n",
      "Batch 336, Loss: 2.7468\n",
      "Batch 337, Loss: 2.7421\n",
      "Batch 338, Loss: 2.7415\n",
      "Batch 339, Loss: 2.7584\n",
      "Batch 340, Loss: 2.6795\n",
      "Batch 341, Loss: 2.7489\n",
      "Batch 342, Loss: 2.7017\n",
      "Batch 343, Loss: 2.7155\n",
      "Batch 344, Loss: 2.7260\n",
      "Batch 345, Loss: 2.6763\n",
      "Batch 346, Loss: 2.7458\n",
      "Batch 347, Loss: 2.7294\n",
      "Batch 348, Loss: 2.6792\n",
      "Batch 349, Loss: 2.6698\n",
      "Batch 350, Loss: 2.6357\n",
      "Batch 351, Loss: 2.7234\n",
      "Batch 352, Loss: 2.5384\n",
      "Batch 353, Loss: 2.7334\n",
      "Batch 354, Loss: 2.7810\n",
      "Batch 355, Loss: 2.7370\n",
      "Batch 356, Loss: 2.7588\n",
      "Batch 357, Loss: 2.7224\n",
      "Batch 358, Loss: 2.7218\n",
      "Batch 359, Loss: 2.6853\n",
      "Batch 360, Loss: 2.6822\n",
      "Batch 361, Loss: 2.7534\n",
      "Batch 362, Loss: 2.7253\n",
      "Batch 363, Loss: 2.5825\n",
      "Batch 364, Loss: 2.7047\n",
      "Batch 365, Loss: 2.7048\n",
      "Batch 366, Loss: 2.7428\n",
      "Batch 367, Loss: 2.7271\n",
      "Batch 368, Loss: 2.7093\n",
      "Batch 369, Loss: 2.7279\n",
      "Batch 370, Loss: 2.7169\n",
      "Batch 371, Loss: 2.7127\n",
      "Batch 372, Loss: 2.6872\n",
      "Batch 373, Loss: 2.7002\n",
      "Batch 374, Loss: 2.6897\n",
      "Batch 375, Loss: 2.7052\n",
      "Batch 376, Loss: 2.6460\n",
      "Batch 377, Loss: 2.7109\n",
      "Batch 378, Loss: 2.6146\n",
      "Epoch 4, Average Loss: 2.9030\n",
      "Once upon a time, there was a little boy called Timmy. Timmy had a very sillymituel smile. One day, Timmy he went to the beach and his pathion. Timmy didn't \n",
      "know what to go. Eventually, Timmy's bought Timmy saw an in a tikey fair. Timmy said walking around he couldn't hear it and replied what to go to say he \n",
      "could play with a trike buy him. Oh nobrise and threadyear boy was ruin. Timmy was sad and then he said \"Don't worry, Timmy, I leat\". They watched the while, \n",
      "\"Can you are you eat?\" Timmy and his grasa hearted to get the test and told Max the tiple.\" Timmy loved his friends counted Timmy's delight that the trikiny was \n",
      "even thince who could not find him. He told her he was never seen a chairy tuler and Timmy was so proud of him for miserners. Timmy realized she had \n",
      "a little. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.5713\n",
      "Batch 2, Loss: 2.6471\n",
      "Batch 3, Loss: 2.7175\n",
      "Batch 4, Loss: 2.6182\n",
      "Batch 5, Loss: 2.6770\n",
      "Batch 6, Loss: 2.6776\n",
      "Batch 7, Loss: 2.7034\n",
      "Batch 8, Loss: 2.6811\n",
      "Batch 9, Loss: 2.7145\n",
      "Batch 10, Loss: 2.6441\n",
      "Batch 11, Loss: 2.6617\n",
      "Batch 12, Loss: 2.6650\n",
      "Batch 13, Loss: 2.6792\n",
      "Batch 14, Loss: 2.5647\n",
      "Batch 15, Loss: 2.6802\n",
      "Batch 16, Loss: 2.6745\n",
      "Batch 17, Loss: 2.5848\n",
      "Batch 18, Loss: 2.6607\n",
      "Batch 19, Loss: 2.5987\n",
      "Batch 20, Loss: 2.6808\n",
      "Batch 21, Loss: 2.6627\n",
      "Batch 22, Loss: 2.6756\n",
      "Batch 23, Loss: 2.6566\n",
      "Batch 24, Loss: 2.6521\n",
      "Batch 25, Loss: 2.6524\n",
      "Batch 26, Loss: 2.6157\n",
      "Batch 27, Loss: 2.6760\n",
      "Batch 28, Loss: 2.6794\n",
      "Batch 29, Loss: 2.6763\n",
      "Batch 30, Loss: 2.6440\n",
      "Batch 31, Loss: 2.5768\n",
      "Batch 32, Loss: 2.6767\n",
      "Batch 33, Loss: 2.6397\n",
      "Batch 34, Loss: 2.5757\n",
      "Batch 35, Loss: 2.6438\n",
      "Batch 36, Loss: 2.6333\n",
      "Batch 37, Loss: 2.6132\n",
      "Batch 38, Loss: 2.6517\n",
      "Batch 39, Loss: 2.6763\n",
      "Batch 40, Loss: 2.6380\n",
      "Batch 41, Loss: 2.6426\n",
      "Batch 42, Loss: 2.6642\n",
      "Batch 43, Loss: 2.6429\n",
      "Batch 44, Loss: 2.6679\n",
      "Batch 45, Loss: 2.6420\n",
      "Batch 46, Loss: 2.6362\n",
      "Batch 47, Loss: 2.6818\n",
      "Batch 48, Loss: 2.6139\n",
      "Batch 49, Loss: 2.6264\n",
      "Batch 50, Loss: 2.5365\n",
      "Batch 51, Loss: 2.5468\n",
      "Batch 52, Loss: 2.5698\n",
      "Batch 53, Loss: 2.5204\n",
      "Batch 54, Loss: 2.5575\n",
      "Batch 55, Loss: 2.6466\n",
      "Batch 56, Loss: 2.6248\n",
      "Batch 57, Loss: 2.6230\n",
      "Batch 58, Loss: 2.6507\n",
      "Batch 59, Loss: 2.5499\n",
      "Batch 60, Loss: 2.5643\n",
      "Batch 61, Loss: 2.6238\n",
      "Batch 62, Loss: 2.6739\n",
      "Batch 63, Loss: 2.6296\n",
      "Batch 64, Loss: 2.6182\n",
      "Batch 65, Loss: 2.5270\n",
      "Batch 66, Loss: 2.5811\n",
      "Batch 67, Loss: 2.5493\n",
      "Batch 68, Loss: 2.5861\n",
      "Batch 69, Loss: 2.5010\n",
      "Batch 70, Loss: 2.5098\n",
      "Batch 71, Loss: 2.6363\n",
      "Batch 72, Loss: 2.5915\n",
      "Batch 73, Loss: 2.6217\n",
      "Batch 74, Loss: 2.5775\n",
      "Batch 75, Loss: 2.6300\n",
      "Batch 76, Loss: 2.6073\n",
      "Batch 77, Loss: 2.5782\n",
      "Batch 78, Loss: 2.6151\n",
      "Batch 79, Loss: 2.5735\n",
      "Batch 80, Loss: 2.5965\n",
      "Batch 81, Loss: 2.5188\n",
      "Batch 82, Loss: 2.6452\n",
      "Batch 83, Loss: 2.5238\n",
      "Batch 84, Loss: 2.6131\n",
      "Batch 85, Loss: 2.4601\n",
      "Batch 86, Loss: 2.6404\n",
      "Batch 87, Loss: 2.6063\n",
      "Batch 88, Loss: 2.5886\n",
      "Batch 89, Loss: 2.5541\n",
      "Batch 90, Loss: 2.6003\n",
      "Batch 91, Loss: 2.5856\n",
      "Batch 92, Loss: 2.5862\n",
      "Batch 93, Loss: 2.5647\n",
      "Batch 94, Loss: 2.5991\n",
      "Batch 95, Loss: 2.4889\n",
      "Batch 96, Loss: 2.5991\n",
      "Batch 97, Loss: 2.6269\n",
      "Batch 98, Loss: 2.5806\n",
      "Batch 99, Loss: 2.6261\n",
      "Batch 100, Loss: 2.5828\n",
      "Batch 101, Loss: 2.6111\n",
      "Batch 102, Loss: 2.6728\n",
      "Batch 103, Loss: 2.6482\n",
      "Batch 104, Loss: 2.6295\n",
      "Batch 105, Loss: 2.5610\n",
      "Batch 106, Loss: 2.5561\n",
      "Batch 107, Loss: 2.6523\n",
      "Batch 108, Loss: 2.6243\n",
      "Batch 109, Loss: 2.5685\n",
      "Batch 110, Loss: 2.6364\n",
      "Batch 111, Loss: 2.5904\n",
      "Batch 112, Loss: 2.6615\n",
      "Batch 113, Loss: 2.6635\n",
      "Batch 114, Loss: 2.5775\n",
      "Batch 115, Loss: 2.6338\n",
      "Batch 116, Loss: 2.6299\n",
      "Batch 117, Loss: 2.6174\n",
      "Batch 118, Loss: 2.6229\n",
      "Batch 119, Loss: 2.5100\n",
      "Batch 120, Loss: 2.6075\n",
      "Batch 121, Loss: 2.5993\n",
      "Batch 122, Loss: 2.5213\n",
      "Batch 123, Loss: 2.6163\n",
      "Batch 124, Loss: 2.6075\n",
      "Batch 125, Loss: 2.5790\n",
      "Batch 126, Loss: 2.5904\n",
      "Batch 127, Loss: 2.5917\n",
      "Batch 128, Loss: 2.5697\n",
      "Batch 129, Loss: 2.6164\n",
      "Batch 130, Loss: 2.5650\n",
      "Batch 131, Loss: 2.6155\n",
      "Batch 132, Loss: 2.5713\n",
      "Batch 133, Loss: 2.6085\n",
      "Batch 134, Loss: 2.5918\n",
      "Batch 135, Loss: 2.5867\n",
      "Batch 136, Loss: 2.6051\n",
      "Batch 137, Loss: 2.5209\n",
      "Batch 138, Loss: 2.5416\n",
      "Batch 139, Loss: 2.5411\n",
      "Batch 140, Loss: 2.5404\n",
      "Batch 141, Loss: 2.5169\n",
      "Batch 142, Loss: 2.4270\n",
      "Batch 143, Loss: 2.5596\n",
      "Batch 144, Loss: 2.5770\n",
      "Batch 145, Loss: 2.5552\n",
      "Batch 146, Loss: 2.5221\n",
      "Batch 147, Loss: 2.5253\n",
      "Batch 148, Loss: 2.5162\n",
      "Batch 149, Loss: 2.5646\n",
      "Batch 150, Loss: 2.5484\n",
      "Batch 151, Loss: 2.5628\n",
      "Batch 152, Loss: 2.5244\n",
      "Batch 153, Loss: 2.3988\n",
      "Batch 154, Loss: 2.5472\n",
      "Batch 155, Loss: 2.4197\n",
      "Batch 156, Loss: 2.5362\n",
      "Batch 157, Loss: 2.4743\n",
      "Batch 158, Loss: 2.4030\n",
      "Batch 159, Loss: 2.5308\n",
      "Batch 160, Loss: 2.5371\n",
      "Batch 161, Loss: 2.5448\n",
      "Batch 162, Loss: 2.5766\n",
      "Batch 163, Loss: 2.5477\n",
      "Batch 164, Loss: 2.5480\n",
      "Batch 165, Loss: 2.5530\n",
      "Batch 166, Loss: 2.5631\n",
      "Batch 167, Loss: 2.4273\n",
      "Batch 168, Loss: 2.5568\n",
      "Batch 169, Loss: 2.5341\n",
      "Batch 170, Loss: 2.5340\n",
      "Batch 171, Loss: 2.5379\n",
      "Batch 172, Loss: 2.5497\n",
      "Batch 173, Loss: 2.5212\n",
      "Batch 174, Loss: 2.5177\n",
      "Batch 175, Loss: 2.5160\n",
      "Batch 176, Loss: 2.5464\n",
      "Batch 177, Loss: 2.5685\n",
      "Batch 178, Loss: 2.5520\n",
      "Batch 179, Loss: 2.5302\n",
      "Batch 180, Loss: 2.5186\n",
      "Batch 181, Loss: 2.5337\n",
      "Batch 182, Loss: 2.5461\n",
      "Batch 183, Loss: 2.5389\n",
      "Batch 184, Loss: 2.5424\n",
      "Batch 185, Loss: 2.5067\n",
      "Batch 186, Loss: 2.4564\n",
      "Batch 187, Loss: 2.5034\n",
      "Batch 188, Loss: 2.4327\n",
      "Batch 189, Loss: 2.5115\n",
      "Batch 190, Loss: 2.5621\n",
      "Batch 191, Loss: 2.5146\n",
      "Batch 192, Loss: 2.5288\n",
      "Batch 193, Loss: 2.4297\n",
      "Batch 194, Loss: 2.5324\n",
      "Batch 195, Loss: 2.5400\n",
      "Batch 196, Loss: 2.5016\n",
      "Batch 197, Loss: 2.5330\n",
      "Batch 198, Loss: 2.5288\n",
      "Batch 199, Loss: 2.5231\n",
      "Batch 200, Loss: 2.5124\n",
      "Once upon a time there was a red fish who had a special spcusy. The spear loved being up and enjoyed all day picking benes of day and into a \n",
      "flight. The big persister! It looked very brave and happy. One day, the bucky sparkly suggested incut items and outs in her handets. But the bucky thought it would never \n",
      "have a little bucket. The girl noticed the blanket up softly threw he sect looked at the magic. The bucky cat was small. It looked scared, but couldn't. At the \n",
      "little girl then heard the cappy: \"This do it! You're not very bad!\" The garden smiled at this pruver. The buck bucky waved and cheered in the inf. There was \n",
      "so fast that it gathered for the busy into lots of the forest for something. Suddenly, a magic made the magical magical toy in the magicals. It was a while, \n",
      "should not move. When it came in, the cauzing the magical bear who went home. She kinked until she began to the middles got tall and she felt a long \n",
      "time. The magical caushed in the tanket. After a few matched the magic ended, sweet was trying for the bucketence for her. He smiled with it! The magical ended, and \n",
      "the magical would never to watch the me. From that day forward that day on even more oth of their special luck. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 25.83 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.5434\n",
      "Batch 202, Loss: 2.4740\n",
      "Batch 203, Loss: 2.4517\n",
      "Batch 204, Loss: 2.4473\n",
      "Batch 205, Loss: 2.4365\n",
      "Batch 206, Loss: 2.4021\n",
      "Batch 207, Loss: 2.5177\n",
      "Batch 208, Loss: 2.5180\n",
      "Batch 209, Loss: 2.5108\n",
      "Batch 210, Loss: 2.4919\n",
      "Batch 211, Loss: 2.4845\n",
      "Batch 212, Loss: 2.5420\n",
      "Batch 213, Loss: 2.4864\n",
      "Batch 214, Loss: 2.4590\n",
      "Batch 215, Loss: 2.4906\n",
      "Batch 216, Loss: 2.5292\n",
      "Batch 217, Loss: 2.4996\n",
      "Batch 218, Loss: 2.5082\n",
      "Batch 219, Loss: 2.4908\n",
      "Batch 220, Loss: 2.4892\n",
      "Batch 221, Loss: 2.4026\n",
      "Batch 222, Loss: 2.4512\n",
      "Batch 223, Loss: 2.4959\n",
      "Batch 224, Loss: 2.4347\n",
      "Batch 225, Loss: 2.4793\n",
      "Batch 226, Loss: 2.4722\n",
      "Batch 227, Loss: 2.5340\n",
      "Batch 228, Loss: 2.5153\n",
      "Batch 229, Loss: 2.3906\n",
      "Batch 230, Loss: 2.5316\n",
      "Batch 231, Loss: 2.3770\n",
      "Batch 232, Loss: 2.5209\n",
      "Batch 233, Loss: 2.4982\n",
      "Batch 234, Loss: 2.5163\n",
      "Batch 235, Loss: 2.5369\n",
      "Batch 236, Loss: 2.4980\n",
      "Batch 237, Loss: 2.4757\n",
      "Batch 238, Loss: 2.5005\n",
      "Batch 239, Loss: 2.4914\n",
      "Batch 240, Loss: 2.4758\n",
      "Batch 241, Loss: 2.4774\n",
      "Batch 242, Loss: 2.4892\n",
      "Batch 243, Loss: 2.4589\n",
      "Batch 244, Loss: 2.4184\n",
      "Batch 245, Loss: 2.4656\n",
      "Batch 246, Loss: 2.4675\n",
      "Batch 247, Loss: 2.3751\n",
      "Batch 248, Loss: 2.4793\n",
      "Batch 249, Loss: 2.3459\n",
      "Batch 250, Loss: 2.4659\n",
      "Batch 251, Loss: 2.4788\n",
      "Batch 252, Loss: 2.4427\n",
      "Batch 253, Loss: 2.4521\n",
      "Batch 254, Loss: 2.3352\n",
      "Batch 255, Loss: 2.3445\n",
      "Batch 256, Loss: 2.4277\n",
      "Batch 257, Loss: 2.4652\n",
      "Batch 258, Loss: 2.4462\n",
      "Batch 259, Loss: 2.4615\n",
      "Batch 260, Loss: 2.5258\n",
      "Batch 261, Loss: 2.5177\n",
      "Batch 262, Loss: 2.4881\n",
      "Batch 263, Loss: 2.4785\n",
      "Batch 264, Loss: 2.4751\n",
      "Batch 265, Loss: 2.4699\n",
      "Batch 266, Loss: 2.4431\n",
      "Batch 267, Loss: 2.4822\n",
      "Batch 268, Loss: 2.4869\n",
      "Batch 269, Loss: 2.4975\n",
      "Batch 270, Loss: 2.4934\n",
      "Batch 271, Loss: 2.5165\n",
      "Batch 272, Loss: 2.4878\n",
      "Batch 273, Loss: 2.4932\n",
      "Batch 274, Loss: 2.4737\n",
      "Batch 275, Loss: 2.4703\n",
      "Batch 276, Loss: 2.4666\n",
      "Batch 277, Loss: 2.4734\n",
      "Batch 278, Loss: 2.4650\n",
      "Batch 279, Loss: 2.4791\n",
      "Batch 280, Loss: 2.4932\n",
      "Batch 281, Loss: 2.4706\n",
      "Batch 282, Loss: 2.4770\n",
      "Batch 283, Loss: 2.4943\n",
      "Batch 284, Loss: 2.4869\n",
      "Batch 285, Loss: 2.4675\n",
      "Batch 286, Loss: 2.4430\n",
      "Batch 287, Loss: 2.4444\n",
      "Batch 288, Loss: 2.3504\n",
      "Batch 289, Loss: 2.4639\n",
      "Batch 290, Loss: 2.4499\n",
      "Batch 291, Loss: 2.4617\n",
      "Batch 292, Loss: 2.4434\n",
      "Batch 293, Loss: 2.4673\n",
      "Batch 294, Loss: 2.4429\n",
      "Batch 295, Loss: 2.4027\n",
      "Batch 296, Loss: 2.4431\n",
      "Batch 297, Loss: 2.4245\n",
      "Batch 298, Loss: 2.4415\n",
      "Batch 299, Loss: 2.4617\n",
      "Batch 300, Loss: 2.4603\n",
      "Batch 301, Loss: 2.4423\n",
      "Batch 302, Loss: 2.4414\n",
      "Batch 303, Loss: 2.2629\n",
      "Batch 304, Loss: 2.3422\n",
      "Batch 305, Loss: 2.4640\n",
      "Batch 306, Loss: 2.3549\n",
      "Batch 307, Loss: 2.4876\n",
      "Batch 308, Loss: 2.4963\n",
      "Batch 309, Loss: 2.3417\n",
      "Batch 310, Loss: 2.4121\n",
      "Batch 311, Loss: 2.5064\n",
      "Batch 312, Loss: 2.4081\n",
      "Batch 313, Loss: 2.4782\n",
      "Batch 314, Loss: 2.4676\n",
      "Batch 315, Loss: 2.4342\n",
      "Batch 316, Loss: 2.4923\n",
      "Batch 317, Loss: 2.4172\n",
      "Batch 318, Loss: 2.3787\n",
      "Batch 319, Loss: 2.4675\n",
      "Batch 320, Loss: 2.4032\n",
      "Batch 321, Loss: 2.4468\n",
      "Batch 322, Loss: 2.3870\n",
      "Batch 323, Loss: 2.4401\n",
      "Batch 324, Loss: 2.4285\n",
      "Batch 325, Loss: 2.4643\n",
      "Batch 326, Loss: 2.3429\n",
      "Batch 327, Loss: 2.4708\n",
      "Batch 328, Loss: 2.4709\n",
      "Batch 329, Loss: 2.3353\n",
      "Batch 330, Loss: 2.4555\n",
      "Batch 331, Loss: 2.3580\n",
      "Batch 332, Loss: 2.4746\n",
      "Batch 333, Loss: 2.4701\n",
      "Batch 334, Loss: 2.4590\n",
      "Batch 335, Loss: 2.4421\n",
      "Batch 336, Loss: 2.4373\n",
      "Batch 337, Loss: 2.4650\n",
      "Batch 338, Loss: 2.5092\n",
      "Batch 339, Loss: 2.4864\n",
      "Batch 340, Loss: 2.4680\n",
      "Batch 341, Loss: 2.4706\n",
      "Batch 342, Loss: 2.4755\n",
      "Batch 343, Loss: 2.5058\n",
      "Batch 344, Loss: 2.4614\n",
      "Batch 345, Loss: 2.4595\n",
      "Batch 346, Loss: 2.4505\n",
      "Batch 347, Loss: 2.4252\n",
      "Batch 348, Loss: 2.4606\n",
      "Batch 349, Loss: 2.3747\n",
      "Batch 350, Loss: 2.4320\n",
      "Batch 351, Loss: 2.4870\n",
      "Batch 352, Loss: 2.4714\n",
      "Batch 353, Loss: 2.3680\n",
      "Batch 354, Loss: 2.4861\n",
      "Batch 355, Loss: 2.4339\n",
      "Batch 356, Loss: 2.4288\n",
      "Batch 357, Loss: 2.4371\n",
      "Batch 358, Loss: 2.3334\n",
      "Batch 359, Loss: 2.4035\n",
      "Batch 360, Loss: 2.4401\n",
      "Batch 361, Loss: 2.4707\n",
      "Batch 362, Loss: 2.4164\n",
      "Batch 363, Loss: 2.3633\n",
      "Batch 364, Loss: 2.4624\n",
      "Batch 365, Loss: 2.3829\n",
      "Batch 366, Loss: 2.4760\n",
      "Batch 367, Loss: 2.3991\n",
      "Batch 368, Loss: 2.3327\n",
      "Batch 369, Loss: 2.4647\n",
      "Batch 370, Loss: 2.4702\n",
      "Batch 371, Loss: 2.4538\n",
      "Batch 372, Loss: 2.3412\n",
      "Batch 373, Loss: 2.3719\n",
      "Batch 374, Loss: 2.4696\n",
      "Batch 375, Loss: 2.3704\n",
      "Batch 376, Loss: 2.4223\n",
      "Batch 377, Loss: 2.3089\n",
      "Batch 378, Loss: 2.3703\n",
      "Epoch 5, Average Loss: 2.5214\n",
      "Once upon a time there was a perfect girl. On her names was Lellying and flowers on the ground. One mummy, she went to the tolter how birthday they could \n",
      "slow. They had lots of fun. When it was too letters to cry. When she spent fast, until she could became very tired with a real drag. She felt a \n",
      "pretty story and wing. After that the days were going on a game. She closed still. She blew all over her arms sweet and ran away. The girl felt nice \n",
      "and strips having awer lots of oppy she quickly said. When she opened into the sky, she was time for herself and she could. It tasted and feeling thankful. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.4590\n",
      "Batch 2, Loss: 2.3062\n",
      "Batch 3, Loss: 2.4174\n",
      "Batch 4, Loss: 2.3195\n",
      "Batch 5, Loss: 2.4537\n",
      "Batch 6, Loss: 2.4550\n",
      "Batch 7, Loss: 2.3331\n",
      "Batch 8, Loss: 2.2803\n",
      "Batch 9, Loss: 2.4462\n",
      "Batch 10, Loss: 2.4362\n",
      "Batch 11, Loss: 2.4225\n",
      "Batch 12, Loss: 2.4242\n",
      "Batch 13, Loss: 2.2618\n",
      "Batch 14, Loss: 2.4123\n",
      "Batch 15, Loss: 2.4253\n",
      "Batch 16, Loss: 2.4299\n",
      "Batch 17, Loss: 2.4097\n",
      "Batch 18, Loss: 2.3786\n",
      "Batch 19, Loss: 2.3061\n",
      "Batch 20, Loss: 2.3987\n",
      "Batch 21, Loss: 2.3986\n",
      "Batch 22, Loss: 2.3906\n",
      "Batch 23, Loss: 2.4057\n",
      "Batch 24, Loss: 2.4054\n",
      "Batch 25, Loss: 2.3787\n",
      "Batch 26, Loss: 2.3892\n",
      "Batch 27, Loss: 2.3925\n",
      "Batch 28, Loss: 2.3504\n",
      "Batch 29, Loss: 2.3756\n",
      "Batch 30, Loss: 2.4108\n",
      "Batch 31, Loss: 2.3221\n",
      "Batch 32, Loss: 2.3316\n",
      "Batch 33, Loss: 2.3068\n",
      "Batch 34, Loss: 2.3764\n",
      "Batch 35, Loss: 2.4077\n",
      "Batch 36, Loss: 2.3517\n",
      "Batch 37, Loss: 2.3614\n",
      "Batch 38, Loss: 2.3680\n",
      "Batch 39, Loss: 2.4095\n",
      "Batch 40, Loss: 2.3412\n",
      "Batch 41, Loss: 2.3886\n",
      "Batch 42, Loss: 2.4130\n",
      "Batch 43, Loss: 2.3778\n",
      "Batch 44, Loss: 2.3532\n",
      "Batch 45, Loss: 2.3914\n",
      "Batch 46, Loss: 2.3402\n",
      "Batch 47, Loss: 2.4132\n",
      "Batch 48, Loss: 2.4207\n",
      "Batch 49, Loss: 2.3621\n",
      "Batch 50, Loss: 2.3526\n",
      "Batch 51, Loss: 2.3894\n",
      "Batch 52, Loss: 2.3644\n",
      "Batch 53, Loss: 2.3652\n",
      "Batch 54, Loss: 2.3555\n",
      "Batch 55, Loss: 2.3151\n",
      "Batch 56, Loss: 2.3903\n",
      "Batch 57, Loss: 2.3957\n",
      "Batch 58, Loss: 2.3649\n",
      "Batch 59, Loss: 2.3716\n",
      "Batch 60, Loss: 2.3405\n",
      "Batch 61, Loss: 2.3539\n",
      "Batch 62, Loss: 2.2522\n",
      "Batch 63, Loss: 2.3634\n",
      "Batch 64, Loss: 2.3721\n",
      "Batch 65, Loss: 2.3758\n",
      "Batch 66, Loss: 2.3518\n",
      "Batch 67, Loss: 2.3624\n",
      "Batch 68, Loss: 2.2757\n",
      "Batch 69, Loss: 2.3404\n",
      "Batch 70, Loss: 2.3414\n",
      "Batch 71, Loss: 2.2509\n",
      "Batch 72, Loss: 2.3304\n",
      "Batch 73, Loss: 2.3593\n",
      "Batch 74, Loss: 2.3620\n",
      "Batch 75, Loss: 2.3321\n",
      "Batch 76, Loss: 2.3079\n",
      "Batch 77, Loss: 2.3508\n",
      "Batch 78, Loss: 2.3953\n",
      "Batch 79, Loss: 2.3776\n",
      "Batch 80, Loss: 2.2992\n",
      "Batch 81, Loss: 2.3772\n",
      "Batch 82, Loss: 2.3511\n",
      "Batch 83, Loss: 2.3579\n",
      "Batch 84, Loss: 2.3960\n",
      "Batch 85, Loss: 2.3436\n",
      "Batch 86, Loss: 2.3752\n",
      "Batch 87, Loss: 2.3689\n",
      "Batch 88, Loss: 2.3252\n",
      "Batch 89, Loss: 2.3106\n",
      "Batch 90, Loss: 2.3499\n",
      "Batch 91, Loss: 2.3123\n",
      "Batch 92, Loss: 2.3695\n",
      "Batch 93, Loss: 2.2877\n",
      "Batch 94, Loss: 2.3811\n",
      "Batch 95, Loss: 2.3532\n",
      "Batch 96, Loss: 2.3370\n",
      "Batch 97, Loss: 2.3461\n",
      "Batch 98, Loss: 2.3879\n",
      "Batch 99, Loss: 2.3405\n",
      "Batch 100, Loss: 2.3491\n",
      "Batch 101, Loss: 2.2501\n",
      "Batch 102, Loss: 2.3443\n",
      "Batch 103, Loss: 2.3543\n",
      "Batch 104, Loss: 2.3590\n",
      "Batch 105, Loss: 2.3329\n",
      "Batch 106, Loss: 2.2743\n",
      "Batch 107, Loss: 2.3464\n",
      "Batch 108, Loss: 2.3877\n",
      "Batch 109, Loss: 2.3655\n",
      "Batch 110, Loss: 2.3932\n",
      "Batch 111, Loss: 2.3296\n",
      "Batch 112, Loss: 2.3617\n",
      "Batch 113, Loss: 2.2616\n",
      "Batch 114, Loss: 2.3446\n",
      "Batch 115, Loss: 2.3796\n",
      "Batch 116, Loss: 2.3436\n",
      "Batch 117, Loss: 2.3597\n",
      "Batch 118, Loss: 2.3371\n",
      "Batch 119, Loss: 2.2650\n",
      "Batch 120, Loss: 2.2238\n",
      "Batch 121, Loss: 2.2653\n",
      "Batch 122, Loss: 2.3818\n",
      "Batch 123, Loss: 2.0761\n",
      "Batch 124, Loss: 2.3784\n",
      "Batch 125, Loss: 2.4032\n",
      "Batch 126, Loss: 2.3820\n",
      "Batch 127, Loss: 2.3936\n",
      "Batch 128, Loss: 2.3803\n",
      "Batch 129, Loss: 2.3784\n",
      "Batch 130, Loss: 2.3410\n",
      "Batch 131, Loss: 2.3919\n",
      "Batch 132, Loss: 2.3529\n",
      "Batch 133, Loss: 2.2635\n",
      "Batch 134, Loss: 2.3328\n",
      "Batch 135, Loss: 2.3472\n",
      "Batch 136, Loss: 2.3624\n",
      "Batch 137, Loss: 2.2473\n",
      "Batch 138, Loss: 2.3401\n",
      "Batch 139, Loss: 2.2822\n",
      "Batch 140, Loss: 2.3813\n",
      "Batch 141, Loss: 2.3876\n",
      "Batch 142, Loss: 2.3458\n",
      "Batch 143, Loss: 2.3610\n",
      "Batch 144, Loss: 2.3318\n",
      "Batch 145, Loss: 2.3009\n",
      "Batch 146, Loss: 2.2491\n",
      "Batch 147, Loss: 2.3630\n",
      "Batch 148, Loss: 2.3402\n",
      "Batch 149, Loss: 2.3419\n",
      "Batch 150, Loss: 2.2390\n",
      "Batch 151, Loss: 2.3367\n",
      "Batch 152, Loss: 2.3792\n",
      "Batch 153, Loss: 2.3304\n",
      "Batch 154, Loss: 2.3543\n",
      "Batch 155, Loss: 2.3431\n",
      "Batch 156, Loss: 2.3320\n",
      "Batch 157, Loss: 2.2848\n",
      "Batch 158, Loss: 2.3591\n",
      "Batch 159, Loss: 2.3263\n",
      "Batch 160, Loss: 2.3674\n",
      "Batch 161, Loss: 2.3464\n",
      "Batch 162, Loss: 2.3726\n",
      "Batch 163, Loss: 2.3194\n",
      "Batch 164, Loss: 2.3618\n",
      "Batch 165, Loss: 2.3345\n",
      "Batch 166, Loss: 2.3455\n",
      "Batch 167, Loss: 2.3366\n",
      "Batch 168, Loss: 2.3710\n",
      "Batch 169, Loss: 2.3555\n",
      "Batch 170, Loss: 2.2894\n",
      "Batch 171, Loss: 2.3301\n",
      "Batch 172, Loss: 2.3180\n",
      "Batch 173, Loss: 2.3629\n",
      "Batch 174, Loss: 2.3212\n",
      "Batch 175, Loss: 2.3255\n",
      "Batch 176, Loss: 2.3144\n",
      "Batch 177, Loss: 2.3036\n",
      "Batch 178, Loss: 2.3502\n",
      "Batch 179, Loss: 2.3577\n",
      "Batch 180, Loss: 2.3580\n",
      "Batch 181, Loss: 2.3045\n",
      "Batch 182, Loss: 2.3504\n",
      "Batch 183, Loss: 2.3426\n",
      "Batch 184, Loss: 2.2201\n",
      "Batch 185, Loss: 2.2307\n",
      "Batch 186, Loss: 2.3336\n",
      "Batch 187, Loss: 2.3184\n",
      "Batch 188, Loss: 2.3057\n",
      "Batch 189, Loss: 2.1973\n",
      "Batch 190, Loss: 2.3514\n",
      "Batch 191, Loss: 2.2927\n",
      "Batch 192, Loss: 2.3052\n",
      "Batch 193, Loss: 2.3004\n",
      "Batch 194, Loss: 2.2155\n",
      "Batch 195, Loss: 2.2619\n",
      "Batch 196, Loss: 2.1872\n",
      "Batch 197, Loss: 2.3249\n",
      "Batch 198, Loss: 2.3282\n",
      "Batch 199, Loss: 2.3333\n",
      "Batch 200, Loss: 2.2852\n",
      "Once there was a two baby who wanted to jold his jacks. He looked around his motored, \"Ex, sun! What do you dist?\" The moter said, \"Thank you from then \n",
      "come out together from milk.\" Inside, there was a bright pular. It sounded like a broken pular. Everywhere the people got to lift to the perswat. One night, the pole \n",
      "was strong in their very house. He decided to help on Bob. He by. Then he went to the pole to the pole and got each time for the pole \n",
      "in. The motoral fair made everyone else the people the pole's jealous. The pole was so proud of being iglowerful to be with it at him. All the end of \n",
      "this things were hand in the others. He was so happy he'. Paddy and the pole were deep when they saw a cloth holden in the motor. They all played \n",
      "and laughings. Peing was very happy. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 23.23 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.3438\n",
      "Batch 202, Loss: 2.3157\n",
      "Batch 203, Loss: 2.2466\n",
      "Batch 204, Loss: 2.2242\n",
      "Batch 205, Loss: 2.3154\n",
      "Batch 206, Loss: 2.3727\n",
      "Batch 207, Loss: 2.3038\n",
      "Batch 208, Loss: 2.2333\n",
      "Batch 209, Loss: 2.3239\n",
      "Batch 210, Loss: 2.3300\n",
      "Batch 211, Loss: 2.2129\n",
      "Batch 212, Loss: 2.3477\n",
      "Batch 213, Loss: 2.2962\n",
      "Batch 214, Loss: 2.3562\n",
      "Batch 215, Loss: 2.2503\n",
      "Batch 216, Loss: 2.3522\n",
      "Batch 217, Loss: 2.2388\n",
      "Batch 218, Loss: 2.2469\n",
      "Batch 219, Loss: 2.1879\n",
      "Batch 220, Loss: 2.2736\n",
      "Batch 221, Loss: 2.2516\n",
      "Batch 222, Loss: 2.3238\n",
      "Batch 223, Loss: 2.3507\n",
      "Batch 224, Loss: 2.2529\n",
      "Batch 225, Loss: 2.3391\n",
      "Batch 226, Loss: 2.2822\n",
      "Batch 227, Loss: 2.2937\n",
      "Batch 228, Loss: 2.3256\n",
      "Batch 229, Loss: 2.2296\n",
      "Batch 230, Loss: 2.3079\n",
      "Batch 231, Loss: 2.3197\n",
      "Batch 232, Loss: 2.3301\n",
      "Batch 233, Loss: 2.2855\n",
      "Batch 234, Loss: 2.3176\n",
      "Batch 235, Loss: 2.2196\n",
      "Batch 236, Loss: 2.3282\n",
      "Batch 237, Loss: 2.3099\n",
      "Batch 238, Loss: 2.2826\n",
      "Batch 239, Loss: 2.2829\n",
      "Batch 240, Loss: 2.3084\n",
      "Batch 241, Loss: 2.3071\n",
      "Batch 242, Loss: 2.3013\n",
      "Batch 243, Loss: 2.3208\n",
      "Batch 244, Loss: 2.3438\n",
      "Batch 245, Loss: 2.2694\n",
      "Batch 246, Loss: 2.2712\n",
      "Batch 247, Loss: 2.2714\n",
      "Batch 248, Loss: 2.2883\n",
      "Batch 249, Loss: 2.1975\n",
      "Batch 250, Loss: 2.3075\n",
      "Batch 251, Loss: 2.2806\n",
      "Batch 252, Loss: 2.3062\n",
      "Batch 253, Loss: 2.2829\n",
      "Batch 254, Loss: 2.2851\n",
      "Batch 255, Loss: 2.3147\n",
      "Batch 256, Loss: 2.2841\n",
      "Batch 257, Loss: 2.2891\n",
      "Batch 258, Loss: 2.2817\n",
      "Batch 259, Loss: 2.2621\n",
      "Batch 260, Loss: 2.1925\n",
      "Batch 261, Loss: 2.2535\n",
      "Batch 262, Loss: 2.2784\n",
      "Batch 263, Loss: 2.3023\n",
      "Batch 264, Loss: 2.2892\n",
      "Batch 265, Loss: 2.2673\n",
      "Batch 266, Loss: 2.2929\n",
      "Batch 267, Loss: 2.2928\n",
      "Batch 268, Loss: 2.2915\n",
      "Batch 269, Loss: 2.2852\n",
      "Batch 270, Loss: 2.2852\n",
      "Batch 271, Loss: 2.2924\n",
      "Batch 272, Loss: 2.1611\n",
      "Batch 273, Loss: 2.2555\n",
      "Batch 274, Loss: 2.2981\n",
      "Batch 275, Loss: 2.1731\n",
      "Batch 276, Loss: 2.2836\n",
      "Batch 277, Loss: 2.2837\n",
      "Batch 278, Loss: 2.2469\n",
      "Batch 279, Loss: 2.2961\n",
      "Batch 280, Loss: 2.3042\n",
      "Batch 281, Loss: 2.1948\n",
      "Batch 282, Loss: 2.2709\n",
      "Batch 283, Loss: 2.2482\n",
      "Batch 284, Loss: 2.2579\n",
      "Batch 285, Loss: 2.2863\n",
      "Batch 286, Loss: 2.2672\n",
      "Batch 287, Loss: 2.3009\n",
      "Batch 288, Loss: 2.2443\n",
      "Batch 289, Loss: 2.2958\n",
      "Batch 290, Loss: 2.2759\n",
      "Batch 291, Loss: 2.2780\n",
      "Batch 292, Loss: 2.2596\n",
      "Batch 293, Loss: 2.2571\n",
      "Batch 294, Loss: 2.2667\n",
      "Batch 295, Loss: 2.2527\n",
      "Batch 296, Loss: 2.2152\n",
      "Batch 297, Loss: 2.2524\n",
      "Batch 298, Loss: 2.2686\n",
      "Batch 299, Loss: 2.1385\n",
      "Batch 300, Loss: 2.2806\n",
      "Batch 301, Loss: 2.2914\n",
      "Batch 302, Loss: 2.2543\n",
      "Batch 303, Loss: 2.2239\n",
      "Batch 304, Loss: 2.2996\n",
      "Batch 305, Loss: 2.2680\n",
      "Batch 306, Loss: 2.2697\n",
      "Batch 307, Loss: 2.2645\n",
      "Batch 308, Loss: 2.2264\n",
      "Batch 309, Loss: 2.2143\n",
      "Batch 310, Loss: 2.2190\n",
      "Batch 311, Loss: 2.2820\n",
      "Batch 312, Loss: 2.2866\n",
      "Batch 313, Loss: 2.2584\n",
      "Batch 314, Loss: 2.1488\n",
      "Batch 315, Loss: 2.1919\n",
      "Batch 316, Loss: 2.2841\n",
      "Batch 317, Loss: 2.2247\n",
      "Batch 318, Loss: 2.2588\n",
      "Batch 319, Loss: 2.3005\n",
      "Batch 320, Loss: 2.2249\n",
      "Batch 321, Loss: 2.1751\n",
      "Batch 322, Loss: 2.2983\n",
      "Batch 323, Loss: 2.2795\n",
      "Batch 324, Loss: 2.1962\n",
      "Batch 325, Loss: 2.2032\n",
      "Batch 326, Loss: 2.1970\n",
      "Batch 327, Loss: 2.2066\n",
      "Batch 328, Loss: 2.1979\n",
      "Batch 329, Loss: 2.2884\n",
      "Batch 330, Loss: 2.2738\n",
      "Batch 331, Loss: 2.2610\n",
      "Batch 332, Loss: 2.1712\n",
      "Batch 333, Loss: 2.1969\n",
      "Batch 334, Loss: 2.2476\n",
      "Batch 335, Loss: 2.2432\n",
      "Batch 336, Loss: 2.2869\n",
      "Batch 337, Loss: 2.2431\n",
      "Batch 338, Loss: 2.2719\n",
      "Batch 339, Loss: 2.2833\n",
      "Batch 340, Loss: 2.3073\n",
      "Batch 341, Loss: 2.2740\n",
      "Batch 342, Loss: 2.1889\n",
      "Batch 343, Loss: 2.1681\n",
      "Batch 344, Loss: 2.1782\n",
      "Batch 345, Loss: 2.1504\n",
      "Batch 346, Loss: 2.1962\n",
      "Batch 347, Loss: 2.2887\n",
      "Batch 348, Loss: 2.2787\n",
      "Batch 349, Loss: 2.3031\n",
      "Batch 350, Loss: 2.3128\n",
      "Batch 351, Loss: 2.2657\n",
      "Batch 352, Loss: 2.2446\n",
      "Batch 353, Loss: 2.2792\n",
      "Batch 354, Loss: 2.2250\n",
      "Batch 355, Loss: 2.1756\n",
      "Batch 356, Loss: 2.2574\n",
      "Batch 357, Loss: 2.2704\n",
      "Batch 358, Loss: 2.2449\n",
      "Batch 359, Loss: 2.2216\n",
      "Batch 360, Loss: 2.2743\n",
      "Batch 361, Loss: 2.2581\n",
      "Batch 362, Loss: 2.2803\n",
      "Batch 363, Loss: 2.2367\n",
      "Batch 364, Loss: 2.2627\n",
      "Batch 365, Loss: 2.2226\n",
      "Batch 366, Loss: 2.2680\n",
      "Batch 367, Loss: 2.1603\n",
      "Batch 368, Loss: 2.2889\n",
      "Batch 369, Loss: 2.2409\n",
      "Batch 370, Loss: 2.2837\n",
      "Batch 371, Loss: 2.2441\n",
      "Batch 372, Loss: 2.2716\n",
      "Batch 373, Loss: 2.2675\n",
      "Batch 374, Loss: 2.2815\n",
      "Batch 375, Loss: 2.2735\n",
      "Batch 376, Loss: 2.2893\n",
      "Batch 377, Loss: 2.2583\n",
      "Batch 378, Loss: 2.2589\n",
      "Epoch 6, Average Loss: 2.3074\n",
      "Once upon a time there was a bear and a bear who was very sick. He liked to play outside and explore the offer all day. One day he bug \n",
      "an old man and ran for a stand in the middle of his magical new books. He saw something very weak. The old man decided to presistare the old man \n",
      "so hard to explore. He walked closer to look us to all the birds. He carefully let go away and finally found a beautiful standows. Every day he flew around \n",
      "with the stand, and it was gone. He waited in sadly taste to him. Then he found a pretty game, wondering a lot of new ways and miss helping everywhere. \n",
      "He turned the tree master miss how happy him he needed, at the nothing food behavited. He stepped along it up. After some time to his friend heart tight. He \n",
      "found that his special helper had made a better look help until he had rub Momed him in its magic was on the stand. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.2407\n",
      "Batch 2, Loss: 2.2207\n",
      "Batch 3, Loss: 2.2227\n",
      "Batch 4, Loss: 2.1929\n",
      "Batch 5, Loss: 2.2249\n",
      "Batch 6, Loss: 2.2586\n",
      "Batch 7, Loss: 2.1611\n",
      "Batch 8, Loss: 2.2588\n",
      "Batch 9, Loss: 2.2539\n",
      "Batch 10, Loss: 2.2468\n",
      "Batch 11, Loss: 2.2107\n",
      "Batch 12, Loss: 1.9625\n",
      "Batch 13, Loss: 2.2476\n",
      "Batch 14, Loss: 2.2197\n",
      "Batch 15, Loss: 2.1459\n",
      "Batch 16, Loss: 2.2033\n",
      "Batch 17, Loss: 2.1535\n",
      "Batch 18, Loss: 2.2121\n",
      "Batch 19, Loss: 2.2807\n",
      "Batch 20, Loss: 2.2288\n",
      "Batch 21, Loss: 2.1644\n",
      "Batch 22, Loss: 2.2434\n",
      "Batch 23, Loss: 2.2358\n",
      "Batch 24, Loss: 2.2449\n",
      "Batch 25, Loss: 2.2714\n",
      "Batch 26, Loss: 2.3014\n",
      "Batch 27, Loss: 2.2114\n",
      "Batch 28, Loss: 2.2423\n",
      "Batch 29, Loss: 2.2594\n",
      "Batch 30, Loss: 2.1285\n",
      "Batch 31, Loss: 2.2665\n",
      "Batch 32, Loss: 2.2237\n",
      "Batch 33, Loss: 2.1479\n",
      "Batch 34, Loss: 2.2652\n",
      "Batch 35, Loss: 2.1899\n",
      "Batch 36, Loss: 2.2502\n",
      "Batch 37, Loss: 2.2572\n",
      "Batch 38, Loss: 2.2572\n",
      "Batch 39, Loss: 2.2500\n",
      "Batch 40, Loss: 2.2500\n",
      "Batch 41, Loss: 2.2264\n",
      "Batch 42, Loss: 2.1798\n",
      "Batch 43, Loss: 2.2587\n",
      "Batch 44, Loss: 2.2302\n",
      "Batch 45, Loss: 2.1972\n",
      "Batch 46, Loss: 2.1352\n",
      "Batch 47, Loss: 2.1260\n",
      "Batch 48, Loss: 2.2227\n",
      "Batch 49, Loss: 2.1200\n",
      "Batch 50, Loss: 2.2361\n",
      "Batch 51, Loss: 2.2507\n",
      "Batch 52, Loss: 2.2446\n",
      "Batch 53, Loss: 2.1498\n",
      "Batch 54, Loss: 2.2258\n",
      "Batch 55, Loss: 2.2359\n",
      "Batch 56, Loss: 2.2388\n",
      "Batch 57, Loss: 2.2363\n",
      "Batch 58, Loss: 2.1986\n",
      "Batch 59, Loss: 2.2232\n",
      "Batch 60, Loss: 2.2645\n",
      "Batch 61, Loss: 2.2181\n",
      "Batch 62, Loss: 2.1996\n",
      "Batch 63, Loss: 2.2365\n",
      "Batch 64, Loss: 2.2237\n",
      "Batch 65, Loss: 2.1915\n",
      "Batch 66, Loss: 2.2470\n",
      "Batch 67, Loss: 2.0894\n",
      "Batch 68, Loss: 2.2081\n",
      "Batch 69, Loss: 2.1558\n",
      "Batch 70, Loss: 2.2028\n",
      "Batch 71, Loss: 2.2418\n",
      "Batch 72, Loss: 2.2134\n",
      "Batch 73, Loss: 2.2137\n",
      "Batch 74, Loss: 2.1521\n",
      "Batch 75, Loss: 2.1610\n",
      "Batch 76, Loss: 2.1529\n",
      "Batch 77, Loss: 2.2282\n",
      "Batch 78, Loss: 2.1665\n",
      "Batch 79, Loss: 2.2442\n",
      "Batch 80, Loss: 2.2226\n",
      "Batch 81, Loss: 2.2237\n",
      "Batch 82, Loss: 2.1120\n",
      "Batch 83, Loss: 2.2064\n",
      "Batch 84, Loss: 2.2199\n",
      "Batch 85, Loss: 2.2242\n",
      "Batch 86, Loss: 2.1770\n",
      "Batch 87, Loss: 2.1881\n",
      "Batch 88, Loss: 2.2287\n",
      "Batch 89, Loss: 2.2392\n",
      "Batch 90, Loss: 2.2692\n",
      "Batch 91, Loss: 2.2273\n",
      "Batch 92, Loss: 2.2318\n",
      "Batch 93, Loss: 2.2791\n",
      "Batch 94, Loss: 2.1326\n",
      "Batch 95, Loss: 2.1846\n",
      "Batch 96, Loss: 2.2161\n",
      "Batch 97, Loss: 2.2223\n",
      "Batch 98, Loss: 2.1133\n",
      "Batch 99, Loss: 2.1984\n",
      "Batch 100, Loss: 2.1808\n",
      "Batch 101, Loss: 2.2463\n",
      "Batch 102, Loss: 2.2560\n",
      "Batch 103, Loss: 2.2397\n",
      "Batch 104, Loss: 2.1968\n",
      "Batch 105, Loss: 2.1598\n",
      "Batch 106, Loss: 2.2012\n",
      "Batch 107, Loss: 2.2112\n",
      "Batch 108, Loss: 2.2197\n",
      "Batch 109, Loss: 2.1617\n",
      "Batch 110, Loss: 2.1802\n",
      "Batch 111, Loss: 2.1003\n",
      "Batch 112, Loss: 2.2183\n",
      "Batch 113, Loss: 2.0890\n",
      "Batch 114, Loss: 2.2184\n",
      "Batch 115, Loss: 2.2209\n",
      "Batch 116, Loss: 2.2157\n",
      "Batch 117, Loss: 2.1016\n",
      "Batch 118, Loss: 2.1398\n",
      "Batch 119, Loss: 2.1945\n",
      "Batch 120, Loss: 2.1823\n",
      "Batch 121, Loss: 2.1972\n",
      "Batch 122, Loss: 2.2303\n",
      "Batch 123, Loss: 2.2020\n",
      "Batch 124, Loss: 2.1578\n",
      "Batch 125, Loss: 2.2235\n",
      "Batch 126, Loss: 2.1517\n",
      "Batch 127, Loss: 2.1942\n",
      "Batch 128, Loss: 2.2099\n",
      "Batch 129, Loss: 2.2300\n",
      "Batch 130, Loss: 2.1298\n",
      "Batch 131, Loss: 2.1545\n",
      "Batch 132, Loss: 2.1536\n",
      "Batch 133, Loss: 2.1500\n",
      "Batch 134, Loss: 2.1896\n",
      "Batch 135, Loss: 2.1769\n",
      "Batch 136, Loss: 2.1615\n",
      "Batch 137, Loss: 2.0771\n",
      "Batch 138, Loss: 2.0889\n",
      "Batch 139, Loss: 2.1493\n",
      "Batch 140, Loss: 2.2072\n",
      "Batch 141, Loss: 2.2042\n",
      "Batch 142, Loss: 2.2123\n",
      "Batch 143, Loss: 2.1648\n",
      "Batch 144, Loss: 2.2027\n",
      "Batch 145, Loss: 2.1868\n",
      "Batch 146, Loss: 2.1546\n",
      "Batch 147, Loss: 2.1974\n",
      "Batch 148, Loss: 2.1199\n",
      "Batch 149, Loss: 2.2336\n",
      "Batch 150, Loss: 2.2322\n",
      "Batch 151, Loss: 2.2259\n",
      "Batch 152, Loss: 2.2022\n",
      "Batch 153, Loss: 2.2242\n",
      "Batch 154, Loss: 2.1939\n",
      "Batch 155, Loss: 2.1862\n",
      "Batch 156, Loss: 2.1978\n",
      "Batch 157, Loss: 2.2236\n",
      "Batch 158, Loss: 2.2102\n",
      "Batch 159, Loss: 2.1487\n",
      "Batch 160, Loss: 2.1945\n",
      "Batch 161, Loss: 2.2159\n",
      "Batch 162, Loss: 2.2127\n",
      "Batch 163, Loss: 2.1859\n",
      "Batch 164, Loss: 2.2235\n",
      "Batch 165, Loss: 2.1101\n",
      "Batch 166, Loss: 2.1598\n",
      "Batch 167, Loss: 2.2045\n",
      "Batch 168, Loss: 2.2120\n",
      "Batch 169, Loss: 2.1270\n",
      "Batch 170, Loss: 2.1994\n",
      "Batch 171, Loss: 2.0737\n",
      "Batch 172, Loss: 2.1812\n",
      "Batch 173, Loss: 2.1091\n",
      "Batch 174, Loss: 2.1571\n",
      "Batch 175, Loss: 2.1974\n",
      "Batch 176, Loss: 2.0776\n",
      "Batch 177, Loss: 2.2026\n",
      "Batch 178, Loss: 2.1874\n",
      "Batch 179, Loss: 2.2324\n",
      "Batch 180, Loss: 2.1756\n",
      "Batch 181, Loss: 2.2023\n",
      "Batch 182, Loss: 2.2013\n",
      "Batch 183, Loss: 2.2111\n",
      "Batch 184, Loss: 2.0901\n",
      "Batch 185, Loss: 2.1394\n",
      "Batch 186, Loss: 2.1424\n",
      "Batch 187, Loss: 2.1130\n",
      "Batch 188, Loss: 2.1966\n",
      "Batch 189, Loss: 2.1716\n",
      "Batch 190, Loss: 2.2104\n",
      "Batch 191, Loss: 2.2067\n",
      "Batch 192, Loss: 2.1911\n",
      "Batch 193, Loss: 2.1453\n",
      "Batch 194, Loss: 2.2453\n",
      "Batch 195, Loss: 2.1937\n",
      "Batch 196, Loss: 2.1952\n",
      "Batch 197, Loss: 2.1865\n",
      "Batch 198, Loss: 2.1035\n",
      "Batch 199, Loss: 2.1841\n",
      "Batch 200, Loss: 2.1837\n",
      "Sara and Ben were playing with their toy houses in the park. They liked to make different and flowers. They had lots of other bugs each at animals. They had \n",
      "fun with spin ask and trucks. One day, their mom and dad took their arm coats. It was thin and Ben had brown and bugs to make it broken. She \n",
      "told them not to share with the car for their mom's neck. She hugged it to see that they were hungry. She started to smell it. \"Ouch!\" Ben said, holding \n",
      "up the world from that makes us. \"It's a big change!\" Sara smiled. She was very angry and happy. They roll it down on their spin into their hands. \"Help! \n",
      "Wow here is a very noisor. The people said it will beon!\" Sara exclaimed. \"You together! That is very lovely asking. Just go!\" Tom and Ben were scared and angry. \n",
      "They said sorry to ourselves. They ran back to the animal together. There was very brave and cried. \"Who did the world?\" Sara smiled. \"It is not memiting. You have \n",
      "to fix them someone go heart with your mom. It's your neck is red and strong.\" Sara smiled. She said, smiling. She did not want to play with your toys. \n",
      "She loves them in the grass and play with the world. She pickeds them off the toy inside the drawer. \"Lily, okay. The world was scare you have not played \n",
      "with our ocean. It's bad light and you can have your toy? You should always be careful.\" Sara was happy. She took Sara and thought Ben she liked the mistake \n",
      "and so much. She also learned that evening other things can be caring. They remembered to be happy again. They learned that some world and be happy. They learned a \n",
      "valuable to smell fix the world. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 27.76 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.2170\n",
      "Batch 202, Loss: 2.1696\n",
      "Batch 203, Loss: 2.1917\n",
      "Batch 204, Loss: 2.1770\n",
      "Batch 205, Loss: 2.1771\n",
      "Batch 206, Loss: 2.1885\n",
      "Batch 207, Loss: 2.1221\n",
      "Batch 208, Loss: 2.0837\n",
      "Batch 209, Loss: 2.2350\n",
      "Batch 210, Loss: 2.2146\n",
      "Batch 211, Loss: 2.1178\n",
      "Batch 212, Loss: 2.1349\n",
      "Batch 213, Loss: 2.1825\n",
      "Batch 214, Loss: 2.0638\n",
      "Batch 215, Loss: 2.1157\n",
      "Batch 216, Loss: 2.1230\n",
      "Batch 217, Loss: 2.2110\n",
      "Batch 218, Loss: 2.1526\n",
      "Batch 219, Loss: 2.1815\n",
      "Batch 220, Loss: 2.1982\n",
      "Batch 221, Loss: 2.1854\n",
      "Batch 222, Loss: 2.1930\n",
      "Batch 223, Loss: 2.0724\n",
      "Batch 224, Loss: 2.1725\n",
      "Batch 225, Loss: 2.1599\n",
      "Batch 226, Loss: 2.1155\n",
      "Batch 227, Loss: 2.1977\n",
      "Batch 228, Loss: 2.2241\n",
      "Batch 229, Loss: 2.1937\n",
      "Batch 230, Loss: 2.1508\n",
      "Batch 231, Loss: 2.1706\n",
      "Batch 232, Loss: 2.2044\n",
      "Batch 233, Loss: 2.1830\n",
      "Batch 234, Loss: 2.1504\n",
      "Batch 235, Loss: 2.1489\n",
      "Batch 236, Loss: 2.1765\n",
      "Batch 237, Loss: 2.1498\n",
      "Batch 238, Loss: 2.0920\n",
      "Batch 239, Loss: 2.1316\n",
      "Batch 240, Loss: 2.2022\n",
      "Batch 241, Loss: 2.1602\n",
      "Batch 242, Loss: 2.1637\n",
      "Batch 243, Loss: 2.1817\n",
      "Batch 244, Loss: 2.0923\n",
      "Batch 245, Loss: 2.1623\n",
      "Batch 246, Loss: 2.2224\n",
      "Batch 247, Loss: 2.1377\n",
      "Batch 248, Loss: 2.1801\n",
      "Batch 249, Loss: 2.1586\n",
      "Batch 250, Loss: 2.1601\n",
      "Batch 251, Loss: 2.1588\n",
      "Batch 252, Loss: 2.1545\n",
      "Batch 253, Loss: 2.1605\n",
      "Batch 254, Loss: 2.1166\n",
      "Batch 255, Loss: 2.2028\n",
      "Batch 256, Loss: 2.0907\n",
      "Batch 257, Loss: 2.1056\n",
      "Batch 258, Loss: 2.1445\n",
      "Batch 259, Loss: 2.0732\n",
      "Batch 260, Loss: 2.1481\n",
      "Batch 261, Loss: 2.1771\n",
      "Batch 262, Loss: 2.1415\n",
      "Batch 263, Loss: 2.1642\n",
      "Batch 264, Loss: 2.1606\n",
      "Batch 265, Loss: 2.1040\n",
      "Batch 266, Loss: 2.0894\n",
      "Batch 267, Loss: 2.1520\n",
      "Batch 268, Loss: 2.1903\n",
      "Batch 269, Loss: 2.1965\n",
      "Batch 270, Loss: 2.1801\n",
      "Batch 271, Loss: 2.1746\n",
      "Batch 272, Loss: 2.1444\n",
      "Batch 273, Loss: 2.1593\n",
      "Batch 274, Loss: 2.1569\n",
      "Batch 275, Loss: 2.1713\n",
      "Batch 276, Loss: 2.1664\n",
      "Batch 277, Loss: 2.1725\n",
      "Batch 278, Loss: 2.1495\n",
      "Batch 279, Loss: 2.1450\n",
      "Batch 280, Loss: 2.1676\n",
      "Batch 281, Loss: 2.1523\n",
      "Batch 282, Loss: 2.1713\n",
      "Batch 283, Loss: 2.1604\n",
      "Batch 284, Loss: 2.0743\n",
      "Batch 285, Loss: 2.1457\n",
      "Batch 286, Loss: 2.1634\n",
      "Batch 287, Loss: 2.1545\n",
      "Batch 288, Loss: 2.0854\n",
      "Batch 289, Loss: 2.1829\n",
      "Batch 290, Loss: 2.1498\n",
      "Batch 291, Loss: 2.0884\n",
      "Batch 292, Loss: 2.0948\n",
      "Batch 293, Loss: 2.1542\n",
      "Batch 294, Loss: 2.2066\n",
      "Batch 295, Loss: 2.1567\n",
      "Batch 296, Loss: 2.1757\n",
      "Batch 297, Loss: 2.1532\n",
      "Batch 298, Loss: 2.1692\n",
      "Batch 299, Loss: 2.1737\n",
      "Batch 300, Loss: 2.1548\n",
      "Batch 301, Loss: 2.1667\n",
      "Batch 302, Loss: 2.0931\n",
      "Batch 303, Loss: 2.1588\n",
      "Batch 304, Loss: 2.1287\n",
      "Batch 305, Loss: 2.1423\n",
      "Batch 306, Loss: 2.1393\n",
      "Batch 307, Loss: 2.1799\n",
      "Batch 308, Loss: 2.1706\n",
      "Batch 309, Loss: 2.1405\n",
      "Batch 310, Loss: 2.1558\n",
      "Batch 311, Loss: 2.1531\n",
      "Batch 312, Loss: 2.1730\n",
      "Batch 313, Loss: 2.1499\n",
      "Batch 314, Loss: 2.1841\n",
      "Batch 315, Loss: 2.1122\n",
      "Batch 316, Loss: 2.1827\n",
      "Batch 317, Loss: 2.1612\n",
      "Batch 318, Loss: 2.1526\n",
      "Batch 319, Loss: 2.1666\n",
      "Batch 320, Loss: 2.1809\n",
      "Batch 321, Loss: 2.1385\n",
      "Batch 322, Loss: 2.1507\n",
      "Batch 323, Loss: 2.1820\n",
      "Batch 324, Loss: 2.1647\n",
      "Batch 325, Loss: 2.1744\n",
      "Batch 326, Loss: 2.1379\n",
      "Batch 327, Loss: 2.0465\n",
      "Batch 328, Loss: 2.1730\n",
      "Batch 329, Loss: 2.1470\n",
      "Batch 330, Loss: 2.1189\n",
      "Batch 331, Loss: 2.1612\n",
      "Batch 332, Loss: 2.1670\n",
      "Batch 333, Loss: 2.1760\n",
      "Batch 334, Loss: 2.0378\n",
      "Batch 335, Loss: 2.1438\n",
      "Batch 336, Loss: 2.1611\n",
      "Batch 337, Loss: 2.0144\n",
      "Batch 338, Loss: 2.1475\n",
      "Batch 339, Loss: 2.1541\n",
      "Batch 340, Loss: 2.1537\n",
      "Batch 341, Loss: 2.1653\n",
      "Batch 342, Loss: 2.1506\n",
      "Batch 343, Loss: 2.1589\n",
      "Batch 344, Loss: 2.1805\n",
      "Batch 345, Loss: 2.1500\n",
      "Batch 346, Loss: 2.1522\n",
      "Batch 347, Loss: 2.1676\n",
      "Batch 348, Loss: 2.1264\n",
      "Batch 349, Loss: 2.0177\n",
      "Batch 350, Loss: 2.0991\n",
      "Batch 351, Loss: 2.1586\n",
      "Batch 352, Loss: 2.1369\n",
      "Batch 353, Loss: 2.1949\n",
      "Batch 354, Loss: 2.1624\n",
      "Batch 355, Loss: 2.1677\n",
      "Batch 356, Loss: 2.1112\n",
      "Batch 357, Loss: 2.0963\n",
      "Batch 358, Loss: 2.1667\n",
      "Batch 359, Loss: 2.1569\n",
      "Batch 360, Loss: 2.1801\n",
      "Batch 361, Loss: 2.0625\n",
      "Batch 362, Loss: 2.1780\n",
      "Batch 363, Loss: 2.1423\n",
      "Batch 364, Loss: 2.1504\n",
      "Batch 365, Loss: 2.1443\n",
      "Batch 366, Loss: 2.1629\n",
      "Batch 367, Loss: 2.1222\n",
      "Batch 368, Loss: 2.1738\n",
      "Batch 369, Loss: 2.1414\n",
      "Batch 370, Loss: 2.1103\n",
      "Batch 371, Loss: 2.1811\n",
      "Batch 372, Loss: 2.1519\n",
      "Batch 373, Loss: 2.1379\n",
      "Batch 374, Loss: 2.0426\n",
      "Batch 375, Loss: 2.0224\n",
      "Batch 376, Loss: 2.1346\n",
      "Batch 377, Loss: 2.1331\n",
      "Batch 378, Loss: 2.0987\n",
      "Epoch 7, Average Loss: 2.1740\n",
      "One day, a boy named Bisy went to the park with his mom. Bubby liked to see the flowers every happy. One day, when they got to the park, they \n",
      "saw a big cochen with a pretty capes. The cocoon watched and spun the cap, but Sue wanted to display them around the room. Blue gave them a hug and \n",
      "said, \"What is the cochen! We can put a lollipipop for it!\" But Mom said. Mandy said, \"No, hide is scary to not reach mind.\" Blue thought for a moment. \n",
      "\"You can't play too, too!\" Mom felt proud of her mom. \"You're welcome, can hurt your mom. We don't take care of the cochen and the cochen. With now, can \n",
      "you home that too you don't spray?\" the cochen floured it out fromto her feet and said, \"Let's slide down, hone!\" After it reached the door, Sue's behind for someone \n",
      "to drate the bunch. Blue saw the boy. She heard the kite and swimming standing in its bed to the stitch. Blue behind her head out and chasing the cap \n",
      "tight on the quishance and flew away. Blue quickly opened the small cap, but she was lost in shadow. The door figure happily becomed and cross the cap away. She \n",
      "quickly crossed the window. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.0996\n",
      "Batch 2, Loss: 2.1340\n",
      "Batch 3, Loss: 2.1028\n",
      "Batch 4, Loss: 2.1204\n",
      "Batch 5, Loss: 2.1207\n",
      "Batch 6, Loss: 2.1233\n",
      "Batch 7, Loss: 2.0340\n",
      "Batch 8, Loss: 2.1042\n",
      "Batch 9, Loss: 2.1229\n",
      "Batch 10, Loss: 2.0316\n",
      "Batch 11, Loss: 2.1235\n",
      "Batch 12, Loss: 2.0596\n",
      "Batch 13, Loss: 2.0743\n",
      "Batch 14, Loss: 2.1446\n",
      "Batch 15, Loss: 2.1216\n",
      "Batch 16, Loss: 2.0824\n",
      "Batch 17, Loss: 2.0995\n",
      "Batch 18, Loss: 2.0186\n",
      "Batch 19, Loss: 2.1093\n",
      "Batch 20, Loss: 2.1177\n",
      "Batch 21, Loss: 2.0787\n",
      "Batch 22, Loss: 2.1659\n",
      "Batch 23, Loss: 2.1293\n",
      "Batch 24, Loss: 2.1091\n",
      "Batch 25, Loss: 2.0100\n",
      "Batch 26, Loss: 2.1257\n",
      "Batch 27, Loss: 2.0488\n",
      "Batch 28, Loss: 2.1372\n",
      "Batch 29, Loss: 1.9842\n",
      "Batch 30, Loss: 2.1230\n",
      "Batch 31, Loss: 2.1142\n",
      "Batch 32, Loss: 2.1365\n",
      "Batch 33, Loss: 2.0877\n",
      "Batch 34, Loss: 2.1191\n",
      "Batch 35, Loss: 2.1418\n",
      "Batch 36, Loss: 2.1206\n",
      "Batch 37, Loss: 2.1102\n",
      "Batch 38, Loss: 2.0841\n",
      "Batch 39, Loss: 2.1248\n",
      "Batch 40, Loss: 1.9912\n",
      "Batch 41, Loss: 2.1719\n",
      "Batch 42, Loss: 2.1514\n",
      "Batch 43, Loss: 2.1225\n",
      "Batch 44, Loss: 2.1418\n",
      "Batch 45, Loss: 2.1012\n",
      "Batch 46, Loss: 2.1098\n",
      "Batch 47, Loss: 2.1425\n",
      "Batch 48, Loss: 2.1247\n",
      "Batch 49, Loss: 2.1655\n",
      "Batch 50, Loss: 2.0914\n",
      "Batch 51, Loss: 2.1027\n",
      "Batch 52, Loss: 2.1566\n",
      "Batch 53, Loss: 2.1566\n",
      "Batch 54, Loss: 2.1311\n",
      "Batch 55, Loss: 2.1008\n",
      "Batch 56, Loss: 2.1586\n",
      "Batch 57, Loss: 2.0902\n",
      "Batch 58, Loss: 2.1078\n",
      "Batch 59, Loss: 2.0684\n",
      "Batch 60, Loss: 1.9943\n",
      "Batch 61, Loss: 2.1458\n",
      "Batch 62, Loss: 2.1185\n",
      "Batch 63, Loss: 2.0316\n",
      "Batch 64, Loss: 2.1286\n",
      "Batch 65, Loss: 2.1076\n",
      "Batch 66, Loss: 2.0471\n",
      "Batch 67, Loss: 2.1128\n",
      "Batch 68, Loss: 2.1654\n",
      "Batch 69, Loss: 2.1293\n",
      "Batch 70, Loss: 2.0127\n",
      "Batch 71, Loss: 2.1409\n",
      "Batch 72, Loss: 2.1098\n",
      "Batch 73, Loss: 2.1461\n",
      "Batch 74, Loss: 2.1054\n",
      "Batch 75, Loss: 2.0993\n",
      "Batch 76, Loss: 2.1464\n",
      "Batch 77, Loss: 2.0906\n",
      "Batch 78, Loss: 2.1492\n",
      "Batch 79, Loss: 2.1491\n",
      "Batch 80, Loss: 2.0710\n",
      "Batch 81, Loss: 2.0299\n",
      "Batch 82, Loss: 2.0613\n",
      "Batch 83, Loss: 2.1132\n",
      "Batch 84, Loss: 2.1177\n",
      "Batch 85, Loss: 2.0682\n",
      "Batch 86, Loss: 2.0694\n",
      "Batch 87, Loss: 2.1159\n",
      "Batch 88, Loss: 2.1215\n",
      "Batch 89, Loss: 2.0961\n",
      "Batch 90, Loss: 2.0769\n",
      "Batch 91, Loss: 2.1139\n",
      "Batch 92, Loss: 2.1180\n",
      "Batch 93, Loss: 2.1232\n",
      "Batch 94, Loss: 2.1220\n",
      "Batch 95, Loss: 2.0863\n",
      "Batch 96, Loss: 2.0947\n",
      "Batch 97, Loss: 2.0061\n",
      "Batch 98, Loss: 2.1433\n",
      "Batch 99, Loss: 2.1266\n",
      "Batch 100, Loss: 2.1063\n",
      "Batch 101, Loss: 2.1000\n",
      "Batch 102, Loss: 2.0938\n",
      "Batch 103, Loss: 2.0569\n",
      "Batch 104, Loss: 2.0955\n",
      "Batch 105, Loss: 2.0791\n",
      "Batch 106, Loss: 2.0766\n",
      "Batch 107, Loss: 2.1167\n",
      "Batch 108, Loss: 2.0969\n",
      "Batch 109, Loss: 2.1085\n",
      "Batch 110, Loss: 2.0668\n",
      "Batch 111, Loss: 2.1084\n",
      "Batch 112, Loss: 2.1047\n",
      "Batch 113, Loss: 2.0738\n",
      "Batch 114, Loss: 2.1111\n",
      "Batch 115, Loss: 2.1461\n",
      "Batch 116, Loss: 2.1084\n",
      "Batch 117, Loss: 2.1284\n",
      "Batch 118, Loss: 2.0174\n",
      "Batch 119, Loss: 2.1032\n",
      "Batch 120, Loss: 2.0147\n",
      "Batch 121, Loss: 1.9980\n",
      "Batch 122, Loss: 2.0596\n",
      "Batch 123, Loss: 2.1028\n",
      "Batch 124, Loss: 2.0280\n",
      "Batch 125, Loss: 2.1089\n",
      "Batch 126, Loss: 2.1175\n",
      "Batch 127, Loss: 2.0835\n",
      "Batch 128, Loss: 2.0943\n",
      "Batch 129, Loss: 2.1048\n",
      "Batch 130, Loss: 2.0823\n",
      "Batch 131, Loss: 2.0563\n",
      "Batch 132, Loss: 1.9867\n",
      "Batch 133, Loss: 2.0713\n",
      "Batch 134, Loss: 2.0611\n",
      "Batch 135, Loss: 2.1273\n",
      "Batch 136, Loss: 2.0839\n",
      "Batch 137, Loss: 2.0877\n",
      "Batch 138, Loss: 2.0934\n",
      "Batch 139, Loss: 2.1150\n",
      "Batch 140, Loss: 2.0936\n",
      "Batch 141, Loss: 2.0699\n",
      "Batch 142, Loss: 1.9991\n",
      "Batch 143, Loss: 2.1239\n",
      "Batch 144, Loss: 2.0786\n",
      "Batch 145, Loss: 2.0306\n",
      "Batch 146, Loss: 2.0974\n",
      "Batch 147, Loss: 2.0702\n",
      "Batch 148, Loss: 2.0995\n",
      "Batch 149, Loss: 2.1074\n",
      "Batch 150, Loss: 2.0582\n",
      "Batch 151, Loss: 2.1157\n",
      "Batch 152, Loss: 2.0855\n",
      "Batch 153, Loss: 2.0284\n",
      "Batch 154, Loss: 2.1293\n",
      "Batch 155, Loss: 2.1215\n",
      "Batch 156, Loss: 2.0936\n",
      "Batch 157, Loss: 2.0953\n",
      "Batch 158, Loss: 2.1024\n",
      "Batch 159, Loss: 2.0816\n",
      "Batch 160, Loss: 2.1078\n",
      "Batch 161, Loss: 2.1165\n",
      "Batch 162, Loss: 2.1173\n",
      "Batch 163, Loss: 2.1084\n",
      "Batch 164, Loss: 2.0941\n",
      "Batch 165, Loss: 2.1133\n",
      "Batch 166, Loss: 2.1127\n",
      "Batch 167, Loss: 2.1192\n",
      "Batch 168, Loss: 2.0608\n",
      "Batch 169, Loss: 2.0851\n",
      "Batch 170, Loss: 2.0775\n",
      "Batch 171, Loss: 2.1146\n",
      "Batch 172, Loss: 2.0553\n",
      "Batch 173, Loss: 2.1446\n",
      "Batch 174, Loss: 2.0723\n",
      "Batch 175, Loss: 2.0948\n",
      "Batch 176, Loss: 2.0888\n",
      "Batch 177, Loss: 2.0415\n",
      "Batch 178, Loss: 2.1049\n",
      "Batch 179, Loss: 2.0994\n",
      "Batch 180, Loss: 2.0913\n",
      "Batch 181, Loss: 2.0247\n",
      "Batch 182, Loss: 2.0880\n",
      "Batch 183, Loss: 2.0877\n",
      "Batch 184, Loss: 2.0103\n",
      "Batch 185, Loss: 2.0735\n",
      "Batch 186, Loss: 2.0900\n",
      "Batch 187, Loss: 2.0773\n",
      "Batch 188, Loss: 2.0979\n",
      "Batch 189, Loss: 2.0360\n",
      "Batch 190, Loss: 2.0179\n",
      "Batch 191, Loss: 2.1335\n",
      "Batch 192, Loss: 2.0976\n",
      "Batch 193, Loss: 2.0830\n",
      "Batch 194, Loss: 2.0966\n",
      "Batch 195, Loss: 2.0934\n",
      "Batch 196, Loss: 2.1002\n",
      "Batch 197, Loss: 2.0720\n",
      "Batch 198, Loss: 2.0920\n",
      "Batch 199, Loss: 2.0262\n",
      "Batch 200, Loss: 2.1086\n",
      "Once upon a time, there was a naughty boy named Jack. Jack loved playing with his chair. One day, Jack was making time to his and read a story. Jack \n",
      "met a sister. He was coming to his friend, a wing bird, and Jack, came to have funny. Jack and Jack wanted to go to the park, but Jack had \n",
      "done watch his mom in toes. Jack was so curious, he started walking and playing with their ball, but he couldn't, hear. Jack and Jack went to the park with \n",
      "their toys, and when Jack was done. Without Max and Daddy's wide and Jack came to help his friends. Jack decided to go and he started to play. The day \n",
      "Jack found his butre sister what it was. For the next day, Jack and Jack gathered from Jack, and Jack went home on his jazz. Jack was very very sad \n",
      "and he was persistent. Jack felt better knowled of how it had gkes, but Jack was there to it. He never forgot Jack and his jazz must abaity. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 22.29 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.0689\n",
      "Batch 202, Loss: 2.0041\n",
      "Batch 203, Loss: 2.0975\n",
      "Batch 204, Loss: 2.0189\n",
      "Batch 205, Loss: 2.1046\n",
      "Batch 206, Loss: 2.0942\n",
      "Batch 207, Loss: 2.0066\n",
      "Batch 208, Loss: 2.0678\n",
      "Batch 209, Loss: 2.0974\n",
      "Batch 210, Loss: 2.0130\n",
      "Batch 211, Loss: 2.0774\n",
      "Batch 212, Loss: 2.0869\n",
      "Batch 213, Loss: 2.0677\n",
      "Batch 214, Loss: 2.0826\n",
      "Batch 215, Loss: 1.9806\n",
      "Batch 216, Loss: 2.0403\n",
      "Batch 217, Loss: 2.0583\n",
      "Batch 218, Loss: 1.9553\n",
      "Batch 219, Loss: 2.0336\n",
      "Batch 220, Loss: 1.9875\n",
      "Batch 221, Loss: 2.1381\n",
      "Batch 222, Loss: 2.0682\n",
      "Batch 223, Loss: 2.0968\n",
      "Batch 224, Loss: 2.0785\n",
      "Batch 225, Loss: 2.0943\n",
      "Batch 226, Loss: 2.0216\n",
      "Batch 227, Loss: 2.1254\n",
      "Batch 228, Loss: 2.0900\n",
      "Batch 229, Loss: 2.0937\n",
      "Batch 230, Loss: 2.0826\n",
      "Batch 231, Loss: 1.9509\n",
      "Batch 232, Loss: 2.0308\n",
      "Batch 233, Loss: 2.0473\n",
      "Batch 234, Loss: 1.9868\n",
      "Batch 235, Loss: 2.0946\n",
      "Batch 236, Loss: 2.1163\n",
      "Batch 237, Loss: 1.9642\n",
      "Batch 238, Loss: 2.0824\n",
      "Batch 239, Loss: 1.9848\n",
      "Batch 240, Loss: 2.0858\n",
      "Batch 241, Loss: 2.1139\n",
      "Batch 242, Loss: 2.0656\n",
      "Batch 243, Loss: 2.0866\n",
      "Batch 244, Loss: 2.1046\n",
      "Batch 245, Loss: 2.0897\n",
      "Batch 246, Loss: 2.0369\n",
      "Batch 247, Loss: 2.0919\n",
      "Batch 248, Loss: 2.1115\n",
      "Batch 249, Loss: 2.0439\n",
      "Batch 250, Loss: 2.0653\n",
      "Batch 251, Loss: 2.0604\n",
      "Batch 252, Loss: 2.1200\n",
      "Batch 253, Loss: 2.0706\n",
      "Batch 254, Loss: 2.0769\n",
      "Batch 255, Loss: 2.1371\n",
      "Batch 256, Loss: 2.0892\n",
      "Batch 257, Loss: 2.0826\n",
      "Batch 258, Loss: 2.1104\n",
      "Batch 259, Loss: 2.1138\n",
      "Batch 260, Loss: 2.0212\n",
      "Batch 261, Loss: 2.1091\n",
      "Batch 262, Loss: 2.1141\n",
      "Batch 263, Loss: 2.1036\n",
      "Batch 264, Loss: 2.0570\n",
      "Batch 265, Loss: 2.0325\n",
      "Batch 266, Loss: 2.0898\n",
      "Batch 267, Loss: 2.0339\n",
      "Batch 268, Loss: 2.0806\n",
      "Batch 269, Loss: 2.0480\n",
      "Batch 270, Loss: 1.8296\n",
      "Batch 271, Loss: 2.0881\n",
      "Batch 272, Loss: 2.1350\n",
      "Batch 273, Loss: 2.1052\n",
      "Batch 274, Loss: 2.0742\n",
      "Batch 275, Loss: 2.0600\n",
      "Batch 276, Loss: 2.0860\n",
      "Batch 277, Loss: 2.0666\n",
      "Batch 278, Loss: 2.0452\n",
      "Batch 279, Loss: 2.1198\n",
      "Batch 280, Loss: 2.1066\n",
      "Batch 281, Loss: 2.0842\n",
      "Batch 282, Loss: 2.0509\n",
      "Batch 283, Loss: 2.0637\n",
      "Batch 284, Loss: 2.1052\n",
      "Batch 285, Loss: 2.0513\n",
      "Batch 286, Loss: 2.0339\n",
      "Batch 287, Loss: 2.0795\n",
      "Batch 288, Loss: 2.0465\n",
      "Batch 289, Loss: 2.0895\n",
      "Batch 290, Loss: 2.0225\n",
      "Batch 291, Loss: 2.0739\n",
      "Batch 292, Loss: 2.0686\n",
      "Batch 293, Loss: 2.1151\n",
      "Batch 294, Loss: 2.0816\n",
      "Batch 295, Loss: 2.1140\n",
      "Batch 296, Loss: 2.0997\n",
      "Batch 297, Loss: 2.0802\n",
      "Batch 298, Loss: 2.0616\n",
      "Batch 299, Loss: 2.0314\n",
      "Batch 300, Loss: 2.0402\n",
      "Batch 301, Loss: 2.0840\n",
      "Batch 302, Loss: 2.0848\n",
      "Batch 303, Loss: 2.0896\n",
      "Batch 304, Loss: 2.1148\n",
      "Batch 305, Loss: 2.0914\n",
      "Batch 306, Loss: 2.0872\n",
      "Batch 307, Loss: 2.0577\n",
      "Batch 308, Loss: 2.0843\n",
      "Batch 309, Loss: 2.0596\n",
      "Batch 310, Loss: 2.0860\n",
      "Batch 311, Loss: 2.0616\n",
      "Batch 312, Loss: 2.0646\n",
      "Batch 313, Loss: 2.0669\n",
      "Batch 314, Loss: 2.0523\n",
      "Batch 315, Loss: 2.0688\n",
      "Batch 316, Loss: 2.1054\n",
      "Batch 317, Loss: 2.0742\n",
      "Batch 318, Loss: 1.9751\n",
      "Batch 319, Loss: 1.9863\n",
      "Batch 320, Loss: 1.9708\n",
      "Batch 321, Loss: 1.9647\n",
      "Batch 322, Loss: 2.0964\n",
      "Batch 323, Loss: 2.0778\n",
      "Batch 324, Loss: 1.9742\n",
      "Batch 325, Loss: 1.9900\n",
      "Batch 326, Loss: 2.0710\n",
      "Batch 327, Loss: 1.9710\n",
      "Batch 328, Loss: 2.1004\n",
      "Batch 329, Loss: 2.0861\n",
      "Batch 330, Loss: 1.9669\n",
      "Batch 331, Loss: 2.0953\n",
      "Batch 332, Loss: 2.0864\n",
      "Batch 333, Loss: 2.0419\n",
      "Batch 334, Loss: 2.1041\n",
      "Batch 335, Loss: 1.9646\n",
      "Batch 336, Loss: 2.0084\n",
      "Batch 337, Loss: 2.0621\n",
      "Batch 338, Loss: 2.0748\n",
      "Batch 339, Loss: 2.0955\n",
      "Batch 340, Loss: 1.9803\n",
      "Batch 341, Loss: 2.0561\n",
      "Batch 342, Loss: 1.9983\n",
      "Batch 343, Loss: 2.0880\n",
      "Batch 344, Loss: 2.0304\n",
      "Batch 345, Loss: 2.0505\n",
      "Batch 346, Loss: 2.0549\n",
      "Batch 347, Loss: 2.0567\n",
      "Batch 348, Loss: 2.0581\n",
      "Batch 349, Loss: 2.0618\n",
      "Batch 350, Loss: 2.0294\n",
      "Batch 351, Loss: 2.0460\n",
      "Batch 352, Loss: 2.0420\n",
      "Batch 353, Loss: 2.0659\n",
      "Batch 354, Loss: 1.9442\n",
      "Batch 355, Loss: 2.0727\n",
      "Batch 356, Loss: 2.0640\n",
      "Batch 357, Loss: 2.0346\n",
      "Batch 358, Loss: 2.0711\n",
      "Batch 359, Loss: 1.9290\n",
      "Batch 360, Loss: 2.0509\n",
      "Batch 361, Loss: 2.0119\n",
      "Batch 362, Loss: 2.0291\n",
      "Batch 363, Loss: 2.0471\n",
      "Batch 364, Loss: 1.9489\n",
      "Batch 365, Loss: 2.0523\n",
      "Batch 366, Loss: 2.0210\n",
      "Batch 367, Loss: 1.9449\n",
      "Batch 368, Loss: 1.9891\n",
      "Batch 369, Loss: 2.0969\n",
      "Batch 370, Loss: 2.0639\n",
      "Batch 371, Loss: 2.0602\n",
      "Batch 372, Loss: 2.0698\n",
      "Batch 373, Loss: 2.0539\n",
      "Batch 374, Loss: 2.0944\n",
      "Batch 375, Loss: 2.0800\n",
      "Batch 376, Loss: 1.9497\n",
      "Batch 377, Loss: 2.0369\n",
      "Batch 378, Loss: 2.0296\n",
      "Epoch 8, Average Loss: 2.0767\n",
      "Once upon a time, there was a grumpy rabbit. Everyone was so high. They tried to jump in the meadow. The rabbit didn't know where the bottom on the floor \n",
      "the rabbit's hair he thought no. Oalk, he would always be able to rest. But when they saw the bottom of hiss. The rabbit knocked up the bottom with wraped \n",
      "the birds and shining him! The bear had big eye and so still lapped. She was so proud of her and for helping off the rabbit. Luckily, the bath was \n",
      "always fish, but decided to take Spaxed as she dream she could look proudly safe. They felt very happy when they had given him up for it all the rabbit! \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.0331\n",
      "Batch 2, Loss: 2.0419\n",
      "Batch 3, Loss: 2.0059\n",
      "Batch 4, Loss: 2.0642\n",
      "Batch 5, Loss: 2.0386\n",
      "Batch 6, Loss: 2.0113\n",
      "Batch 7, Loss: 2.0453\n",
      "Batch 8, Loss: 2.0090\n",
      "Batch 9, Loss: 2.0304\n",
      "Batch 10, Loss: 2.0250\n",
      "Batch 11, Loss: 2.0344\n",
      "Batch 12, Loss: 1.9251\n",
      "Batch 13, Loss: 1.9378\n",
      "Batch 14, Loss: 2.0635\n",
      "Batch 15, Loss: 1.9823\n",
      "Batch 16, Loss: 2.0622\n",
      "Batch 17, Loss: 1.9896\n",
      "Batch 18, Loss: 2.0689\n",
      "Batch 19, Loss: 2.0618\n",
      "Batch 20, Loss: 2.0434\n",
      "Batch 21, Loss: 1.9302\n",
      "Batch 22, Loss: 2.0540\n",
      "Batch 23, Loss: 2.0214\n",
      "Batch 24, Loss: 2.0418\n",
      "Batch 25, Loss: 1.9293\n",
      "Batch 26, Loss: 2.0611\n",
      "Batch 27, Loss: 1.9840\n",
      "Batch 28, Loss: 2.0348\n",
      "Batch 29, Loss: 2.0132\n",
      "Batch 30, Loss: 1.9453\n",
      "Batch 31, Loss: 1.9430\n",
      "Batch 32, Loss: 2.0005\n",
      "Batch 33, Loss: 2.0391\n",
      "Batch 34, Loss: 2.0959\n",
      "Batch 35, Loss: 2.0936\n",
      "Batch 36, Loss: 2.0078\n",
      "Batch 37, Loss: 2.0357\n",
      "Batch 38, Loss: 2.0124\n",
      "Batch 39, Loss: 2.0746\n",
      "Batch 40, Loss: 2.0750\n",
      "Batch 41, Loss: 2.1313\n",
      "Batch 42, Loss: 2.5037\n",
      "Batch 43, Loss: 4.9705\n",
      "Batch 44, Loss: 4.2957\n",
      "Batch 45, Loss: 4.0371\n",
      "Batch 46, Loss: 3.7222\n",
      "Batch 47, Loss: 3.5805\n",
      "Batch 48, Loss: 3.5746\n",
      "Batch 49, Loss: 3.3308\n",
      "Batch 50, Loss: 3.3707\n",
      "Batch 51, Loss: 3.2374\n",
      "Batch 52, Loss: 3.1658\n",
      "Batch 53, Loss: 3.0594\n",
      "Batch 54, Loss: 2.9816\n",
      "Batch 55, Loss: 2.9926\n",
      "Batch 56, Loss: 2.9281\n",
      "Batch 57, Loss: 2.8647\n",
      "Batch 58, Loss: 2.6722\n",
      "Batch 59, Loss: 2.7724\n",
      "Batch 60, Loss: 2.7099\n",
      "Batch 61, Loss: 2.6444\n",
      "Batch 62, Loss: 2.6552\n",
      "Batch 63, Loss: 2.6997\n",
      "Batch 64, Loss: 2.5976\n",
      "Batch 65, Loss: 2.5868\n",
      "Batch 66, Loss: 2.5226\n",
      "Batch 67, Loss: 2.5421\n",
      "Batch 68, Loss: 2.2255\n",
      "Batch 69, Loss: 2.4399\n",
      "Batch 70, Loss: 2.4427\n",
      "Batch 71, Loss: 2.2896\n",
      "Batch 72, Loss: 2.4306\n",
      "Batch 73, Loss: 2.4049\n",
      "Batch 74, Loss: 2.3228\n",
      "Batch 75, Loss: 2.2316\n",
      "Batch 76, Loss: 2.2906\n",
      "Batch 77, Loss: 2.3420\n",
      "Batch 78, Loss: 2.3090\n",
      "Batch 79, Loss: 2.2828\n",
      "Batch 80, Loss: 2.2396\n",
      "Batch 81, Loss: 2.2176\n",
      "Batch 82, Loss: 2.2157\n",
      "Batch 83, Loss: 2.2971\n",
      "Batch 84, Loss: 2.1298\n",
      "Batch 85, Loss: 2.1499\n",
      "Batch 86, Loss: 2.1092\n",
      "Batch 87, Loss: 2.2303\n",
      "Batch 88, Loss: 2.2404\n",
      "Batch 89, Loss: 2.2344\n",
      "Batch 90, Loss: 2.2110\n",
      "Batch 91, Loss: 2.1211\n",
      "Batch 92, Loss: 2.2190\n",
      "Batch 93, Loss: 2.1665\n",
      "Batch 94, Loss: 2.1640\n",
      "Batch 95, Loss: 2.0653\n",
      "Batch 96, Loss: 2.1418\n",
      "Batch 97, Loss: 2.1949\n",
      "Batch 98, Loss: 2.0807\n",
      "Batch 99, Loss: 2.1205\n",
      "Batch 100, Loss: 2.0943\n",
      "Batch 101, Loss: 2.1505\n",
      "Batch 102, Loss: 2.1037\n",
      "Batch 103, Loss: 2.1512\n",
      "Batch 104, Loss: 2.1288\n",
      "Batch 105, Loss: 2.1061\n",
      "Batch 106, Loss: 2.1398\n",
      "Batch 107, Loss: 2.1234\n",
      "Batch 108, Loss: 2.1101\n",
      "Batch 109, Loss: 2.1207\n",
      "Batch 110, Loss: 2.1187\n",
      "Batch 111, Loss: 2.1576\n",
      "Batch 112, Loss: 2.1343\n",
      "Batch 113, Loss: 2.0754\n",
      "Batch 114, Loss: 2.1336\n",
      "Batch 115, Loss: 2.0527\n",
      "Batch 116, Loss: 2.0218\n",
      "Batch 117, Loss: 2.1198\n",
      "Batch 118, Loss: 2.1461\n",
      "Batch 119, Loss: 2.0813\n",
      "Batch 120, Loss: 1.9886\n",
      "Batch 121, Loss: 2.0869\n",
      "Batch 122, Loss: 2.0869\n",
      "Batch 123, Loss: 2.1193\n",
      "Batch 124, Loss: 2.0020\n",
      "Batch 125, Loss: 2.1269\n",
      "Batch 126, Loss: 2.1074\n",
      "Batch 127, Loss: 2.0720\n",
      "Batch 128, Loss: 2.0830\n",
      "Batch 129, Loss: 2.1076\n",
      "Batch 130, Loss: 2.0962\n",
      "Batch 131, Loss: 2.0990\n",
      "Batch 132, Loss: 2.1196\n",
      "Batch 133, Loss: 2.0233\n",
      "Batch 134, Loss: 2.1102\n",
      "Batch 135, Loss: 2.0782\n",
      "Batch 136, Loss: 2.0745\n",
      "Batch 137, Loss: 2.0533\n",
      "Batch 138, Loss: 2.0712\n",
      "Batch 139, Loss: 2.0766\n",
      "Batch 140, Loss: 2.0165\n",
      "Batch 141, Loss: 2.0627\n",
      "Batch 142, Loss: 2.0327\n",
      "Batch 143, Loss: 2.0821\n",
      "Batch 144, Loss: 2.0646\n",
      "Batch 145, Loss: 1.9511\n",
      "Batch 146, Loss: 2.0655\n",
      "Batch 147, Loss: 2.1012\n",
      "Batch 148, Loss: 2.0728\n",
      "Batch 149, Loss: 2.0834\n",
      "Batch 150, Loss: 2.0174\n",
      "Batch 151, Loss: 1.9751\n",
      "Batch 152, Loss: 2.0246\n",
      "Batch 153, Loss: 2.0523\n",
      "Batch 154, Loss: 2.1062\n",
      "Batch 155, Loss: 1.9576\n",
      "Batch 156, Loss: 2.0747\n",
      "Batch 157, Loss: 2.0614\n",
      "Batch 158, Loss: 2.0748\n",
      "Batch 159, Loss: 2.0586\n",
      "Batch 160, Loss: 2.0715\n",
      "Batch 161, Loss: 2.0485\n",
      "Batch 162, Loss: 2.0954\n",
      "Batch 163, Loss: 2.0434\n",
      "Batch 164, Loss: 2.0838\n",
      "Batch 165, Loss: 2.0500\n",
      "Batch 166, Loss: 2.0284\n",
      "Batch 167, Loss: 2.0756\n",
      "Batch 168, Loss: 2.0276\n",
      "Batch 169, Loss: 2.0688\n",
      "Batch 170, Loss: 2.0731\n",
      "Batch 171, Loss: 2.0213\n",
      "Batch 172, Loss: 2.0226\n",
      "Batch 173, Loss: 2.0431\n",
      "Batch 174, Loss: 2.0602\n",
      "Batch 175, Loss: 2.0441\n",
      "Batch 176, Loss: 2.0471\n",
      "Batch 177, Loss: 1.9720\n",
      "Batch 178, Loss: 2.0494\n",
      "Batch 179, Loss: 2.0285\n",
      "Batch 180, Loss: 2.0721\n",
      "Batch 181, Loss: 2.0477\n",
      "Batch 182, Loss: 2.0314\n",
      "Batch 183, Loss: 2.0343\n",
      "Batch 184, Loss: 1.9879\n",
      "Batch 185, Loss: 1.9829\n",
      "Batch 186, Loss: 2.0407\n",
      "Batch 187, Loss: 2.0270\n",
      "Batch 188, Loss: 2.0508\n",
      "Batch 189, Loss: 2.0656\n",
      "Batch 190, Loss: 1.9741\n",
      "Batch 191, Loss: 2.0882\n",
      "Batch 192, Loss: 2.0333\n",
      "Batch 193, Loss: 2.0047\n",
      "Batch 194, Loss: 2.0143\n",
      "Batch 195, Loss: 1.9633\n",
      "Batch 196, Loss: 2.0509\n",
      "Batch 197, Loss: 2.0771\n",
      "Batch 198, Loss: 2.0520\n",
      "Batch 199, Loss: 2.0330\n",
      "Batch 200, Loss: 2.0579\n",
      "Once upon a time there was a little girl called Lize. She was very wide and always putting in her pocket all around the room furry things. Suddenly, a sack \n",
      "blew Frove to her. It was soft and copy and teddy bear very coign. Liz was so sad. A little boy and Lizz were dancing because they had to wait. \n",
      "They were all overced and got very careful to wait. The little girl started to caling with our little boy and Lizzy fell off in the cuad, but Lizzy was \n",
      "very sad. She asked him everyone about the sack. The inch quickly was so guilty and hungs, and the book would find the biggest thingâ€“ it was a cold idea \n",
      "- a special guid. She had an idea. She said that the boy had found a cold and act edgy of edgy, the edge. She saw what had found, and \n",
      "they had a great time playing with the cold. From that day on, and es were the best of friends. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 23.19 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.0529\n",
      "Batch 202, Loss: 2.0624\n",
      "Batch 203, Loss: 1.9858\n",
      "Batch 204, Loss: 2.0474\n",
      "Batch 205, Loss: 2.0478\n",
      "Batch 206, Loss: 1.9508\n",
      "Batch 207, Loss: 2.0485\n",
      "Batch 208, Loss: 2.0549\n",
      "Batch 209, Loss: 2.0195\n",
      "Batch 210, Loss: 2.0418\n",
      "Batch 211, Loss: 1.9385\n",
      "Batch 212, Loss: 2.0855\n",
      "Batch 213, Loss: 2.1013\n",
      "Batch 214, Loss: 2.0531\n",
      "Batch 215, Loss: 2.0136\n",
      "Batch 216, Loss: 2.0287\n",
      "Batch 217, Loss: 2.0670\n",
      "Batch 218, Loss: 2.0741\n",
      "Batch 219, Loss: 1.9725\n",
      "Batch 220, Loss: 2.0759\n",
      "Batch 221, Loss: 1.9919\n",
      "Batch 222, Loss: 2.0341\n",
      "Batch 223, Loss: 2.0449\n",
      "Batch 224, Loss: 2.0314\n",
      "Batch 225, Loss: 2.0147\n",
      "Batch 226, Loss: 2.0157\n",
      "Batch 227, Loss: 2.0136\n",
      "Batch 228, Loss: 1.9394\n",
      "Batch 229, Loss: 1.9678\n",
      "Batch 230, Loss: 1.9639\n",
      "Batch 231, Loss: 2.0431\n",
      "Batch 232, Loss: 2.0573\n",
      "Batch 233, Loss: 1.9164\n",
      "Batch 234, Loss: 2.0252\n",
      "Batch 235, Loss: 1.9918\n",
      "Batch 236, Loss: 2.0531\n",
      "Batch 237, Loss: 2.0372\n",
      "Batch 238, Loss: 2.0279\n",
      "Batch 239, Loss: 2.0294\n",
      "Batch 240, Loss: 2.0542\n",
      "Batch 241, Loss: 1.9977\n",
      "Batch 242, Loss: 2.0564\n",
      "Batch 243, Loss: 2.0267\n",
      "Batch 244, Loss: 2.0129\n",
      "Batch 245, Loss: 2.0294\n",
      "Batch 246, Loss: 2.0039\n",
      "Batch 247, Loss: 2.0284\n",
      "Batch 248, Loss: 1.9915\n",
      "Batch 249, Loss: 1.9643\n",
      "Batch 250, Loss: 2.0376\n",
      "Batch 251, Loss: 2.0226\n",
      "Batch 252, Loss: 2.0668\n",
      "Batch 253, Loss: 1.9691\n",
      "Batch 254, Loss: 2.0354\n",
      "Batch 255, Loss: 2.0334\n",
      "Batch 256, Loss: 2.0181\n",
      "Batch 257, Loss: 2.0188\n",
      "Batch 258, Loss: 1.9052\n",
      "Batch 259, Loss: 2.0082\n",
      "Batch 260, Loss: 2.0230\n",
      "Batch 261, Loss: 2.0153\n",
      "Batch 262, Loss: 2.0411\n",
      "Batch 263, Loss: 1.9619\n",
      "Batch 264, Loss: 2.0297\n",
      "Batch 265, Loss: 2.0392\n",
      "Batch 266, Loss: 1.9486\n",
      "Batch 267, Loss: 1.9099\n",
      "Batch 268, Loss: 1.9947\n",
      "Batch 269, Loss: 2.0295\n",
      "Batch 270, Loss: 1.9594\n",
      "Batch 271, Loss: 2.0029\n",
      "Batch 272, Loss: 2.0010\n",
      "Batch 273, Loss: 2.0245\n",
      "Batch 274, Loss: 1.9461\n",
      "Batch 275, Loss: 1.9061\n",
      "Batch 276, Loss: 2.0124\n",
      "Batch 277, Loss: 2.0027\n",
      "Batch 278, Loss: 2.0400\n",
      "Batch 279, Loss: 2.0206\n",
      "Batch 280, Loss: 1.9614\n",
      "Batch 281, Loss: 2.0075\n",
      "Batch 282, Loss: 2.0190\n",
      "Batch 283, Loss: 1.9800\n",
      "Batch 284, Loss: 2.0586\n",
      "Batch 285, Loss: 2.0249\n",
      "Batch 286, Loss: 1.9757\n",
      "Batch 287, Loss: 2.0629\n",
      "Batch 288, Loss: 2.0175\n",
      "Batch 289, Loss: 2.0179\n",
      "Batch 290, Loss: 2.0095\n",
      "Batch 291, Loss: 2.0088\n",
      "Batch 292, Loss: 1.9234\n",
      "Batch 293, Loss: 1.9688\n",
      "Batch 294, Loss: 2.0200\n",
      "Batch 295, Loss: 2.0000\n",
      "Batch 296, Loss: 2.0297\n",
      "Batch 297, Loss: 2.0475\n",
      "Batch 298, Loss: 1.9191\n",
      "Batch 299, Loss: 2.0195\n",
      "Batch 300, Loss: 2.0071\n",
      "Batch 301, Loss: 2.0386\n",
      "Batch 302, Loss: 1.9462\n",
      "Batch 303, Loss: 2.0369\n",
      "Batch 304, Loss: 2.0220\n",
      "Batch 305, Loss: 2.0344\n",
      "Batch 306, Loss: 1.9713\n",
      "Batch 307, Loss: 1.9148\n",
      "Batch 308, Loss: 2.0129\n",
      "Batch 309, Loss: 2.0035\n",
      "Batch 310, Loss: 1.8862\n",
      "Batch 311, Loss: 1.9624\n",
      "Batch 312, Loss: 2.0786\n",
      "Batch 313, Loss: 2.0074\n",
      "Batch 314, Loss: 2.0061\n",
      "Batch 315, Loss: 2.0140\n",
      "Batch 316, Loss: 2.0250\n",
      "Batch 317, Loss: 2.0359\n",
      "Batch 318, Loss: 1.9992\n",
      "Batch 319, Loss: 2.0057\n",
      "Batch 320, Loss: 1.9385\n",
      "Batch 321, Loss: 2.0331\n",
      "Batch 322, Loss: 2.0280\n",
      "Batch 323, Loss: 2.0267\n",
      "Batch 324, Loss: 1.9788\n",
      "Batch 325, Loss: 1.9416\n",
      "Batch 326, Loss: 1.9152\n",
      "Batch 327, Loss: 2.0387\n",
      "Batch 328, Loss: 2.0386\n",
      "Batch 329, Loss: 1.9986\n",
      "Batch 330, Loss: 2.0192\n",
      "Batch 331, Loss: 2.0322\n",
      "Batch 332, Loss: 2.0304\n",
      "Batch 333, Loss: 1.9812\n",
      "Batch 334, Loss: 2.0417\n",
      "Batch 335, Loss: 1.9785\n",
      "Batch 336, Loss: 2.0217\n",
      "Batch 337, Loss: 1.9669\n",
      "Batch 338, Loss: 1.9857\n",
      "Batch 339, Loss: 2.0082\n",
      "Batch 340, Loss: 2.0163\n",
      "Batch 341, Loss: 2.0300\n",
      "Batch 342, Loss: 2.0095\n",
      "Batch 343, Loss: 2.0107\n",
      "Batch 344, Loss: 2.0188\n",
      "Batch 345, Loss: 1.9976\n",
      "Batch 346, Loss: 2.0351\n",
      "Batch 347, Loss: 1.9900\n",
      "Batch 348, Loss: 2.0085\n",
      "Batch 349, Loss: 1.8928\n",
      "Batch 350, Loss: 1.9714\n",
      "Batch 351, Loss: 2.0381\n",
      "Batch 352, Loss: 2.0354\n",
      "Batch 353, Loss: 2.0144\n",
      "Batch 354, Loss: 1.9203\n",
      "Batch 355, Loss: 2.0001\n",
      "Batch 356, Loss: 1.8757\n",
      "Batch 357, Loss: 1.9309\n",
      "Batch 358, Loss: 2.0135\n",
      "Batch 359, Loss: 2.0010\n",
      "Batch 360, Loss: 1.9334\n",
      "Batch 361, Loss: 2.0131\n",
      "Batch 362, Loss: 2.0435\n",
      "Batch 363, Loss: 2.0206\n",
      "Batch 364, Loss: 2.0328\n",
      "Batch 365, Loss: 1.9691\n",
      "Batch 366, Loss: 1.9488\n",
      "Batch 367, Loss: 2.0048\n",
      "Batch 368, Loss: 1.9594\n",
      "Batch 369, Loss: 2.0207\n",
      "Batch 370, Loss: 1.9964\n",
      "Batch 371, Loss: 1.9722\n",
      "Batch 372, Loss: 2.0028\n",
      "Batch 373, Loss: 2.0149\n",
      "Batch 374, Loss: 1.9774\n",
      "Batch 375, Loss: 2.0075\n",
      "Batch 376, Loss: 1.9960\n",
      "Batch 377, Loss: 2.0155\n",
      "Batch 378, Loss: 2.0034\n",
      "Epoch 9, Average Loss: 2.1189\n",
      "Molly was feeling 3 years old. She did not like the way home, she wanted to sufferge her friend, but one day she didn't have any fun. Molly saw her \n",
      "many different moving in the middle and heard the lif. All of a broken surprise - a secret door from this butterfly was not very hidden! So, she decided to \n",
      "punish it, but this time she met an amazing well. She punished her wings fright back and out forth. The two friends thanked Molly off that moment when the butterfly \n",
      "stroking her, but eventually she remembered it. She took a big bite of the wind and punished for a few friend. The secret drawer was hiding. The other farmer stayed \n",
      "quiet and scrunched over the little punch and be enough. Molly was so sad but *pecially he didn't know what to do. She then went to eat her wings and \n",
      "said \"It was really angry\". Then, but she punished forgetting how long they could do any Jack to know with it more kiss. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.9508\n",
      "Batch 2, Loss: 2.0052\n",
      "Batch 3, Loss: 1.9230\n",
      "Batch 4, Loss: 2.0221\n",
      "Batch 5, Loss: 1.9655\n",
      "Batch 6, Loss: 1.9399\n",
      "Batch 7, Loss: 1.9835\n",
      "Batch 8, Loss: 2.0265\n",
      "Batch 9, Loss: 1.9753\n",
      "Batch 10, Loss: 1.8924\n",
      "Batch 11, Loss: 2.0087\n",
      "Batch 12, Loss: 1.9561\n",
      "Batch 13, Loss: 2.0073\n",
      "Batch 14, Loss: 1.9071\n",
      "Batch 15, Loss: 1.9580\n",
      "Batch 16, Loss: 1.9666\n",
      "Batch 17, Loss: 1.9810\n",
      "Batch 18, Loss: 1.9831\n",
      "Batch 19, Loss: 1.9476\n",
      "Batch 20, Loss: 1.8883\n",
      "Batch 21, Loss: 1.9752\n",
      "Batch 22, Loss: 1.9678\n",
      "Batch 23, Loss: 1.9619\n",
      "Batch 24, Loss: 1.9469\n",
      "Batch 25, Loss: 1.9981\n",
      "Batch 26, Loss: 1.8832\n",
      "Batch 27, Loss: 1.9774\n",
      "Batch 28, Loss: 2.0048\n",
      "Batch 29, Loss: 2.0064\n",
      "Batch 30, Loss: 1.9465\n",
      "Batch 31, Loss: 1.9625\n",
      "Batch 32, Loss: 1.9920\n",
      "Batch 33, Loss: 1.9991\n",
      "Batch 34, Loss: 1.9678\n",
      "Batch 35, Loss: 1.9896\n",
      "Batch 36, Loss: 2.0408\n",
      "Batch 37, Loss: 1.9650\n",
      "Batch 38, Loss: 1.9546\n",
      "Batch 39, Loss: 1.9944\n",
      "Batch 40, Loss: 2.0322\n",
      "Batch 41, Loss: 1.9843\n",
      "Batch 42, Loss: 1.9334\n",
      "Batch 43, Loss: 1.9516\n",
      "Batch 44, Loss: 2.0182\n",
      "Batch 45, Loss: 2.0146\n",
      "Batch 46, Loss: 1.9575\n",
      "Batch 47, Loss: 1.9702\n",
      "Batch 48, Loss: 1.8623\n",
      "Batch 49, Loss: 1.9852\n",
      "Batch 50, Loss: 1.9025\n",
      "Batch 51, Loss: 2.0157\n",
      "Batch 52, Loss: 2.0106\n",
      "Batch 53, Loss: 1.9136\n",
      "Batch 54, Loss: 1.9338\n",
      "Batch 55, Loss: 2.0181\n",
      "Batch 56, Loss: 1.8773\n",
      "Batch 57, Loss: 1.9915\n",
      "Batch 58, Loss: 1.9525\n",
      "Batch 59, Loss: 1.9737\n",
      "Batch 60, Loss: 2.0119\n",
      "Batch 61, Loss: 2.0078\n",
      "Batch 62, Loss: 1.9719\n",
      "Batch 63, Loss: 1.9213\n",
      "Batch 64, Loss: 1.9169\n",
      "Batch 65, Loss: 1.9912\n",
      "Batch 66, Loss: 1.9798\n",
      "Batch 67, Loss: 2.0247\n",
      "Batch 68, Loss: 2.0214\n",
      "Batch 69, Loss: 1.9970\n",
      "Batch 70, Loss: 2.0058\n",
      "Batch 71, Loss: 1.9071\n",
      "Batch 72, Loss: 1.9616\n",
      "Batch 73, Loss: 1.9672\n",
      "Batch 74, Loss: 1.9452\n",
      "Batch 75, Loss: 1.9599\n",
      "Batch 76, Loss: 1.9616\n",
      "Batch 77, Loss: 1.9749\n",
      "Batch 78, Loss: 1.9885\n",
      "Batch 79, Loss: 1.9703\n",
      "Batch 80, Loss: 1.9974\n",
      "Batch 81, Loss: 1.9716\n",
      "Batch 82, Loss: 1.9619\n",
      "Batch 83, Loss: 1.8799\n",
      "Batch 84, Loss: 1.9773\n",
      "Batch 85, Loss: 2.0108\n",
      "Batch 86, Loss: 1.9297\n",
      "Batch 87, Loss: 2.0008\n",
      "Batch 88, Loss: 1.9730\n",
      "Batch 89, Loss: 1.9853\n",
      "Batch 90, Loss: 1.9805\n",
      "Batch 91, Loss: 2.0153\n",
      "Batch 92, Loss: 1.9938\n",
      "Batch 93, Loss: 2.0117\n",
      "Batch 94, Loss: 1.9471\n",
      "Batch 95, Loss: 1.8789\n",
      "Batch 96, Loss: 1.9945\n",
      "Batch 97, Loss: 1.8959\n",
      "Batch 98, Loss: 1.9087\n",
      "Batch 99, Loss: 2.0072\n",
      "Batch 100, Loss: 2.0031\n",
      "Batch 101, Loss: 1.9416\n",
      "Batch 102, Loss: 2.0082\n",
      "Batch 103, Loss: 1.8611\n",
      "Batch 104, Loss: 1.8734\n",
      "Batch 105, Loss: 2.0018\n",
      "Batch 106, Loss: 2.0002\n",
      "Batch 107, Loss: 2.0175\n",
      "Batch 108, Loss: 1.9071\n",
      "Batch 109, Loss: 1.8812\n",
      "Batch 110, Loss: 1.9947\n",
      "Batch 111, Loss: 1.9983\n",
      "Batch 112, Loss: 2.0106\n",
      "Batch 113, Loss: 1.9699\n",
      "Batch 114, Loss: 1.9042\n",
      "Batch 115, Loss: 2.0036\n",
      "Batch 116, Loss: 1.9072\n",
      "Batch 117, Loss: 1.9791\n",
      "Batch 118, Loss: 1.9535\n",
      "Batch 119, Loss: 1.9389\n",
      "Batch 120, Loss: 2.0454\n",
      "Batch 121, Loss: 1.9802\n",
      "Batch 122, Loss: 1.9991\n",
      "Batch 123, Loss: 1.9755\n",
      "Batch 124, Loss: 1.9896\n",
      "Batch 125, Loss: 1.9219\n",
      "Batch 126, Loss: 1.9776\n",
      "Batch 127, Loss: 1.9788\n",
      "Batch 128, Loss: 1.9937\n",
      "Batch 129, Loss: 1.9026\n",
      "Batch 130, Loss: 1.9667\n",
      "Batch 131, Loss: 1.8999\n",
      "Batch 132, Loss: 1.9516\n",
      "Batch 133, Loss: 1.9487\n",
      "Batch 134, Loss: 1.8769\n",
      "Batch 135, Loss: 1.9584\n",
      "Batch 136, Loss: 1.9896\n",
      "Batch 137, Loss: 1.9989\n",
      "Batch 138, Loss: 1.9905\n",
      "Batch 139, Loss: 1.8549\n",
      "Batch 140, Loss: 1.8912\n",
      "Batch 141, Loss: 1.9872\n",
      "Batch 142, Loss: 1.9916\n",
      "Batch 143, Loss: 1.9595\n",
      "Batch 144, Loss: 1.9725\n",
      "Batch 145, Loss: 2.0004\n",
      "Batch 146, Loss: 1.9999\n",
      "Batch 147, Loss: 1.9612\n",
      "Batch 148, Loss: 1.9004\n",
      "Batch 149, Loss: 1.9788\n",
      "Batch 150, Loss: 2.0055\n",
      "Batch 151, Loss: 1.9849\n",
      "Batch 152, Loss: 1.9547\n",
      "Batch 153, Loss: 1.9650\n",
      "Batch 154, Loss: 1.9744\n",
      "Batch 155, Loss: 1.9707\n",
      "Batch 156, Loss: 1.9141\n",
      "Batch 157, Loss: 2.0141\n",
      "Batch 158, Loss: 1.9931\n",
      "Batch 159, Loss: 1.9762\n",
      "Batch 160, Loss: 1.9693\n",
      "Batch 161, Loss: 1.9788\n",
      "Batch 162, Loss: 1.9626\n",
      "Batch 163, Loss: 1.9990\n",
      "Batch 164, Loss: 1.9507\n",
      "Batch 165, Loss: 1.9422\n",
      "Batch 166, Loss: 1.9228\n",
      "Batch 167, Loss: 1.9466\n",
      "Batch 168, Loss: 1.9988\n",
      "Batch 169, Loss: 1.9534\n",
      "Batch 170, Loss: 1.9629\n",
      "Batch 171, Loss: 1.9587\n",
      "Batch 172, Loss: 1.8809\n",
      "Batch 173, Loss: 2.0095\n",
      "Batch 174, Loss: 1.8525\n",
      "Batch 175, Loss: 1.9741\n",
      "Batch 176, Loss: 1.9087\n",
      "Batch 177, Loss: 1.9973\n",
      "Batch 178, Loss: 1.9748\n",
      "Batch 179, Loss: 1.8524\n",
      "Batch 180, Loss: 1.9433\n",
      "Batch 181, Loss: 1.9705\n",
      "Batch 182, Loss: 1.8976\n",
      "Batch 183, Loss: 1.9797\n",
      "Batch 184, Loss: 1.9468\n",
      "Batch 185, Loss: 1.9842\n",
      "Batch 186, Loss: 1.9925\n",
      "Batch 187, Loss: 1.9477\n",
      "Batch 188, Loss: 2.0072\n",
      "Batch 189, Loss: 1.9773\n",
      "Batch 190, Loss: 1.9066\n",
      "Batch 191, Loss: 1.9657\n",
      "Batch 192, Loss: 1.9970\n",
      "Batch 193, Loss: 1.9446\n",
      "Batch 194, Loss: 1.9530\n",
      "Batch 195, Loss: 1.9798\n",
      "Batch 196, Loss: 1.9721\n",
      "Batch 197, Loss: 1.8687\n",
      "Batch 198, Loss: 1.9594\n",
      "Batch 199, Loss: 1.9306\n",
      "Batch 200, Loss: 1.9196\n",
      "Ben and Lily are curious to the lake with Mom. They like to play with their toys. But today, they are not fragil. It is too big for Mom and \n",
      "Dad and Ben crying. One day, their mom told him they had to clean it and the lake. She gave them her and said, \"Nay keep us to clean it \n",
      "up!\" Lily said, \"I know! You're sorry to clean it. Your toy with supply will put it in.\" But the lum did not listen. She did not like crayons and \n",
      "cool. She thought it was funny. She wanted to clean it. Ben liked crying and looked for the lake and forgots. He threw her toys and went to look at \n",
      "all their clean. Soon, Ben's and eyes were friendly. They turned mom and dad at the lake and circle them. They liked to watch the lake and draw. They put \n",
      "the line and their Mom put it on the lake and put it in a raft in it. They made some kids and shapes. Mom and Dad said, \"Wow, Mom, \n",
      "my brand new!\" They laughed and share the lake. They had a lot of fun. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 20.98 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.8420\n",
      "Batch 202, Loss: 1.9964\n",
      "Batch 203, Loss: 1.9558\n",
      "Batch 204, Loss: 1.8526\n",
      "Batch 205, Loss: 1.9400\n",
      "Batch 206, Loss: 1.9675\n",
      "Batch 207, Loss: 1.9637\n",
      "Batch 208, Loss: 1.9583\n",
      "Batch 209, Loss: 1.9577\n",
      "Batch 210, Loss: 1.9925\n",
      "Batch 211, Loss: 1.9666\n",
      "Batch 212, Loss: 1.9552\n",
      "Batch 213, Loss: 1.9504\n",
      "Batch 214, Loss: 1.9403\n",
      "Batch 215, Loss: 1.9085\n",
      "Batch 216, Loss: 1.9651\n",
      "Batch 217, Loss: 1.9991\n",
      "Batch 218, Loss: 1.9153\n",
      "Batch 219, Loss: 1.9597\n",
      "Batch 220, Loss: 1.9246\n",
      "Batch 221, Loss: 1.9891\n",
      "Batch 222, Loss: 1.9756\n",
      "Batch 223, Loss: 1.8915\n",
      "Batch 224, Loss: 1.9792\n",
      "Batch 225, Loss: 1.9390\n",
      "Batch 226, Loss: 1.9674\n",
      "Batch 227, Loss: 1.9839\n",
      "Batch 228, Loss: 1.9956\n",
      "Batch 229, Loss: 1.9431\n",
      "Batch 230, Loss: 1.9868\n",
      "Batch 231, Loss: 1.9517\n",
      "Batch 232, Loss: 1.9589\n",
      "Batch 233, Loss: 1.9758\n",
      "Batch 234, Loss: 1.9904\n",
      "Batch 235, Loss: 1.9050\n",
      "Batch 236, Loss: 2.0050\n",
      "Batch 237, Loss: 1.9714\n",
      "Batch 238, Loss: 1.9321\n",
      "Batch 239, Loss: 1.9855\n",
      "Batch 240, Loss: 1.9173\n",
      "Batch 241, Loss: 1.9660\n",
      "Batch 242, Loss: 1.9359\n",
      "Batch 243, Loss: 1.8859\n",
      "Batch 244, Loss: 1.9578\n",
      "Batch 245, Loss: 1.9622\n",
      "Batch 246, Loss: 1.9264\n",
      "Batch 247, Loss: 1.9894\n",
      "Batch 248, Loss: 1.9238\n",
      "Batch 249, Loss: 1.9671\n",
      "Batch 250, Loss: 1.9511\n",
      "Batch 251, Loss: 1.9374\n",
      "Batch 252, Loss: 1.8407\n",
      "Batch 253, Loss: 1.8819\n",
      "Batch 254, Loss: 2.0026\n",
      "Batch 255, Loss: 1.9632\n",
      "Batch 256, Loss: 1.8678\n",
      "Batch 257, Loss: 1.9382\n",
      "Batch 258, Loss: 1.9272\n",
      "Batch 259, Loss: 1.9824\n",
      "Batch 260, Loss: 1.7010\n",
      "Batch 261, Loss: 1.9074\n",
      "Batch 262, Loss: 1.9372\n",
      "Batch 263, Loss: 1.9666\n",
      "Batch 264, Loss: 1.9763\n",
      "Batch 265, Loss: 1.9708\n",
      "Batch 266, Loss: 1.9786\n",
      "Batch 267, Loss: 1.9723\n",
      "Batch 268, Loss: 1.9091\n",
      "Batch 269, Loss: 1.9894\n",
      "Batch 270, Loss: 1.8781\n",
      "Batch 271, Loss: 1.9779\n",
      "Batch 272, Loss: 2.0215\n",
      "Batch 273, Loss: 1.9757\n",
      "Batch 274, Loss: 1.9764\n",
      "Batch 275, Loss: 1.9580\n",
      "Batch 276, Loss: 1.9676\n",
      "Batch 277, Loss: 1.8707\n",
      "Batch 278, Loss: 1.9476\n",
      "Batch 279, Loss: 1.9503\n",
      "Batch 280, Loss: 1.9428\n",
      "Batch 281, Loss: 1.9431\n",
      "Batch 282, Loss: 1.8999\n",
      "Batch 283, Loss: 1.9682\n",
      "Batch 284, Loss: 1.8714\n",
      "Batch 285, Loss: 1.9839\n",
      "Batch 286, Loss: 1.9909\n",
      "Batch 287, Loss: 1.9170\n",
      "Batch 288, Loss: 1.9887\n",
      "Batch 289, Loss: 1.9725\n",
      "Batch 290, Loss: 1.9590\n",
      "Batch 291, Loss: 1.9487\n",
      "Batch 292, Loss: 1.9262\n",
      "Batch 293, Loss: 1.9645\n",
      "Batch 294, Loss: 1.9618\n",
      "Batch 295, Loss: 1.9411\n",
      "Batch 296, Loss: 1.9359\n",
      "Batch 297, Loss: 1.9592\n",
      "Batch 298, Loss: 1.9646\n",
      "Batch 299, Loss: 1.9967\n",
      "Batch 300, Loss: 1.9594\n",
      "Batch 301, Loss: 1.9393\n",
      "Batch 302, Loss: 1.9815\n",
      "Batch 303, Loss: 1.9435\n",
      "Batch 304, Loss: 1.9555\n",
      "Batch 305, Loss: 1.9484\n",
      "Batch 306, Loss: 1.9418\n",
      "Batch 307, Loss: 1.8591\n",
      "Batch 308, Loss: 1.9613\n",
      "Batch 309, Loss: 1.9546\n",
      "Batch 310, Loss: 1.9551\n",
      "Batch 311, Loss: 1.8501\n",
      "Batch 312, Loss: 1.9396\n",
      "Batch 313, Loss: 1.8581\n",
      "Batch 314, Loss: 1.9487\n",
      "Batch 315, Loss: 1.9934\n",
      "Batch 316, Loss: 1.9228\n",
      "Batch 317, Loss: 1.8558\n",
      "Batch 318, Loss: 1.9618\n",
      "Batch 319, Loss: 1.9522\n",
      "Batch 320, Loss: 1.9528\n",
      "Batch 321, Loss: 1.9357\n",
      "Batch 322, Loss: 1.9678\n",
      "Batch 323, Loss: 1.9786\n",
      "Batch 324, Loss: 2.0141\n",
      "Batch 325, Loss: 1.9868\n",
      "Batch 326, Loss: 1.9570\n",
      "Batch 327, Loss: 1.9769\n",
      "Batch 328, Loss: 1.9934\n",
      "Batch 329, Loss: 1.9977\n",
      "Batch 330, Loss: 1.9784\n",
      "Batch 331, Loss: 1.9546\n",
      "Batch 332, Loss: 1.9676\n",
      "Batch 333, Loss: 1.9647\n",
      "Batch 334, Loss: 1.8306\n",
      "Batch 335, Loss: 1.8614\n",
      "Batch 336, Loss: 1.9032\n",
      "Batch 337, Loss: 1.9691\n",
      "Batch 338, Loss: 1.9022\n",
      "Batch 339, Loss: 1.9516\n",
      "Batch 340, Loss: 1.9714\n",
      "Batch 341, Loss: 1.9364\n",
      "Batch 342, Loss: 1.9881\n",
      "Batch 343, Loss: 1.9449\n",
      "Batch 344, Loss: 1.8688\n",
      "Batch 345, Loss: 1.9814\n",
      "Batch 346, Loss: 1.9392\n",
      "Batch 347, Loss: 1.9482\n",
      "Batch 348, Loss: 1.9951\n",
      "Batch 349, Loss: 1.9644\n",
      "Batch 350, Loss: 1.9942\n",
      "Batch 351, Loss: 1.9381\n",
      "Batch 352, Loss: 1.9513\n",
      "Batch 353, Loss: 1.9201\n",
      "Batch 354, Loss: 1.9291\n",
      "Batch 355, Loss: 1.8354\n",
      "Batch 356, Loss: 1.8459\n",
      "Batch 357, Loss: 1.9922\n",
      "Batch 358, Loss: 1.9594\n",
      "Batch 359, Loss: 1.9527\n",
      "Batch 360, Loss: 1.9350\n",
      "Batch 361, Loss: 1.9700\n",
      "Batch 362, Loss: 1.8992\n",
      "Batch 363, Loss: 1.9530\n",
      "Batch 364, Loss: 1.9196\n",
      "Batch 365, Loss: 1.9605\n",
      "Batch 366, Loss: 1.9675\n",
      "Batch 367, Loss: 1.9595\n",
      "Batch 368, Loss: 1.9607\n",
      "Batch 369, Loss: 1.9638\n",
      "Batch 370, Loss: 1.9804\n",
      "Batch 371, Loss: 1.9842\n",
      "Batch 372, Loss: 1.9557\n",
      "Batch 373, Loss: 1.8655\n",
      "Batch 374, Loss: 1.9538\n",
      "Batch 375, Loss: 1.9687\n",
      "Batch 376, Loss: 1.9016\n",
      "Batch 377, Loss: 1.9187\n",
      "Batch 378, Loss: 1.9722\n",
      "Epoch 10, Average Loss: 1.9554\n",
      "The sun was shining with eager today, where he went to the park, today of his grace. He was enjoying a white winal tree that they had to pay at \n",
      "each other, and wanted to go through adventures. One day, when they got there, the sun came out to the park, the wind turning a big tree branch but he \n",
      "just stumbled across a black yell. He said, â€œLetâ€™s across it,â€. The wind bused happily, so Tom cat, \"Oh no!â€ The wind thought it was fun to jump in and \n",
      "try to cool down and go to the tree. The black grass saw End and said, â€œI'm going to be his friend. I'm sorry here to stay in the park \n",
      "first.â€ The two eyes were so pleased by the wind. Now Stour, the raindrops coming to find friends for himself. He petted and jumped forward to the lake of his \n",
      "ball. The kitten stayed until the sun began to shine in the wind. After playing and the sun started to break again. The towers were wet and world the bus \n",
      "together, not to grab the back of the night hity. The moral of the story is to waste that make them feel happy. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.9131\n",
      "Batch 2, Loss: 1.9288\n",
      "Batch 3, Loss: 1.8375\n",
      "Batch 4, Loss: 1.9213\n",
      "Batch 5, Loss: 1.9728\n",
      "Batch 6, Loss: 1.9072\n",
      "Batch 7, Loss: 1.9555\n",
      "Batch 8, Loss: 1.8879\n",
      "Batch 9, Loss: 1.8992\n",
      "Batch 10, Loss: 1.8569\n",
      "Batch 11, Loss: 1.9038\n",
      "Batch 12, Loss: 1.9185\n",
      "Batch 13, Loss: 1.9598\n",
      "Batch 14, Loss: 1.9232\n",
      "Batch 15, Loss: 1.8427\n",
      "Batch 16, Loss: 1.9089\n",
      "Batch 17, Loss: 1.9549\n",
      "Batch 18, Loss: 1.9279\n",
      "Batch 19, Loss: 1.8611\n",
      "Batch 20, Loss: 1.9267\n",
      "Batch 21, Loss: 1.9015\n",
      "Batch 22, Loss: 1.8665\n",
      "Batch 23, Loss: 1.9275\n",
      "Batch 24, Loss: 1.9438\n",
      "Batch 25, Loss: 1.9323\n",
      "Batch 26, Loss: 1.8453\n",
      "Batch 27, Loss: 1.9604\n",
      "Batch 28, Loss: 1.9719\n",
      "Batch 29, Loss: 1.8523\n",
      "Batch 30, Loss: 1.9307\n",
      "Batch 31, Loss: 1.9064\n",
      "Batch 32, Loss: 1.8506\n",
      "Batch 33, Loss: 1.9051\n",
      "Batch 34, Loss: 1.9129\n",
      "Batch 35, Loss: 1.9448\n",
      "Batch 36, Loss: 1.9679\n",
      "Batch 37, Loss: 1.9215\n",
      "Batch 38, Loss: 1.8834\n",
      "Batch 39, Loss: 1.9567\n",
      "Batch 40, Loss: 1.8284\n",
      "Batch 41, Loss: 1.9226\n",
      "Batch 42, Loss: 1.9472\n",
      "Batch 43, Loss: 1.8213\n",
      "Batch 44, Loss: 1.8509\n",
      "Batch 45, Loss: 1.9712\n",
      "Batch 46, Loss: 1.9573\n",
      "Batch 47, Loss: 1.8560\n",
      "Batch 48, Loss: 1.9594\n",
      "Batch 49, Loss: 1.9447\n",
      "Batch 50, Loss: 1.9134\n",
      "Batch 51, Loss: 1.9354\n",
      "Batch 52, Loss: 1.8866\n",
      "Batch 53, Loss: 1.9129\n",
      "Batch 54, Loss: 1.9125\n",
      "Batch 55, Loss: 1.9387\n",
      "Batch 56, Loss: 1.9205\n",
      "Batch 57, Loss: 1.9692\n",
      "Batch 58, Loss: 1.9125\n",
      "Batch 59, Loss: 1.9290\n",
      "Batch 60, Loss: 1.9117\n",
      "Batch 61, Loss: 1.9631\n",
      "Batch 62, Loss: 1.9257\n",
      "Batch 63, Loss: 1.8269\n",
      "Batch 64, Loss: 1.9184\n",
      "Batch 65, Loss: 1.7781\n",
      "Batch 66, Loss: 1.9290\n",
      "Batch 67, Loss: 1.8581\n",
      "Batch 68, Loss: 1.9207\n",
      "Batch 69, Loss: 1.9121\n",
      "Batch 70, Loss: 1.9197\n",
      "Batch 71, Loss: 1.9591\n",
      "Batch 72, Loss: 1.9008\n",
      "Batch 73, Loss: 1.9593\n",
      "Batch 74, Loss: 1.8564\n",
      "Batch 75, Loss: 1.9059\n",
      "Batch 76, Loss: 1.8905\n",
      "Batch 77, Loss: 1.9213\n",
      "Batch 78, Loss: 1.8538\n",
      "Batch 79, Loss: 1.9492\n",
      "Batch 80, Loss: 1.9237\n",
      "Batch 81, Loss: 1.9600\n",
      "Batch 82, Loss: 1.8134\n",
      "Batch 83, Loss: 1.9096\n",
      "Batch 84, Loss: 1.9159\n",
      "Batch 85, Loss: 1.9393\n",
      "Batch 86, Loss: 1.9286\n",
      "Batch 87, Loss: 1.9249\n",
      "Batch 88, Loss: 1.8201\n",
      "Batch 89, Loss: 1.9308\n",
      "Batch 90, Loss: 1.9273\n",
      "Batch 91, Loss: 1.9318\n",
      "Batch 92, Loss: 1.8377\n",
      "Batch 93, Loss: 1.9084\n",
      "Batch 94, Loss: 1.9223\n",
      "Batch 95, Loss: 1.9411\n",
      "Batch 96, Loss: 1.9346\n",
      "Batch 97, Loss: 1.8181\n",
      "Batch 98, Loss: 1.8978\n",
      "Batch 99, Loss: 1.9118\n",
      "Batch 100, Loss: 1.9113\n",
      "Batch 101, Loss: 1.9234\n",
      "Batch 102, Loss: 1.8103\n",
      "Batch 103, Loss: 1.9089\n",
      "Batch 104, Loss: 1.9180\n",
      "Batch 105, Loss: 1.9171\n",
      "Batch 106, Loss: 1.9236\n",
      "Batch 107, Loss: 1.9249\n",
      "Batch 108, Loss: 1.7995\n",
      "Batch 109, Loss: 1.9093\n",
      "Batch 110, Loss: 1.9728\n",
      "Batch 111, Loss: 1.9287\n",
      "Batch 112, Loss: 1.9270\n",
      "Batch 113, Loss: 1.9460\n",
      "Batch 114, Loss: 1.9508\n",
      "Batch 115, Loss: 1.7962\n",
      "Batch 116, Loss: 1.9483\n",
      "Batch 117, Loss: 1.9290\n",
      "Batch 118, Loss: 1.9266\n",
      "Batch 119, Loss: 1.9505\n",
      "Batch 120, Loss: 1.9601\n",
      "Batch 121, Loss: 1.9655\n",
      "Batch 122, Loss: 1.9127\n",
      "Batch 123, Loss: 1.9534\n",
      "Batch 124, Loss: 1.9663\n",
      "Batch 125, Loss: 1.9516\n",
      "Batch 126, Loss: 1.8924\n",
      "Batch 127, Loss: 1.9547\n",
      "Batch 128, Loss: 1.9188\n",
      "Batch 129, Loss: 1.8608\n",
      "Batch 130, Loss: 1.9605\n",
      "Batch 131, Loss: 1.8794\n",
      "Batch 132, Loss: 1.9611\n",
      "Batch 133, Loss: 1.8283\n",
      "Batch 134, Loss: 1.9296\n",
      "Batch 135, Loss: 1.9149\n",
      "Batch 136, Loss: 1.9593\n",
      "Batch 137, Loss: 1.9486\n",
      "Batch 138, Loss: 1.9276\n",
      "Batch 139, Loss: 1.9175\n",
      "Batch 140, Loss: 1.8774\n",
      "Batch 141, Loss: 1.9206\n",
      "Batch 142, Loss: 1.8869\n",
      "Batch 143, Loss: 1.9178\n",
      "Batch 144, Loss: 1.9401\n",
      "Batch 145, Loss: 1.8642\n",
      "Batch 146, Loss: 1.8921\n",
      "Batch 147, Loss: 1.9608\n",
      "Batch 148, Loss: 1.9254\n",
      "Batch 149, Loss: 1.9636\n",
      "Batch 150, Loss: 1.9313\n",
      "Batch 151, Loss: 1.9115\n",
      "Batch 152, Loss: 1.9472\n",
      "Batch 153, Loss: 1.9125\n",
      "Batch 154, Loss: 1.8878\n",
      "Batch 155, Loss: 1.8942\n",
      "Batch 156, Loss: 1.9013\n",
      "Batch 157, Loss: 1.9620\n",
      "Batch 158, Loss: 1.9425\n",
      "Batch 159, Loss: 1.9287\n",
      "Batch 160, Loss: 1.8798\n",
      "Batch 161, Loss: 1.8152\n",
      "Batch 162, Loss: 1.9606\n",
      "Batch 163, Loss: 1.8846\n",
      "Batch 164, Loss: 1.9248\n",
      "Batch 165, Loss: 1.9395\n",
      "Batch 166, Loss: 1.9413\n",
      "Batch 167, Loss: 1.9150\n",
      "Batch 168, Loss: 1.9090\n",
      "Batch 169, Loss: 1.8198\n",
      "Batch 170, Loss: 1.8806\n",
      "Batch 171, Loss: 1.9274\n",
      "Batch 172, Loss: 1.9045\n",
      "Batch 173, Loss: 1.9193\n",
      "Batch 174, Loss: 1.8608\n",
      "Batch 175, Loss: 1.9048\n",
      "Batch 176, Loss: 1.9219\n",
      "Batch 177, Loss: 1.9118\n",
      "Batch 178, Loss: 1.8694\n",
      "Batch 179, Loss: 1.8151\n",
      "Batch 180, Loss: 1.9255\n",
      "Batch 181, Loss: 1.9351\n",
      "Batch 182, Loss: 1.9186\n",
      "Batch 183, Loss: 1.9416\n",
      "Batch 184, Loss: 1.9470\n",
      "Batch 185, Loss: 1.8885\n",
      "Batch 186, Loss: 1.8477\n",
      "Batch 187, Loss: 1.9261\n",
      "Batch 188, Loss: 1.9457\n",
      "Batch 189, Loss: 1.8190\n",
      "Batch 190, Loss: 1.8094\n",
      "Batch 191, Loss: 1.8904\n",
      "Batch 192, Loss: 1.8013\n",
      "Batch 193, Loss: 1.9391\n",
      "Batch 194, Loss: 1.8695\n",
      "Batch 195, Loss: 1.9169\n",
      "Batch 196, Loss: 1.9332\n",
      "Batch 197, Loss: 1.9480\n",
      "Batch 198, Loss: 1.9405\n",
      "Batch 199, Loss: 1.8565\n",
      "Batch 200, Loss: 1.9358\n",
      "John was a dependable boy who liked to play outside. One day, he started to feel dizzy, but his mommy had a too car. John was so excited and started \n",
      "doing what the only exam. His mom took him, then noticed the automobile paws playing in the garden. He stopped and okayediented them from the game, holding her lips, and \n",
      "obedive. Finally, all of their buckles, John became curious about his friend what the fastesten do kind. He thought about this, where he might care for find, it takes a \n",
      "stick. John was disappointed, but he knew he would be helpful. Then he got a little chilipping posking the paws from home. He had a great time and soft friends \n",
      "sang them around the house. He promised to never take turning passing louder too. It was a very soft and warned up to explore the great with his desmend. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 24.31 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.8489\n",
      "Batch 202, Loss: 1.9192\n",
      "Batch 203, Loss: 1.9354\n",
      "Batch 204, Loss: 1.9125\n",
      "Batch 205, Loss: 1.9269\n",
      "Batch 206, Loss: 1.8571\n",
      "Batch 207, Loss: 1.9182\n",
      "Batch 208, Loss: 1.9156\n",
      "Batch 209, Loss: 1.9305\n",
      "Batch 210, Loss: 1.9428\n",
      "Batch 211, Loss: 1.9076\n",
      "Batch 212, Loss: 1.9384\n",
      "Batch 213, Loss: 1.9017\n",
      "Batch 214, Loss: 1.9296\n",
      "Batch 215, Loss: 1.8916\n",
      "Batch 216, Loss: 1.9301\n",
      "Batch 217, Loss: 1.8313\n",
      "Batch 218, Loss: 1.9055\n",
      "Batch 219, Loss: 1.8722\n",
      "Batch 220, Loss: 1.8928\n",
      "Batch 221, Loss: 1.9641\n",
      "Batch 222, Loss: 1.8862\n",
      "Batch 223, Loss: 1.9073\n",
      "Batch 224, Loss: 1.9173\n",
      "Batch 225, Loss: 1.9249\n",
      "Batch 226, Loss: 1.8388\n",
      "Batch 227, Loss: 1.9244\n",
      "Batch 228, Loss: 1.9186\n",
      "Batch 229, Loss: 1.9436\n",
      "Batch 230, Loss: 1.9161\n",
      "Batch 231, Loss: 1.8961\n",
      "Batch 232, Loss: 1.9066\n",
      "Batch 233, Loss: 1.9126\n",
      "Batch 234, Loss: 1.9165\n",
      "Batch 235, Loss: 1.8825\n",
      "Batch 236, Loss: 1.8875\n",
      "Batch 237, Loss: 1.9582\n",
      "Batch 238, Loss: 1.9253\n",
      "Batch 239, Loss: 1.8418\n",
      "Batch 240, Loss: 1.8129\n",
      "Batch 241, Loss: 1.8589\n",
      "Batch 242, Loss: 1.9588\n",
      "Batch 243, Loss: 1.9209\n",
      "Batch 244, Loss: 1.8184\n",
      "Batch 245, Loss: 1.9716\n",
      "Batch 246, Loss: 1.9539\n",
      "Batch 247, Loss: 1.9553\n",
      "Batch 248, Loss: 1.8981\n",
      "Batch 249, Loss: 1.8982\n",
      "Batch 250, Loss: 1.9144\n",
      "Batch 251, Loss: 1.8426\n",
      "Batch 252, Loss: 1.9186\n",
      "Batch 253, Loss: 1.9217\n",
      "Batch 254, Loss: 1.9189\n",
      "Batch 255, Loss: 1.8939\n",
      "Batch 256, Loss: 1.9479\n",
      "Batch 257, Loss: 1.9181\n",
      "Batch 258, Loss: 1.9005\n",
      "Batch 259, Loss: 1.9303\n",
      "Batch 260, Loss: 1.8692\n",
      "Batch 261, Loss: 1.9163\n",
      "Batch 262, Loss: 1.9105\n",
      "Batch 263, Loss: 1.9259\n",
      "Batch 264, Loss: 1.9028\n",
      "Batch 265, Loss: 1.8586\n",
      "Batch 266, Loss: 1.9058\n",
      "Batch 267, Loss: 1.8881\n",
      "Batch 268, Loss: 1.9425\n",
      "Batch 269, Loss: 1.8738\n",
      "Batch 270, Loss: 1.9343\n",
      "Batch 271, Loss: 1.8932\n",
      "Batch 272, Loss: 1.9196\n",
      "Batch 273, Loss: 1.9241\n",
      "Batch 274, Loss: 1.8819\n",
      "Batch 275, Loss: 1.9243\n",
      "Batch 276, Loss: 1.8638\n",
      "Batch 277, Loss: 1.8773\n",
      "Batch 278, Loss: 1.8518\n",
      "Batch 279, Loss: 1.9209\n",
      "Batch 280, Loss: 1.9425\n",
      "Batch 281, Loss: 1.8242\n",
      "Batch 282, Loss: 1.9159\n",
      "Batch 283, Loss: 1.9404\n",
      "Batch 284, Loss: 1.9229\n",
      "Batch 285, Loss: 1.8879\n",
      "Batch 286, Loss: 1.8900\n",
      "Batch 287, Loss: 1.8516\n",
      "Batch 288, Loss: 1.8715\n",
      "Batch 289, Loss: 1.9465\n",
      "Batch 290, Loss: 1.8952\n",
      "Batch 291, Loss: 1.9330\n",
      "Batch 292, Loss: 1.9115\n",
      "Batch 293, Loss: 1.9329\n",
      "Batch 294, Loss: 1.9557\n",
      "Batch 295, Loss: 1.9543\n",
      "Batch 296, Loss: 1.8602\n",
      "Batch 297, Loss: 1.9753\n",
      "Batch 298, Loss: 1.8801\n",
      "Batch 299, Loss: 1.9555\n",
      "Batch 300, Loss: 1.9478\n",
      "Batch 301, Loss: 1.8859\n",
      "Batch 302, Loss: 1.8769\n",
      "Batch 303, Loss: 1.9004\n",
      "Batch 304, Loss: 1.9204\n",
      "Batch 305, Loss: 1.9126\n",
      "Batch 306, Loss: 1.9236\n",
      "Batch 307, Loss: 1.9007\n",
      "Batch 308, Loss: 1.8697\n",
      "Batch 309, Loss: 1.8779\n",
      "Batch 310, Loss: 1.9294\n",
      "Batch 311, Loss: 1.9054\n",
      "Batch 312, Loss: 1.8973\n",
      "Batch 313, Loss: 1.8928\n",
      "Batch 314, Loss: 1.9285\n",
      "Batch 315, Loss: 1.9279\n",
      "Batch 316, Loss: 1.8794\n",
      "Batch 317, Loss: 1.9102\n",
      "Batch 318, Loss: 1.8676\n",
      "Batch 319, Loss: 1.9253\n",
      "Batch 320, Loss: 1.8224\n",
      "Batch 321, Loss: 1.8123\n",
      "Batch 322, Loss: 1.9299\n",
      "Batch 323, Loss: 1.9016\n",
      "Batch 324, Loss: 1.8133\n",
      "Batch 325, Loss: 1.9758\n",
      "Batch 326, Loss: 1.8712\n",
      "Batch 327, Loss: 1.8568\n",
      "Batch 328, Loss: 1.8911\n",
      "Batch 329, Loss: 1.9362\n",
      "Batch 330, Loss: 1.9015\n",
      "Batch 331, Loss: 1.9335\n",
      "Batch 332, Loss: 1.8132\n",
      "Batch 333, Loss: 1.9563\n",
      "Batch 334, Loss: 1.8443\n",
      "Batch 335, Loss: 1.8731\n",
      "Batch 336, Loss: 1.9257\n",
      "Batch 337, Loss: 1.8144\n",
      "Batch 338, Loss: 1.9558\n",
      "Batch 339, Loss: 1.8644\n",
      "Batch 340, Loss: 1.8753\n",
      "Batch 341, Loss: 1.8550\n",
      "Batch 342, Loss: 1.8892\n",
      "Batch 343, Loss: 1.9634\n",
      "Batch 344, Loss: 1.9445\n",
      "Batch 345, Loss: 1.9324\n",
      "Batch 346, Loss: 1.9420\n",
      "Batch 347, Loss: 1.8524\n",
      "Batch 348, Loss: 1.9400\n",
      "Batch 349, Loss: 1.9370\n",
      "Batch 350, Loss: 1.9559\n",
      "Batch 351, Loss: 1.9588\n",
      "Batch 352, Loss: 1.9316\n",
      "Batch 353, Loss: 1.9380\n",
      "Batch 354, Loss: 1.9505\n",
      "Batch 355, Loss: 1.6560\n",
      "Batch 356, Loss: 1.9289\n",
      "Batch 357, Loss: 1.8230\n",
      "Batch 358, Loss: 1.8496\n",
      "Batch 359, Loss: 1.9388\n",
      "Batch 360, Loss: 1.9610\n",
      "Batch 361, Loss: 1.9622\n",
      "Batch 362, Loss: 1.9594\n",
      "Batch 363, Loss: 1.9127\n",
      "Batch 364, Loss: 1.9459\n",
      "Batch 365, Loss: 1.9171\n",
      "Batch 366, Loss: 1.8965\n",
      "Batch 367, Loss: 1.8841\n",
      "Batch 368, Loss: 1.8843\n",
      "Batch 369, Loss: 1.9280\n",
      "Batch 370, Loss: 1.8411\n",
      "Batch 371, Loss: 1.9373\n",
      "Batch 372, Loss: 1.8522\n",
      "Batch 373, Loss: 1.8919\n",
      "Batch 374, Loss: 1.9524\n",
      "Batch 375, Loss: 1.8066\n",
      "Batch 376, Loss: 1.8840\n",
      "Batch 377, Loss: 1.9070\n",
      "Batch 378, Loss: 1.9169\n",
      "Epoch 11, Average Loss: 1.9071\n",
      "Once upon a time, there was a little girl named Lily who went on a trip. She liked to explore and find new things. One day, she went on a \n",
      "hill and saw a chair and moved over to him. She smiled and ran to her mommy and said, \"Mommy, I found a beautiful red car that in the car\". \n",
      "Her mommy said, \"Okay, that's right. Will you stand with my car?\" Lily said, \"No, you can't stand on touch one. Let's have some stir or there,\" and dug around \n",
      "the room. They started to play with Lily's car. When they were ready, Lily said, \"Come to keep everything confider.\" Sadly, it was time to go on, Lily went back \n",
      "inside. She looked up and saw a shelf. It looked candles on the pande. As they walked, Lily said, \"Wooit, daddy. The subway is not safe, you can eat anything \n",
      "you want.\" Lily's mommy said, \"Sat is charming and you do something use. Let's attach it together.\" Lily hugged her mommy and said, \"Okay, mommy and then2.\" \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.8559\n",
      "Batch 2, Loss: 1.9003\n",
      "Batch 3, Loss: 1.9346\n",
      "Batch 4, Loss: 1.8351\n",
      "Batch 5, Loss: 1.9354\n",
      "Batch 6, Loss: 1.9148\n",
      "Batch 7, Loss: 1.8749\n",
      "Batch 8, Loss: 1.8214\n",
      "Batch 9, Loss: 1.8947\n",
      "Batch 10, Loss: 1.8601\n",
      "Batch 11, Loss: 1.8866\n",
      "Batch 12, Loss: 1.8375\n",
      "Batch 13, Loss: 1.8693\n",
      "Batch 14, Loss: 1.8821\n",
      "Batch 15, Loss: 1.7820\n",
      "Batch 16, Loss: 1.8956\n",
      "Batch 17, Loss: 1.8999\n",
      "Batch 18, Loss: 1.9098\n",
      "Batch 19, Loss: 1.9025\n",
      "Batch 20, Loss: 1.9142\n",
      "Batch 21, Loss: 1.8951\n",
      "Batch 22, Loss: 1.8622\n",
      "Batch 23, Loss: 1.8824\n",
      "Batch 24, Loss: 1.8789\n",
      "Batch 25, Loss: 1.8870\n",
      "Batch 26, Loss: 1.8740\n",
      "Batch 27, Loss: 1.8237\n",
      "Batch 28, Loss: 1.9368\n",
      "Batch 29, Loss: 1.8800\n",
      "Batch 30, Loss: 1.8765\n",
      "Batch 31, Loss: 1.7946\n",
      "Batch 32, Loss: 1.8784\n",
      "Batch 33, Loss: 1.8874\n",
      "Batch 34, Loss: 1.8502\n",
      "Batch 35, Loss: 1.8270\n",
      "Batch 36, Loss: 1.8542\n",
      "Batch 37, Loss: 1.8048\n",
      "Batch 38, Loss: 1.8599\n",
      "Batch 39, Loss: 1.7652\n",
      "Batch 40, Loss: 1.8482\n",
      "Batch 41, Loss: 1.8472\n",
      "Batch 42, Loss: 1.8811\n",
      "Batch 43, Loss: 1.8702\n",
      "Batch 44, Loss: 1.8835\n",
      "Batch 45, Loss: 1.8496\n",
      "Batch 46, Loss: 1.9026\n",
      "Batch 47, Loss: 1.8795\n",
      "Batch 48, Loss: 1.8936\n",
      "Batch 49, Loss: 1.8548\n",
      "Batch 50, Loss: 1.8880\n",
      "Batch 51, Loss: 1.8346\n",
      "Batch 52, Loss: 1.9129\n",
      "Batch 53, Loss: 1.8957\n",
      "Batch 54, Loss: 1.9140\n",
      "Batch 55, Loss: 1.8665\n",
      "Batch 56, Loss: 1.8131\n",
      "Batch 57, Loss: 1.8649\n",
      "Batch 58, Loss: 1.8115\n",
      "Batch 59, Loss: 1.9058\n",
      "Batch 60, Loss: 1.9316\n",
      "Batch 61, Loss: 1.8955\n",
      "Batch 62, Loss: 1.8748\n",
      "Batch 63, Loss: 1.8688\n",
      "Batch 64, Loss: 1.8694\n",
      "Batch 65, Loss: 1.8455\n",
      "Batch 66, Loss: 1.8944\n",
      "Batch 67, Loss: 1.9070\n",
      "Batch 68, Loss: 1.8481\n",
      "Batch 69, Loss: 1.9075\n",
      "Batch 70, Loss: 1.8608\n",
      "Batch 71, Loss: 1.7589\n",
      "Batch 72, Loss: 1.8569\n",
      "Batch 73, Loss: 1.7642\n",
      "Batch 74, Loss: 1.8315\n",
      "Batch 75, Loss: 1.9128\n",
      "Batch 76, Loss: 1.8538\n",
      "Batch 77, Loss: 1.9038\n",
      "Batch 78, Loss: 1.9052\n",
      "Batch 79, Loss: 1.9191\n",
      "Batch 80, Loss: 1.9001\n",
      "Batch 81, Loss: 1.9377\n",
      "Batch 82, Loss: 1.9226\n",
      "Batch 83, Loss: 1.8714\n",
      "Batch 84, Loss: 1.7823\n",
      "Batch 85, Loss: 1.9093\n",
      "Batch 86, Loss: 1.8946\n",
      "Batch 87, Loss: 1.8022\n",
      "Batch 88, Loss: 1.7904\n",
      "Batch 89, Loss: 1.8184\n",
      "Batch 90, Loss: 1.8250\n",
      "Batch 91, Loss: 1.8332\n",
      "Batch 92, Loss: 1.7699\n",
      "Batch 93, Loss: 1.9009\n",
      "Batch 94, Loss: 1.8542\n",
      "Batch 95, Loss: 1.8498\n",
      "Batch 96, Loss: 1.8901\n",
      "Batch 97, Loss: 1.9073\n",
      "Batch 98, Loss: 1.9039\n",
      "Batch 99, Loss: 1.8945\n",
      "Batch 100, Loss: 1.8698\n",
      "Batch 101, Loss: 1.8252\n",
      "Batch 102, Loss: 1.8874\n",
      "Batch 103, Loss: 1.8850\n",
      "Batch 104, Loss: 1.8328\n",
      "Batch 105, Loss: 1.8497\n",
      "Batch 106, Loss: 1.8719\n",
      "Batch 107, Loss: 1.8972\n",
      "Batch 108, Loss: 1.9130\n",
      "Batch 109, Loss: 1.9113\n",
      "Batch 110, Loss: 1.8852\n",
      "Batch 111, Loss: 1.8660\n",
      "Batch 112, Loss: 1.9034\n",
      "Batch 113, Loss: 1.8660\n",
      "Batch 114, Loss: 1.8670\n",
      "Batch 115, Loss: 1.9106\n",
      "Batch 116, Loss: 1.7593\n",
      "Batch 117, Loss: 1.8431\n",
      "Batch 118, Loss: 1.8758\n",
      "Batch 119, Loss: 1.8783\n",
      "Batch 120, Loss: 1.8419\n",
      "Batch 121, Loss: 1.8535\n",
      "Batch 122, Loss: 1.7802\n",
      "Batch 123, Loss: 1.8705\n",
      "Batch 124, Loss: 1.8425\n",
      "Batch 125, Loss: 1.8735\n",
      "Batch 126, Loss: 1.9100\n",
      "Batch 127, Loss: 1.8916\n",
      "Batch 128, Loss: 1.7897\n",
      "Batch 129, Loss: 1.9063\n",
      "Batch 130, Loss: 1.8822\n",
      "Batch 131, Loss: 1.8642\n",
      "Batch 132, Loss: 1.8656\n",
      "Batch 133, Loss: 1.8506\n",
      "Batch 134, Loss: 1.7865\n",
      "Batch 135, Loss: 1.8264\n",
      "Batch 136, Loss: 1.8842\n",
      "Batch 137, Loss: 1.8719\n",
      "Batch 138, Loss: 1.8816\n",
      "Batch 139, Loss: 1.8425\n",
      "Batch 140, Loss: 1.8771\n",
      "Batch 141, Loss: 1.8953\n",
      "Batch 142, Loss: 1.9024\n",
      "Batch 143, Loss: 1.8635\n",
      "Batch 144, Loss: 1.8505\n",
      "Batch 145, Loss: 1.9118\n",
      "Batch 146, Loss: 1.8857\n",
      "Batch 147, Loss: 1.8564\n",
      "Batch 148, Loss: 1.8756\n",
      "Batch 149, Loss: 1.9219\n",
      "Batch 150, Loss: 1.8620\n",
      "Batch 151, Loss: 1.8232\n",
      "Batch 152, Loss: 1.9152\n",
      "Batch 153, Loss: 1.8788\n",
      "Batch 154, Loss: 1.9291\n",
      "Batch 155, Loss: 1.7736\n",
      "Batch 156, Loss: 1.8803\n",
      "Batch 157, Loss: 1.9249\n",
      "Batch 158, Loss: 1.8852\n",
      "Batch 159, Loss: 1.8162\n",
      "Batch 160, Loss: 1.7774\n",
      "Batch 161, Loss: 1.8153\n",
      "Batch 162, Loss: 1.8675\n",
      "Batch 163, Loss: 1.8817\n",
      "Batch 164, Loss: 1.8874\n",
      "Batch 165, Loss: 1.9005\n",
      "Batch 166, Loss: 1.8883\n",
      "Batch 167, Loss: 1.8701\n",
      "Batch 168, Loss: 1.8708\n",
      "Batch 169, Loss: 1.9161\n",
      "Batch 170, Loss: 1.8913\n",
      "Batch 171, Loss: 1.8816\n",
      "Batch 172, Loss: 1.8533\n",
      "Batch 173, Loss: 1.7835\n",
      "Batch 174, Loss: 1.8623\n",
      "Batch 175, Loss: 1.8891\n",
      "Batch 176, Loss: 1.8840\n",
      "Batch 177, Loss: 1.8912\n",
      "Batch 178, Loss: 1.8425\n",
      "Batch 179, Loss: 1.8382\n",
      "Batch 180, Loss: 1.9106\n",
      "Batch 181, Loss: 1.8939\n",
      "Batch 182, Loss: 1.8743\n",
      "Batch 183, Loss: 1.8481\n",
      "Batch 184, Loss: 1.9047\n",
      "Batch 185, Loss: 1.8468\n",
      "Batch 186, Loss: 1.8473\n",
      "Batch 187, Loss: 1.7907\n",
      "Batch 188, Loss: 1.9138\n",
      "Batch 189, Loss: 1.8542\n",
      "Batch 190, Loss: 1.9166\n",
      "Batch 191, Loss: 1.8172\n",
      "Batch 192, Loss: 1.8931\n",
      "Batch 193, Loss: 1.8942\n",
      "Batch 194, Loss: 1.9147\n",
      "Batch 195, Loss: 1.9052\n",
      "Batch 196, Loss: 1.8383\n",
      "Batch 197, Loss: 1.8900\n",
      "Batch 198, Loss: 1.9101\n",
      "Batch 199, Loss: 1.8403\n",
      "Batch 200, Loss: 1.8766\n",
      "Once upon a time, there was a little girl named Lily. She loved to draw with her mug. They would draw the drawerar her favorite spoon. One day, Lily's mom \n",
      "gave her a big homeholar. Lily wanted to read the mug at a big backyard. She tried and tried, but the waiter did not want to stop playing. Lily's mom \n",
      "said, \"Lily, you need to help, Lily! Your cool is ashamed.\" Lily tried to turn but couldn't see her. Her mom came and said, \"It's okay things that happen for \n",
      "things you got too smaller.\" Lily ate her mom stoped that she learned that it's important to listen to her mom and stay there. From that day on, Lily always \n",
      "value her mug playing games and always proud of her mug to usage even if something is important. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 23.45 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.8886\n",
      "Batch 202, Loss: 1.8195\n",
      "Batch 203, Loss: 1.8838\n",
      "Batch 204, Loss: 1.8924\n",
      "Batch 205, Loss: 1.8825\n",
      "Batch 206, Loss: 1.9020\n",
      "Batch 207, Loss: 1.8132\n",
      "Batch 208, Loss: 1.8910\n",
      "Batch 209, Loss: 1.9094\n",
      "Batch 210, Loss: 1.8799\n",
      "Batch 211, Loss: 1.8674\n",
      "Batch 212, Loss: 1.9081\n",
      "Batch 213, Loss: 1.9235\n",
      "Batch 214, Loss: 1.8007\n",
      "Batch 215, Loss: 1.7767\n",
      "Batch 216, Loss: 1.8643\n",
      "Batch 217, Loss: 1.9386\n",
      "Batch 218, Loss: 1.8881\n",
      "Batch 219, Loss: 1.8899\n",
      "Batch 220, Loss: 1.8841\n",
      "Batch 221, Loss: 1.8826\n",
      "Batch 222, Loss: 1.9003\n",
      "Batch 223, Loss: 1.8916\n",
      "Batch 224, Loss: 1.8431\n",
      "Batch 225, Loss: 1.8437\n",
      "Batch 226, Loss: 1.8781\n",
      "Batch 227, Loss: 1.7887\n",
      "Batch 228, Loss: 1.9028\n",
      "Batch 229, Loss: 1.8602\n",
      "Batch 230, Loss: 1.8552\n",
      "Batch 231, Loss: 1.8850\n",
      "Batch 232, Loss: 1.8195\n",
      "Batch 233, Loss: 1.7876\n",
      "Batch 234, Loss: 1.8989\n",
      "Batch 235, Loss: 1.8167\n",
      "Batch 236, Loss: 1.8053\n",
      "Batch 237, Loss: 1.9147\n",
      "Batch 238, Loss: 1.8874\n",
      "Batch 239, Loss: 1.7922\n",
      "Batch 240, Loss: 1.9000\n",
      "Batch 241, Loss: 1.8872\n",
      "Batch 242, Loss: 1.8960\n",
      "Batch 243, Loss: 1.8828\n",
      "Batch 244, Loss: 1.7876\n",
      "Batch 245, Loss: 1.8176\n",
      "Batch 246, Loss: 1.9011\n",
      "Batch 247, Loss: 1.8194\n",
      "Batch 248, Loss: 1.8679\n",
      "Batch 249, Loss: 1.8840\n",
      "Batch 250, Loss: 1.9013\n",
      "Batch 251, Loss: 1.7712\n",
      "Batch 252, Loss: 1.8755\n",
      "Batch 253, Loss: 1.8776\n",
      "Batch 254, Loss: 1.8706\n",
      "Batch 255, Loss: 1.8904\n",
      "Batch 256, Loss: 1.9019\n",
      "Batch 257, Loss: 1.9083\n",
      "Batch 258, Loss: 1.8046\n",
      "Batch 259, Loss: 1.8591\n",
      "Batch 260, Loss: 1.8692\n",
      "Batch 261, Loss: 1.8762\n",
      "Batch 262, Loss: 1.9156\n",
      "Batch 263, Loss: 1.8562\n",
      "Batch 264, Loss: 1.9102\n",
      "Batch 265, Loss: 1.8002\n",
      "Batch 266, Loss: 1.7860\n",
      "Batch 267, Loss: 1.8339\n",
      "Batch 268, Loss: 1.9208\n",
      "Batch 269, Loss: 1.8707\n",
      "Batch 270, Loss: 1.8793\n",
      "Batch 271, Loss: 1.9008\n",
      "Batch 272, Loss: 1.8774\n",
      "Batch 273, Loss: 1.9141\n",
      "Batch 274, Loss: 1.7554\n",
      "Batch 275, Loss: 1.9093\n",
      "Batch 276, Loss: 1.9142\n",
      "Batch 277, Loss: 1.9114\n",
      "Batch 278, Loss: 1.7792\n",
      "Batch 279, Loss: 1.8460\n",
      "Batch 280, Loss: 1.9190\n",
      "Batch 281, Loss: 1.8398\n",
      "Batch 282, Loss: 1.8547\n",
      "Batch 283, Loss: 1.8928\n",
      "Batch 284, Loss: 1.7685\n",
      "Batch 285, Loss: 1.8886\n",
      "Batch 286, Loss: 1.8972\n",
      "Batch 287, Loss: 1.8728\n",
      "Batch 288, Loss: 1.8886\n",
      "Batch 289, Loss: 1.7583\n",
      "Batch 290, Loss: 1.8191\n",
      "Batch 291, Loss: 1.8676\n",
      "Batch 292, Loss: 1.8900\n",
      "Batch 293, Loss: 1.8667\n",
      "Batch 294, Loss: 1.9041\n",
      "Batch 295, Loss: 1.8067\n",
      "Batch 296, Loss: 1.8941\n",
      "Batch 297, Loss: 1.8300\n",
      "Batch 298, Loss: 1.8690\n",
      "Batch 299, Loss: 1.8966\n",
      "Batch 300, Loss: 1.8987\n",
      "Batch 301, Loss: 1.8589\n",
      "Batch 302, Loss: 1.8651\n",
      "Batch 303, Loss: 1.8849\n",
      "Batch 304, Loss: 1.8576\n",
      "Batch 305, Loss: 1.8660\n",
      "Batch 306, Loss: 1.5535\n",
      "Batch 307, Loss: 1.8745\n",
      "Batch 308, Loss: 1.8301\n",
      "Batch 309, Loss: 1.8186\n",
      "Batch 310, Loss: 1.9294\n",
      "Batch 311, Loss: 1.9451\n",
      "Batch 312, Loss: 1.8247\n",
      "Batch 313, Loss: 1.8719\n",
      "Batch 314, Loss: 1.8941\n",
      "Batch 315, Loss: 1.7899\n",
      "Batch 316, Loss: 1.7827\n",
      "Batch 317, Loss: 1.7804\n",
      "Batch 318, Loss: 1.8464\n",
      "Batch 319, Loss: 1.8219\n",
      "Batch 320, Loss: 1.8915\n",
      "Batch 321, Loss: 1.8533\n",
      "Batch 322, Loss: 1.8612\n",
      "Batch 323, Loss: 1.9052\n",
      "Batch 324, Loss: 1.8929\n",
      "Batch 325, Loss: 1.7949\n",
      "Batch 326, Loss: 1.8959\n",
      "Batch 327, Loss: 1.8867\n",
      "Batch 328, Loss: 1.8819\n",
      "Batch 329, Loss: 1.8655\n",
      "Batch 330, Loss: 1.8925\n",
      "Batch 331, Loss: 1.8565\n",
      "Batch 332, Loss: 1.8560\n",
      "Batch 333, Loss: 1.8890\n",
      "Batch 334, Loss: 1.8673\n",
      "Batch 335, Loss: 1.8793\n",
      "Batch 336, Loss: 1.9044\n",
      "Batch 337, Loss: 1.8750\n",
      "Batch 338, Loss: 1.8988\n",
      "Batch 339, Loss: 1.8936\n",
      "Batch 340, Loss: 1.7617\n",
      "Batch 341, Loss: 1.8767\n",
      "Batch 342, Loss: 1.8685\n",
      "Batch 343, Loss: 1.8744\n",
      "Batch 344, Loss: 1.8057\n",
      "Batch 345, Loss: 1.7674\n",
      "Batch 346, Loss: 1.8576\n",
      "Batch 347, Loss: 1.8709\n",
      "Batch 348, Loss: 1.8417\n",
      "Batch 349, Loss: 1.7945\n",
      "Batch 350, Loss: 1.8234\n",
      "Batch 351, Loss: 1.8947\n",
      "Batch 352, Loss: 1.8017\n",
      "Batch 353, Loss: 1.8781\n",
      "Batch 354, Loss: 1.8890\n",
      "Batch 355, Loss: 1.7592\n",
      "Batch 356, Loss: 1.8702\n",
      "Batch 357, Loss: 1.8872\n",
      "Batch 358, Loss: 1.8750\n",
      "Batch 359, Loss: 1.8757\n",
      "Batch 360, Loss: 1.7973\n",
      "Batch 361, Loss: 1.8091\n",
      "Batch 362, Loss: 1.8435\n",
      "Batch 363, Loss: 1.8525\n",
      "Batch 364, Loss: 1.8704\n",
      "Batch 365, Loss: 1.8411\n",
      "Batch 366, Loss: 1.8890\n",
      "Batch 367, Loss: 1.8942\n",
      "Batch 368, Loss: 1.8242\n",
      "Batch 369, Loss: 1.8964\n",
      "Batch 370, Loss: 1.8424\n",
      "Batch 371, Loss: 1.9206\n",
      "Batch 372, Loss: 1.8067\n",
      "Batch 373, Loss: 1.7964\n",
      "Batch 374, Loss: 1.8948\n",
      "Batch 375, Loss: 1.8847\n",
      "Batch 376, Loss: 1.8711\n",
      "Batch 377, Loss: 1.9249\n",
      "Batch 378, Loss: 1.8616\n",
      "Epoch 12, Average Loss: 1.8647\n",
      "Once there was a modern bow. She lived and was so excited to meet and patiently for lunch every day. She wanted people at the park. She wore her patcase \n",
      "and ran through it with shiny rope nose. The squirrels shared every day, the big storm patches, when Jime, one more frost of beautiful patternces, the two branches is tired. \n",
      "She immediately greedy and patient to make the best pattern with joy. One day, a fox came out from the tree. He made a wave and holding it. He put \n",
      "it in his patternches. He was so glad and the branches metal. The bees how it worked! He explored the whole worked hard to reach patterns the beetle. He was \n",
      "so glad he could cover it. He finished step and dreamed of all the chimns because Next, he could hide the sitchells. The end! \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.8456\n",
      "Batch 2, Loss: 1.8814\n",
      "Batch 3, Loss: 1.9011\n",
      "Batch 4, Loss: 1.7936\n",
      "Batch 5, Loss: 1.8251\n",
      "Batch 6, Loss: 1.7396\n",
      "Batch 7, Loss: 1.8979\n",
      "Batch 8, Loss: 1.7542\n",
      "Batch 9, Loss: 1.8504\n",
      "Batch 10, Loss: 1.8301\n",
      "Batch 11, Loss: 1.8691\n",
      "Batch 12, Loss: 1.8506\n",
      "Batch 13, Loss: 1.7793\n",
      "Batch 14, Loss: 1.8330\n",
      "Batch 15, Loss: 1.7702\n",
      "Batch 16, Loss: 1.8290\n",
      "Batch 17, Loss: 1.8507\n",
      "Batch 18, Loss: 1.8445\n",
      "Batch 19, Loss: 1.8619\n",
      "Batch 20, Loss: 1.7458\n",
      "Batch 21, Loss: 1.8071\n",
      "Batch 22, Loss: 1.8929\n",
      "Batch 23, Loss: 1.8647\n",
      "Batch 24, Loss: 1.7507\n",
      "Batch 25, Loss: 1.7105\n",
      "Batch 26, Loss: 1.7079\n",
      "Batch 27, Loss: 1.8249\n",
      "Batch 28, Loss: 1.8852\n",
      "Batch 29, Loss: 1.8740\n",
      "Batch 30, Loss: 1.8950\n",
      "Batch 31, Loss: 1.8725\n",
      "Batch 32, Loss: 1.8327\n",
      "Batch 33, Loss: 1.8579\n",
      "Batch 34, Loss: 1.8402\n",
      "Batch 35, Loss: 1.7794\n",
      "Batch 36, Loss: 1.7580\n",
      "Batch 37, Loss: 1.8184\n",
      "Batch 38, Loss: 1.8687\n",
      "Batch 39, Loss: 1.8735\n",
      "Batch 40, Loss: 1.7971\n",
      "Batch 41, Loss: 1.8785\n",
      "Batch 42, Loss: 1.8578\n",
      "Batch 43, Loss: 1.8779\n",
      "Batch 44, Loss: 1.8521\n",
      "Batch 45, Loss: 1.8395\n",
      "Batch 46, Loss: 1.8421\n",
      "Batch 47, Loss: 1.8096\n",
      "Batch 48, Loss: 1.7652\n",
      "Batch 49, Loss: 1.8230\n",
      "Batch 50, Loss: 1.8786\n",
      "Batch 51, Loss: 1.8020\n",
      "Batch 52, Loss: 1.7482\n",
      "Batch 53, Loss: 1.8943\n",
      "Batch 54, Loss: 1.7872\n",
      "Batch 55, Loss: 1.8352\n",
      "Batch 56, Loss: 1.7371\n",
      "Batch 57, Loss: 1.8369\n",
      "Batch 58, Loss: 1.8767\n",
      "Batch 59, Loss: 1.8729\n",
      "Batch 60, Loss: 1.8550\n",
      "Batch 61, Loss: 1.8427\n",
      "Batch 62, Loss: 1.8490\n",
      "Batch 63, Loss: 1.7676\n",
      "Batch 64, Loss: 1.7515\n",
      "Batch 65, Loss: 1.8568\n",
      "Batch 66, Loss: 1.8385\n",
      "Batch 67, Loss: 1.8685\n",
      "Batch 68, Loss: 1.8019\n",
      "Batch 69, Loss: 1.8049\n",
      "Batch 70, Loss: 1.8917\n",
      "Batch 71, Loss: 1.7809\n",
      "Batch 72, Loss: 1.8498\n",
      "Batch 73, Loss: 1.8233\n",
      "Batch 74, Loss: 1.8881\n",
      "Batch 75, Loss: 1.8367\n",
      "Batch 76, Loss: 1.7860\n",
      "Batch 77, Loss: 1.8397\n",
      "Batch 78, Loss: 1.7489\n",
      "Batch 79, Loss: 1.8120\n",
      "Batch 80, Loss: 1.8296\n",
      "Batch 81, Loss: 1.8255\n",
      "Batch 82, Loss: 1.8728\n",
      "Batch 83, Loss: 1.8861\n",
      "Batch 84, Loss: 1.8442\n",
      "Batch 85, Loss: 1.8596\n",
      "Batch 86, Loss: 1.8378\n",
      "Batch 87, Loss: 1.8588\n",
      "Batch 88, Loss: 1.8620\n",
      "Batch 89, Loss: 1.8081\n",
      "Batch 90, Loss: 1.9082\n",
      "Batch 91, Loss: 1.8755\n",
      "Batch 92, Loss: 1.8158\n",
      "Batch 93, Loss: 1.8360\n",
      "Batch 94, Loss: 1.7863\n",
      "Batch 95, Loss: 1.8627\n",
      "Batch 96, Loss: 1.8064\n",
      "Batch 97, Loss: 1.8448\n",
      "Batch 98, Loss: 1.8547\n",
      "Batch 99, Loss: 1.8883\n",
      "Batch 100, Loss: 1.7726\n",
      "Batch 101, Loss: 1.8627\n",
      "Batch 102, Loss: 1.8248\n",
      "Batch 103, Loss: 1.8617\n",
      "Batch 104, Loss: 1.7408\n",
      "Batch 105, Loss: 1.8367\n",
      "Batch 106, Loss: 1.8239\n",
      "Batch 107, Loss: 1.8409\n",
      "Batch 108, Loss: 1.7986\n",
      "Batch 109, Loss: 1.8168\n",
      "Batch 110, Loss: 1.8479\n",
      "Batch 111, Loss: 1.8335\n",
      "Batch 112, Loss: 1.8663\n",
      "Batch 113, Loss: 1.8345\n",
      "Batch 114, Loss: 1.8242\n",
      "Batch 115, Loss: 1.7880\n",
      "Batch 116, Loss: 1.7574\n",
      "Batch 117, Loss: 1.8032\n",
      "Batch 118, Loss: 1.8650\n",
      "Batch 119, Loss: 1.8678\n",
      "Batch 120, Loss: 1.7778\n",
      "Batch 121, Loss: 1.8384\n",
      "Batch 122, Loss: 1.8423\n",
      "Batch 123, Loss: 1.8098\n",
      "Batch 124, Loss: 1.8383\n",
      "Batch 125, Loss: 1.8398\n",
      "Batch 126, Loss: 1.8209\n",
      "Batch 127, Loss: 1.8470\n",
      "Batch 128, Loss: 1.4523\n",
      "Batch 129, Loss: 1.8584\n",
      "Batch 130, Loss: 1.8821\n",
      "Batch 131, Loss: 1.8426\n",
      "Batch 132, Loss: 1.8628\n",
      "Batch 133, Loss: 1.8577\n",
      "Batch 134, Loss: 1.8873\n",
      "Batch 135, Loss: 1.7982\n",
      "Batch 136, Loss: 1.8603\n",
      "Batch 137, Loss: 1.8650\n",
      "Batch 138, Loss: 1.8786\n",
      "Batch 139, Loss: 1.7863\n",
      "Batch 140, Loss: 1.7603\n",
      "Batch 141, Loss: 1.7964\n",
      "Batch 142, Loss: 1.8594\n",
      "Batch 143, Loss: 1.8640\n",
      "Batch 144, Loss: 1.8566\n",
      "Batch 145, Loss: 1.8727\n",
      "Batch 146, Loss: 1.8728\n",
      "Batch 147, Loss: 1.8968\n",
      "Batch 148, Loss: 1.9036\n",
      "Batch 149, Loss: 1.8453\n",
      "Batch 150, Loss: 1.8447\n",
      "Batch 151, Loss: 1.8898\n",
      "Batch 152, Loss: 1.7897\n",
      "Batch 153, Loss: 1.8563\n",
      "Batch 154, Loss: 1.7425\n",
      "Batch 155, Loss: 1.7934\n",
      "Batch 156, Loss: 1.7441\n",
      "Batch 157, Loss: 1.8119\n",
      "Batch 158, Loss: 1.8860\n",
      "Batch 159, Loss: 1.8765\n",
      "Batch 160, Loss: 1.8556\n",
      "Batch 161, Loss: 1.8415\n",
      "Batch 162, Loss: 1.7817\n",
      "Batch 163, Loss: 1.7858\n",
      "Batch 164, Loss: 1.7570\n",
      "Batch 165, Loss: 1.7682\n",
      "Batch 166, Loss: 1.8566\n",
      "Batch 167, Loss: 1.8630\n",
      "Batch 168, Loss: 1.8507\n",
      "Batch 169, Loss: 1.7530\n",
      "Batch 170, Loss: 1.7418\n",
      "Batch 171, Loss: 1.8862\n",
      "Batch 172, Loss: 1.7370\n",
      "Batch 173, Loss: 1.8697\n",
      "Batch 174, Loss: 1.7645\n",
      "Batch 175, Loss: 1.8501\n",
      "Batch 176, Loss: 1.7278\n",
      "Batch 177, Loss: 1.8270\n",
      "Batch 178, Loss: 1.8421\n",
      "Batch 179, Loss: 1.7359\n",
      "Batch 180, Loss: 1.8237\n",
      "Batch 181, Loss: 1.8808\n",
      "Batch 182, Loss: 1.8770\n",
      "Batch 183, Loss: 1.7891\n",
      "Batch 184, Loss: 1.8617\n",
      "Batch 185, Loss: 1.8303\n",
      "Batch 186, Loss: 1.8346\n",
      "Batch 187, Loss: 1.8475\n",
      "Batch 188, Loss: 1.8564\n",
      "Batch 189, Loss: 1.8503\n",
      "Batch 190, Loss: 1.8160\n",
      "Batch 191, Loss: 1.8489\n",
      "Batch 192, Loss: 1.7967\n",
      "Batch 193, Loss: 1.8688\n",
      "Batch 194, Loss: 1.8022\n",
      "Batch 195, Loss: 1.8382\n",
      "Batch 196, Loss: 1.8619\n",
      "Batch 197, Loss: 1.8773\n",
      "Batch 198, Loss: 1.8357\n",
      "Batch 199, Loss: 1.7241\n",
      "Batch 200, Loss: 1.8847\n",
      "Once upon a time there was a little boy named Jack. He was three friends who liked to play with games on the sidewalk. He would blow a tall branch \n",
      "to his grandpe that was OK! One day Jack had a big tiny shirt. He was very icletra and wanted to have a veterin to play with. But Jack knew \n",
      "he had to do something special because his weak friend had already impossible. He felt so icy. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 23.24 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.8408\n",
      "Batch 202, Loss: 1.8451\n",
      "Batch 203, Loss: 1.8456\n",
      "Batch 204, Loss: 1.8019\n",
      "Batch 205, Loss: 1.8395\n",
      "Batch 206, Loss: 1.8626\n",
      "Batch 207, Loss: 1.7815\n",
      "Batch 208, Loss: 1.7894\n",
      "Batch 209, Loss: 1.7690\n",
      "Batch 210, Loss: 1.8242\n",
      "Batch 211, Loss: 1.8493\n",
      "Batch 212, Loss: 1.8294\n",
      "Batch 213, Loss: 1.8181\n",
      "Batch 214, Loss: 1.8561\n",
      "Batch 215, Loss: 1.8056\n",
      "Batch 216, Loss: 1.8661\n",
      "Batch 217, Loss: 1.8496\n",
      "Batch 218, Loss: 1.8567\n",
      "Batch 219, Loss: 1.8221\n",
      "Batch 220, Loss: 1.8438\n",
      "Batch 221, Loss: 1.8392\n",
      "Batch 222, Loss: 1.7390\n",
      "Batch 223, Loss: 1.8431\n",
      "Batch 224, Loss: 1.7992\n",
      "Batch 225, Loss: 1.8360\n",
      "Batch 226, Loss: 1.8545\n",
      "Batch 227, Loss: 1.7412\n",
      "Batch 228, Loss: 1.8650\n",
      "Batch 229, Loss: 1.8210\n",
      "Batch 230, Loss: 1.8326\n",
      "Batch 231, Loss: 1.8189\n",
      "Batch 232, Loss: 1.8750\n",
      "Batch 233, Loss: 1.8587\n",
      "Batch 234, Loss: 1.7739\n",
      "Batch 235, Loss: 1.8495\n",
      "Batch 236, Loss: 1.7816\n",
      "Batch 237, Loss: 1.8172\n",
      "Batch 238, Loss: 1.7646\n",
      "Batch 239, Loss: 1.8493\n",
      "Batch 240, Loss: 1.8712\n",
      "Batch 241, Loss: 1.8001\n",
      "Batch 242, Loss: 1.8618\n",
      "Batch 243, Loss: 1.8581\n",
      "Batch 244, Loss: 1.8738\n",
      "Batch 245, Loss: 1.8635\n",
      "Batch 246, Loss: 1.8495\n",
      "Batch 247, Loss: 1.8893\n",
      "Batch 248, Loss: 1.8667\n",
      "Batch 249, Loss: 1.8050\n",
      "Batch 250, Loss: 1.7342\n",
      "Batch 251, Loss: 1.8640\n",
      "Batch 252, Loss: 1.8455\n",
      "Batch 253, Loss: 1.7987\n",
      "Batch 254, Loss: 1.8458\n",
      "Batch 255, Loss: 1.8444\n",
      "Batch 256, Loss: 1.7988\n",
      "Batch 257, Loss: 1.8577\n",
      "Batch 258, Loss: 1.8355\n",
      "Batch 259, Loss: 1.8737\n",
      "Batch 260, Loss: 1.8697\n",
      "Batch 261, Loss: 1.8193\n",
      "Batch 262, Loss: 1.8884\n",
      "Batch 263, Loss: 1.8565\n",
      "Batch 264, Loss: 1.8792\n",
      "Batch 265, Loss: 1.8947\n",
      "Batch 266, Loss: 1.8149\n",
      "Batch 267, Loss: 1.8461\n",
      "Batch 268, Loss: 1.8480\n",
      "Batch 269, Loss: 1.8333\n",
      "Batch 270, Loss: 1.8423\n",
      "Batch 271, Loss: 1.7649\n",
      "Batch 272, Loss: 1.8285\n",
      "Batch 273, Loss: 1.7796\n",
      "Batch 274, Loss: 1.8414\n",
      "Batch 275, Loss: 1.7552\n",
      "Batch 276, Loss: 1.8557\n",
      "Batch 277, Loss: 1.8454\n",
      "Batch 278, Loss: 1.8472\n",
      "Batch 279, Loss: 1.8255\n",
      "Batch 280, Loss: 1.7535\n",
      "Batch 281, Loss: 1.8430\n",
      "Batch 282, Loss: 1.8433\n",
      "Batch 283, Loss: 1.8346\n",
      "Batch 284, Loss: 1.8309\n",
      "Batch 285, Loss: 1.8204\n",
      "Batch 286, Loss: 1.8223\n",
      "Batch 287, Loss: 1.8323\n",
      "Batch 288, Loss: 1.7935\n",
      "Batch 289, Loss: 1.8158\n",
      "Batch 290, Loss: 1.8821\n",
      "Batch 291, Loss: 1.8590\n",
      "Batch 292, Loss: 1.8165\n",
      "Batch 293, Loss: 1.7798\n",
      "Batch 294, Loss: 1.8235\n",
      "Batch 295, Loss: 1.8719\n",
      "Batch 296, Loss: 1.8568\n",
      "Batch 297, Loss: 1.8566\n",
      "Batch 298, Loss: 1.8899\n",
      "Batch 299, Loss: 1.8266\n",
      "Batch 300, Loss: 1.8606\n",
      "Batch 301, Loss: 1.7872\n",
      "Batch 302, Loss: 1.8282\n",
      "Batch 303, Loss: 1.8309\n",
      "Batch 304, Loss: 1.7583\n",
      "Batch 305, Loss: 1.8093\n",
      "Batch 306, Loss: 1.8676\n",
      "Batch 307, Loss: 1.8422\n",
      "Batch 308, Loss: 1.8556\n",
      "Batch 309, Loss: 1.8437\n",
      "Batch 310, Loss: 1.8172\n",
      "Batch 311, Loss: 1.8463\n",
      "Batch 312, Loss: 1.7709\n",
      "Batch 313, Loss: 1.7774\n",
      "Batch 314, Loss: 1.8052\n",
      "Batch 315, Loss: 1.8483\n",
      "Batch 316, Loss: 1.7770\n",
      "Batch 317, Loss: 1.8444\n",
      "Batch 318, Loss: 1.6984\n",
      "Batch 319, Loss: 1.8459\n",
      "Batch 320, Loss: 1.8242\n",
      "Batch 321, Loss: 1.8361\n",
      "Batch 322, Loss: 1.8208\n",
      "Batch 323, Loss: 1.8756\n",
      "Batch 324, Loss: 1.8584\n",
      "Batch 325, Loss: 1.8554\n",
      "Batch 326, Loss: 1.8418\n",
      "Batch 327, Loss: 1.8546\n",
      "Batch 328, Loss: 1.8769\n",
      "Batch 329, Loss: 1.8449\n",
      "Batch 330, Loss: 1.8423\n",
      "Batch 331, Loss: 1.8248\n",
      "Batch 332, Loss: 1.7795\n",
      "Batch 333, Loss: 1.8403\n",
      "Batch 334, Loss: 1.8339\n",
      "Batch 335, Loss: 1.7924\n",
      "Batch 336, Loss: 1.8150\n",
      "Batch 337, Loss: 1.8157\n",
      "Batch 338, Loss: 1.7646\n",
      "Batch 339, Loss: 1.8095\n",
      "Batch 340, Loss: 1.8238\n",
      "Batch 341, Loss: 1.8322\n",
      "Batch 342, Loss: 1.7217\n",
      "Batch 343, Loss: 1.7417\n",
      "Batch 344, Loss: 1.8141\n",
      "Batch 345, Loss: 1.8355\n",
      "Batch 346, Loss: 1.7855\n",
      "Batch 347, Loss: 1.8278\n",
      "Batch 348, Loss: 1.8688\n",
      "Batch 349, Loss: 1.8364\n",
      "Batch 350, Loss: 1.8454\n",
      "Batch 351, Loss: 1.7640\n",
      "Batch 352, Loss: 1.8277\n",
      "Batch 353, Loss: 1.8630\n",
      "Batch 354, Loss: 1.8525\n",
      "Batch 355, Loss: 1.8346\n",
      "Batch 356, Loss: 1.8542\n",
      "Batch 357, Loss: 1.8503\n",
      "Batch 358, Loss: 1.8294\n",
      "Batch 359, Loss: 1.7766\n",
      "Batch 360, Loss: 1.8714\n",
      "Batch 361, Loss: 1.7930\n",
      "Batch 362, Loss: 1.7914\n",
      "Batch 363, Loss: 1.8620\n",
      "Batch 364, Loss: 1.8497\n",
      "Batch 365, Loss: 1.8001\n",
      "Batch 366, Loss: 1.8301\n",
      "Batch 367, Loss: 1.8113\n",
      "Batch 368, Loss: 1.8302\n",
      "Batch 369, Loss: 1.7971\n",
      "Batch 370, Loss: 1.7709\n",
      "Batch 371, Loss: 1.8219\n",
      "Batch 372, Loss: 1.7983\n",
      "Batch 373, Loss: 1.8524\n",
      "Batch 374, Loss: 1.7518\n",
      "Batch 375, Loss: 1.8519\n",
      "Batch 376, Loss: 1.7376\n",
      "Batch 377, Loss: 1.7485\n",
      "Batch 378, Loss: 1.8515\n",
      "Epoch 13, Average Loss: 1.8276\n",
      "Tim and Lily are friends. They like to play pretend. They have a cat on each other, or a cat. They have a bottle on it. They pretend they are \n",
      "pretend to make people. One day, they find a chef. They want to make a picture of a most. They run to the box next to the clothes and pretend \n",
      "they are pink hard. The knot is not like is a place that caw is to remove them. Tim has an idea. He gives a doll with a package. It \n",
      "can make sounds. \"Look, a joke, these are bride us!\" Lily says. They put on the sawsom and wands his tail. They pretend to be best friends. They make their \n",
      "house a buttony is tiny. They dive. They ride the box. They decide to make their house feel better. They have fun in stories and find a story. Then they \n",
      "can put the package on the horizon. They pretend to play together in the garden. They look for the stories. They do not like the problem. But they are good \n",
      "friends. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.7925\n",
      "Batch 2, Loss: 1.7787\n",
      "Batch 3, Loss: 1.8081\n",
      "Batch 4, Loss: 1.8255\n",
      "Batch 5, Loss: 1.7838\n",
      "Batch 6, Loss: 1.8354\n",
      "Batch 7, Loss: 1.7909\n",
      "Batch 8, Loss: 1.8530\n",
      "Batch 9, Loss: 1.8257\n",
      "Batch 10, Loss: 1.7891\n",
      "Batch 11, Loss: 1.8493\n",
      "Batch 12, Loss: 1.7872\n",
      "Batch 13, Loss: 1.8169\n",
      "Batch 14, Loss: 1.7450\n",
      "Batch 15, Loss: 1.6990\n",
      "Batch 16, Loss: 1.7879\n",
      "Batch 17, Loss: 1.8148\n",
      "Batch 18, Loss: 1.7990\n",
      "Batch 19, Loss: 1.7787\n",
      "Batch 20, Loss: 1.8468\n",
      "Batch 21, Loss: 1.7600\n",
      "Batch 22, Loss: 1.8426\n",
      "Batch 23, Loss: 1.8529\n",
      "Batch 24, Loss: 1.8277\n",
      "Batch 25, Loss: 1.8051\n",
      "Batch 26, Loss: 1.7041\n",
      "Batch 27, Loss: 1.8056\n",
      "Batch 28, Loss: 1.8061\n",
      "Batch 29, Loss: 1.8438\n",
      "Batch 30, Loss: 1.8152\n",
      "Batch 31, Loss: 1.8073\n",
      "Batch 32, Loss: 1.8406\n",
      "Batch 33, Loss: 1.7947\n",
      "Batch 34, Loss: 1.7964\n",
      "Batch 35, Loss: 1.8397\n",
      "Batch 36, Loss: 1.8180\n",
      "Batch 37, Loss: 1.8369\n",
      "Batch 38, Loss: 1.7745\n",
      "Batch 39, Loss: 1.8577\n",
      "Batch 40, Loss: 1.8440\n",
      "Batch 41, Loss: 1.7050\n",
      "Batch 42, Loss: 1.8488\n",
      "Batch 43, Loss: 1.7347\n",
      "Batch 44, Loss: 1.8008\n",
      "Batch 45, Loss: 1.8177\n",
      "Batch 46, Loss: 1.8100\n",
      "Batch 47, Loss: 1.7085\n",
      "Batch 48, Loss: 1.8100\n",
      "Batch 49, Loss: 1.7318\n",
      "Batch 50, Loss: 1.8311\n",
      "Batch 51, Loss: 1.7758\n",
      "Batch 52, Loss: 1.8160\n",
      "Batch 53, Loss: 1.8247\n",
      "Batch 54, Loss: 1.7756\n",
      "Batch 55, Loss: 1.8075\n",
      "Batch 56, Loss: 1.7903\n",
      "Batch 57, Loss: 1.7127\n",
      "Batch 58, Loss: 1.7421\n",
      "Batch 59, Loss: 1.8349\n",
      "Batch 60, Loss: 1.8044\n",
      "Batch 61, Loss: 1.8161\n",
      "Batch 62, Loss: 1.8337\n",
      "Batch 63, Loss: 1.7837\n",
      "Batch 64, Loss: 1.8323\n",
      "Batch 65, Loss: 1.8175\n",
      "Batch 66, Loss: 1.8210\n",
      "Batch 67, Loss: 1.8688\n",
      "Batch 68, Loss: 1.8255\n",
      "Batch 69, Loss: 1.8518\n",
      "Batch 70, Loss: 1.7896\n",
      "Batch 71, Loss: 1.7293\n",
      "Batch 72, Loss: 1.8006\n",
      "Batch 73, Loss: 1.8199\n",
      "Batch 74, Loss: 1.8128\n",
      "Batch 75, Loss: 1.7748\n",
      "Batch 76, Loss: 1.7227\n",
      "Batch 77, Loss: 1.7941\n",
      "Batch 78, Loss: 1.7279\n",
      "Batch 79, Loss: 1.8255\n",
      "Batch 80, Loss: 1.8199\n",
      "Batch 81, Loss: 1.8081\n",
      "Batch 82, Loss: 1.7845\n",
      "Batch 83, Loss: 1.7678\n",
      "Batch 84, Loss: 1.7076\n",
      "Batch 85, Loss: 1.7161\n",
      "Batch 86, Loss: 1.7321\n",
      "Batch 87, Loss: 1.8438\n",
      "Batch 88, Loss: 1.7925\n",
      "Batch 89, Loss: 1.8028\n",
      "Batch 90, Loss: 1.7206\n",
      "Batch 91, Loss: 1.7125\n",
      "Batch 92, Loss: 1.7668\n",
      "Batch 93, Loss: 1.7479\n",
      "Batch 94, Loss: 1.8410\n",
      "Batch 95, Loss: 1.8133\n",
      "Batch 96, Loss: 1.8044\n",
      "Batch 97, Loss: 1.8497\n",
      "Batch 98, Loss: 1.8521\n",
      "Batch 99, Loss: 1.7797\n",
      "Batch 100, Loss: 1.8301\n",
      "Batch 101, Loss: 1.8271\n",
      "Batch 102, Loss: 1.8361\n",
      "Batch 103, Loss: 1.8105\n",
      "Batch 104, Loss: 1.7863\n",
      "Batch 105, Loss: 1.7130\n",
      "Batch 106, Loss: 1.7482\n",
      "Batch 107, Loss: 1.7877\n",
      "Batch 108, Loss: 1.7655\n",
      "Batch 109, Loss: 1.8165\n",
      "Batch 110, Loss: 1.7740\n",
      "Batch 111, Loss: 1.8167\n",
      "Batch 112, Loss: 1.7956\n",
      "Batch 113, Loss: 1.8617\n",
      "Batch 114, Loss: 1.8277\n",
      "Batch 115, Loss: 1.8450\n",
      "Batch 116, Loss: 1.7618\n",
      "Batch 117, Loss: 1.8330\n",
      "Batch 118, Loss: 1.7542\n",
      "Batch 119, Loss: 1.7995\n",
      "Batch 120, Loss: 1.8223\n",
      "Batch 121, Loss: 1.8263\n",
      "Batch 122, Loss: 1.6836\n",
      "Batch 123, Loss: 1.8523\n",
      "Batch 124, Loss: 1.8294\n",
      "Batch 125, Loss: 1.7275\n",
      "Batch 126, Loss: 1.7902\n",
      "Batch 127, Loss: 1.7466\n",
      "Batch 128, Loss: 1.8109\n",
      "Batch 129, Loss: 1.7828\n",
      "Batch 130, Loss: 1.8400\n",
      "Batch 131, Loss: 1.8474\n",
      "Batch 132, Loss: 1.7050\n",
      "Batch 133, Loss: 1.7951\n",
      "Batch 134, Loss: 1.8197\n",
      "Batch 135, Loss: 1.8217\n",
      "Batch 136, Loss: 1.7801\n",
      "Batch 137, Loss: 1.8225\n",
      "Batch 138, Loss: 1.8025\n",
      "Batch 139, Loss: 1.8129\n",
      "Batch 140, Loss: 1.7969\n",
      "Batch 141, Loss: 1.7961\n",
      "Batch 142, Loss: 1.8014\n",
      "Batch 143, Loss: 1.7840\n",
      "Batch 144, Loss: 1.8173\n",
      "Batch 145, Loss: 1.7657\n",
      "Batch 146, Loss: 1.8148\n",
      "Batch 147, Loss: 1.8628\n",
      "Batch 148, Loss: 1.7057\n",
      "Batch 149, Loss: 1.8208\n",
      "Batch 150, Loss: 1.7481\n",
      "Batch 151, Loss: 1.7615\n",
      "Batch 152, Loss: 1.7322\n",
      "Batch 153, Loss: 1.6903\n",
      "Batch 154, Loss: 1.7083\n",
      "Batch 155, Loss: 1.8225\n",
      "Batch 156, Loss: 1.7807\n",
      "Batch 157, Loss: 1.8268\n",
      "Batch 158, Loss: 1.8560\n",
      "Batch 159, Loss: 1.8187\n",
      "Batch 160, Loss: 1.6937\n",
      "Batch 161, Loss: 1.8145\n",
      "Batch 162, Loss: 1.7950\n",
      "Batch 163, Loss: 1.7141\n",
      "Batch 164, Loss: 1.7314\n",
      "Batch 165, Loss: 1.8538\n",
      "Batch 166, Loss: 1.8135\n",
      "Batch 167, Loss: 1.8388\n",
      "Batch 168, Loss: 1.8104\n",
      "Batch 169, Loss: 1.8312\n",
      "Batch 170, Loss: 1.8493\n",
      "Batch 171, Loss: 1.8384\n",
      "Batch 172, Loss: 1.8240\n",
      "Batch 173, Loss: 1.8130\n",
      "Batch 174, Loss: 1.8163\n",
      "Batch 175, Loss: 1.8256\n",
      "Batch 176, Loss: 1.8266\n",
      "Batch 177, Loss: 1.7579\n",
      "Batch 178, Loss: 1.8169\n",
      "Batch 179, Loss: 1.7855\n",
      "Batch 180, Loss: 1.7364\n",
      "Batch 181, Loss: 1.8369\n",
      "Batch 182, Loss: 1.7358\n",
      "Batch 183, Loss: 1.8180\n",
      "Batch 184, Loss: 1.8309\n",
      "Batch 185, Loss: 1.6965\n",
      "Batch 186, Loss: 1.7090\n",
      "Batch 187, Loss: 1.8151\n",
      "Batch 188, Loss: 1.7700\n",
      "Batch 189, Loss: 1.8443\n",
      "Batch 190, Loss: 1.8368\n",
      "Batch 191, Loss: 1.7581\n",
      "Batch 192, Loss: 1.8087\n",
      "Batch 193, Loss: 1.7932\n",
      "Batch 194, Loss: 1.7902\n",
      "Batch 195, Loss: 1.8015\n",
      "Batch 196, Loss: 1.8106\n",
      "Batch 197, Loss: 1.7852\n",
      "Batch 198, Loss: 1.8060\n",
      "Batch 199, Loss: 1.8027\n",
      "Batch 200, Loss: 1.8351\n",
      "One day, a little boy named Tim went to the zoo with his mom. Tim saw a big patch of purple pipe under or small ice cup patien from the \n",
      "zipper. He wanted to see if it was not captive. The zoo held onto his pipe in his hand and pretti back. Tim's arm thought it was fake and had \n",
      "made a big patch of paper ice cupcake. Tim did not know what the zoo he wanted to be just left. But then, he heard the zoo whoosing. Tim did \n",
      "not know what the zoo and the knife was. The zookeeper had a fake piece of bubbles. The zookeeper was scared and scared. He started to cry. When Tim went \n",
      "home, he felt peaceful and strong. The zookeeper was not scared. He did not know the zookeeper becomids come and saw the consequencess. It was the zookeeper of the coveriest \n",
      "of Queen hopess. His no mames the zoo was not going in the zoo anymore. Tim and his zoo were scared, and they knew it. They learned that sometimes we \n",
      "have to be careful and always know what we don't do. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 21.66 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.8142\n",
      "Batch 202, Loss: 1.7923\n",
      "Batch 203, Loss: 1.7438\n",
      "Batch 204, Loss: 1.7445\n",
      "Batch 205, Loss: 1.8486\n",
      "Batch 206, Loss: 1.7230\n",
      "Batch 207, Loss: 1.7978\n",
      "Batch 208, Loss: 1.7832\n",
      "Batch 209, Loss: 1.8229\n",
      "Batch 210, Loss: 1.7861\n",
      "Batch 211, Loss: 1.7160\n",
      "Batch 212, Loss: 1.8024\n",
      "Batch 213, Loss: 1.8392\n",
      "Batch 214, Loss: 1.7511\n",
      "Batch 215, Loss: 1.8004\n",
      "Batch 216, Loss: 1.8370\n",
      "Batch 217, Loss: 1.8183\n",
      "Batch 218, Loss: 1.8094\n",
      "Batch 219, Loss: 1.8399\n",
      "Batch 220, Loss: 1.7602\n",
      "Batch 221, Loss: 1.8617\n",
      "Batch 222, Loss: 1.8178\n",
      "Batch 223, Loss: 1.8300\n",
      "Batch 224, Loss: 1.8280\n",
      "Batch 225, Loss: 1.8064\n",
      "Batch 226, Loss: 1.8168\n",
      "Batch 227, Loss: 1.7526\n",
      "Batch 228, Loss: 1.7373\n",
      "Batch 229, Loss: 1.8416\n",
      "Batch 230, Loss: 1.8245\n",
      "Batch 231, Loss: 1.8078\n",
      "Batch 232, Loss: 1.7781\n",
      "Batch 233, Loss: 1.7814\n",
      "Batch 234, Loss: 1.8129\n",
      "Batch 235, Loss: 1.8365\n",
      "Batch 236, Loss: 1.7856\n",
      "Batch 237, Loss: 1.8423\n",
      "Batch 238, Loss: 1.8414\n",
      "Batch 239, Loss: 1.8193\n",
      "Batch 240, Loss: 1.8131\n",
      "Batch 241, Loss: 1.8431\n",
      "Batch 242, Loss: 1.7479\n",
      "Batch 243, Loss: 1.7976\n",
      "Batch 244, Loss: 1.7275\n",
      "Batch 245, Loss: 1.8554\n",
      "Batch 246, Loss: 1.7870\n",
      "Batch 247, Loss: 1.7399\n",
      "Batch 248, Loss: 1.7666\n",
      "Batch 249, Loss: 1.7992\n",
      "Batch 250, Loss: 1.8702\n",
      "Batch 251, Loss: 1.8264\n",
      "Batch 252, Loss: 1.7707\n",
      "Batch 253, Loss: 1.8138\n",
      "Batch 254, Loss: 1.8222\n",
      "Batch 255, Loss: 1.8076\n",
      "Batch 256, Loss: 1.7520\n",
      "Batch 257, Loss: 1.8266\n",
      "Batch 258, Loss: 1.7758\n",
      "Batch 259, Loss: 1.7687\n",
      "Batch 260, Loss: 1.8077\n",
      "Batch 261, Loss: 1.7118\n",
      "Batch 262, Loss: 1.7408\n",
      "Batch 263, Loss: 1.8400\n",
      "Batch 264, Loss: 1.8183\n",
      "Batch 265, Loss: 1.8070\n",
      "Batch 266, Loss: 1.8059\n",
      "Batch 267, Loss: 1.8256\n",
      "Batch 268, Loss: 1.8011\n",
      "Batch 269, Loss: 1.7862\n",
      "Batch 270, Loss: 1.7816\n",
      "Batch 271, Loss: 1.8015\n",
      "Batch 272, Loss: 1.7396\n",
      "Batch 273, Loss: 1.7872\n",
      "Batch 274, Loss: 1.8061\n",
      "Batch 275, Loss: 1.8483\n",
      "Batch 276, Loss: 1.7428\n",
      "Batch 277, Loss: 1.7537\n",
      "Batch 278, Loss: 1.7601\n",
      "Batch 279, Loss: 1.8375\n",
      "Batch 280, Loss: 1.7918\n",
      "Batch 281, Loss: 1.8044\n",
      "Batch 282, Loss: 1.7983\n",
      "Batch 283, Loss: 1.8046\n",
      "Batch 284, Loss: 1.7885\n",
      "Batch 285, Loss: 1.8104\n",
      "Batch 286, Loss: 1.8308\n",
      "Batch 287, Loss: 1.7211\n",
      "Batch 288, Loss: 1.7217\n",
      "Batch 289, Loss: 1.7408\n",
      "Batch 290, Loss: 1.8382\n",
      "Batch 291, Loss: 1.7982\n",
      "Batch 292, Loss: 1.8029\n",
      "Batch 293, Loss: 1.8218\n",
      "Batch 294, Loss: 1.8328\n",
      "Batch 295, Loss: 1.8380\n",
      "Batch 296, Loss: 1.7773\n",
      "Batch 297, Loss: 1.8171\n",
      "Batch 298, Loss: 1.8424\n",
      "Batch 299, Loss: 1.8177\n",
      "Batch 300, Loss: 1.7912\n",
      "Batch 301, Loss: 1.8461\n",
      "Batch 302, Loss: 1.7591\n",
      "Batch 303, Loss: 1.8225\n",
      "Batch 304, Loss: 1.7159\n",
      "Batch 305, Loss: 1.8192\n",
      "Batch 306, Loss: 1.7949\n",
      "Batch 307, Loss: 1.8167\n",
      "Batch 308, Loss: 1.6942\n",
      "Batch 309, Loss: 1.8192\n",
      "Batch 310, Loss: 1.7657\n",
      "Batch 311, Loss: 1.6957\n",
      "Batch 312, Loss: 1.8131\n",
      "Batch 313, Loss: 1.8014\n",
      "Batch 314, Loss: 1.7062\n",
      "Batch 315, Loss: 1.7878\n",
      "Batch 316, Loss: 1.7608\n",
      "Batch 317, Loss: 1.7356\n",
      "Batch 318, Loss: 1.7723\n",
      "Batch 319, Loss: 1.7983\n",
      "Batch 320, Loss: 1.7865\n",
      "Batch 321, Loss: 1.8422\n",
      "Batch 322, Loss: 1.8044\n",
      "Batch 323, Loss: 1.8018\n",
      "Batch 324, Loss: 1.7520\n",
      "Batch 325, Loss: 1.8086\n",
      "Batch 326, Loss: 1.8448\n",
      "Batch 327, Loss: 1.7964\n",
      "Batch 328, Loss: 1.7542\n",
      "Batch 329, Loss: 1.7516\n",
      "Batch 330, Loss: 1.7946\n",
      "Batch 331, Loss: 1.8712\n",
      "Batch 332, Loss: 1.7332\n",
      "Batch 333, Loss: 1.8406\n",
      "Batch 334, Loss: 1.7371\n",
      "Batch 335, Loss: 1.4499\n",
      "Batch 336, Loss: 1.8509\n",
      "Batch 337, Loss: 1.7255\n",
      "Batch 338, Loss: 1.8468\n",
      "Batch 339, Loss: 1.8381\n",
      "Batch 340, Loss: 1.8312\n",
      "Batch 341, Loss: 1.7915\n",
      "Batch 342, Loss: 1.8445\n",
      "Batch 343, Loss: 1.8845\n",
      "Batch 344, Loss: 1.7729\n",
      "Batch 345, Loss: 1.7609\n",
      "Batch 346, Loss: 1.8147\n",
      "Batch 347, Loss: 1.8445\n",
      "Batch 348, Loss: 1.7342\n",
      "Batch 349, Loss: 1.8391\n",
      "Batch 350, Loss: 1.8389\n",
      "Batch 351, Loss: 1.8300\n",
      "Batch 352, Loss: 1.7915\n",
      "Batch 353, Loss: 1.7926\n",
      "Batch 354, Loss: 1.8204\n",
      "Batch 355, Loss: 1.7604\n",
      "Batch 356, Loss: 1.8328\n",
      "Batch 357, Loss: 1.7399\n",
      "Batch 358, Loss: 1.8153\n",
      "Batch 359, Loss: 1.8064\n",
      "Batch 360, Loss: 1.8076\n",
      "Batch 361, Loss: 1.7100\n",
      "Batch 362, Loss: 1.8249\n",
      "Batch 363, Loss: 1.8057\n",
      "Batch 364, Loss: 1.8293\n",
      "Batch 365, Loss: 1.8202\n",
      "Batch 366, Loss: 1.7945\n",
      "Batch 367, Loss: 1.8301\n",
      "Batch 368, Loss: 1.7327\n",
      "Batch 369, Loss: 1.8309\n",
      "Batch 370, Loss: 1.8090\n",
      "Batch 371, Loss: 1.8065\n",
      "Batch 372, Loss: 1.7705\n",
      "Batch 373, Loss: 1.6939\n",
      "Batch 374, Loss: 1.8369\n",
      "Batch 375, Loss: 1.8356\n",
      "Batch 376, Loss: 1.8256\n",
      "Batch 377, Loss: 1.8189\n",
      "Batch 378, Loss: 1.7727\n",
      "Epoch 14, Average Loss: 1.7952\n",
      "Once upon a time, there was a little mole who were very wealthy in a jungle. One day, he saw a fox that was very calm. The fox wanted to \n",
      "take a bite it to show the billy to offer. The bill had never seen the wood or that could make a tool. The fox wanted to give the billy \n",
      "look so they both up. He did not want to get it for a regular to explore the jungle. He was happy to tell everyone about the place of the \n",
      "small toy. He even asked his mom, \"Can we get it?\" His mom smiled and said, \"Yes, we can relax now. That's why wealthy.\" The fox waited and waited, but \n",
      "the woodet would always come back. He learned that it was more important to be useful too. He decided to be more careful and do wonder how much the billy \n",
      "was. He had no bills and no one Max shy. That night, when he woke up, the fox got in the ground and the jungle cone. He become piratiasm he \n",
      "heard a noise. He was so scared that his mom was there. She ran to tell him what had happened. The tired and the fox were nowhere to be pirate. \n",
      "They thanked the fox for being careful and they both exertly took the billy to fix the kitchen. Before held out his room and soon they were out of burned \n",
      "again. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.7335\n",
      "Batch 2, Loss: 1.7602\n",
      "Batch 3, Loss: 1.7705\n",
      "Batch 4, Loss: 1.6810\n",
      "Batch 5, Loss: 1.7296\n",
      "Batch 6, Loss: 1.7712\n",
      "Batch 7, Loss: 1.7691\n",
      "Batch 8, Loss: 1.8359\n",
      "Batch 9, Loss: 1.8368\n",
      "Batch 10, Loss: 1.8291\n",
      "Batch 11, Loss: 1.7947\n",
      "Batch 12, Loss: 1.7872\n",
      "Batch 13, Loss: 1.7684\n",
      "Batch 14, Loss: 1.7553\n",
      "Batch 15, Loss: 1.7643\n",
      "Batch 16, Loss: 1.8075\n",
      "Batch 17, Loss: 1.8162\n",
      "Batch 18, Loss: 1.7651\n",
      "Batch 19, Loss: 1.7459\n",
      "Batch 20, Loss: 1.7727\n",
      "Batch 21, Loss: 1.7923\n",
      "Batch 22, Loss: 1.7981\n",
      "Batch 23, Loss: 1.6793\n",
      "Batch 24, Loss: 1.8191\n",
      "Batch 25, Loss: 1.7918\n",
      "Batch 26, Loss: 1.7346\n",
      "Batch 27, Loss: 1.6467\n",
      "Batch 28, Loss: 1.8006\n",
      "Batch 29, Loss: 1.7172\n",
      "Batch 30, Loss: 1.7856\n",
      "Batch 31, Loss: 1.6993\n",
      "Batch 32, Loss: 1.7643\n",
      "Batch 33, Loss: 1.8248\n",
      "Batch 34, Loss: 1.7384\n",
      "Batch 35, Loss: 1.7878\n",
      "Batch 36, Loss: 1.7565\n",
      "Batch 37, Loss: 1.7513\n",
      "Batch 38, Loss: 1.6930\n",
      "Batch 39, Loss: 1.7602\n",
      "Batch 40, Loss: 1.6849\n",
      "Batch 41, Loss: 1.7746\n",
      "Batch 42, Loss: 1.7842\n",
      "Batch 43, Loss: 1.8065\n",
      "Batch 44, Loss: 1.8025\n",
      "Batch 45, Loss: 1.7474\n",
      "Batch 46, Loss: 1.7982\n",
      "Batch 47, Loss: 1.6990\n",
      "Batch 48, Loss: 1.7957\n",
      "Batch 49, Loss: 1.7550\n",
      "Batch 50, Loss: 1.7807\n",
      "Batch 51, Loss: 1.7706\n",
      "Batch 52, Loss: 1.8159\n",
      "Batch 53, Loss: 1.7815\n",
      "Batch 54, Loss: 1.7884\n",
      "Batch 55, Loss: 1.7969\n",
      "Batch 56, Loss: 1.7600\n",
      "Batch 57, Loss: 1.8100\n",
      "Batch 58, Loss: 1.7690\n",
      "Batch 59, Loss: 1.7144\n",
      "Batch 60, Loss: 1.7207\n",
      "Batch 61, Loss: 1.7720\n",
      "Batch 62, Loss: 1.7897\n",
      "Batch 63, Loss: 1.7905\n",
      "Batch 64, Loss: 1.7490\n",
      "Batch 65, Loss: 1.8192\n",
      "Batch 66, Loss: 1.7885\n",
      "Batch 67, Loss: 1.7220\n",
      "Batch 68, Loss: 1.7785\n",
      "Batch 69, Loss: 1.6874\n",
      "Batch 70, Loss: 1.6764\n",
      "Batch 71, Loss: 1.7778\n",
      "Batch 72, Loss: 1.7889\n",
      "Batch 73, Loss: 1.6971\n",
      "Batch 74, Loss: 1.7723\n",
      "Batch 75, Loss: 1.7615\n",
      "Batch 76, Loss: 1.7426\n",
      "Batch 77, Loss: 1.8031\n",
      "Batch 78, Loss: 1.7753\n",
      "Batch 79, Loss: 1.7658\n",
      "Batch 80, Loss: 1.7879\n",
      "Batch 81, Loss: 1.7343\n",
      "Batch 82, Loss: 1.8045\n",
      "Batch 83, Loss: 1.8031\n",
      "Batch 84, Loss: 1.7752\n",
      "Batch 85, Loss: 1.6752\n",
      "Batch 86, Loss: 1.7810\n",
      "Batch 87, Loss: 1.7237\n",
      "Batch 88, Loss: 1.8239\n",
      "Batch 89, Loss: 1.7482\n",
      "Batch 90, Loss: 1.8154\n",
      "Batch 91, Loss: 1.7679\n",
      "Batch 92, Loss: 1.7402\n",
      "Batch 93, Loss: 1.3344\n",
      "Batch 94, Loss: 1.8366\n",
      "Batch 95, Loss: 1.8224\n",
      "Batch 96, Loss: 1.8537\n",
      "Batch 97, Loss: 1.8220\n",
      "Batch 98, Loss: 1.8098\n",
      "Batch 99, Loss: 1.7802\n",
      "Batch 100, Loss: 1.7888\n",
      "Batch 101, Loss: 1.8153\n",
      "Batch 102, Loss: 1.6757\n",
      "Batch 103, Loss: 1.7950\n",
      "Batch 104, Loss: 1.7680\n",
      "Batch 105, Loss: 1.8235\n",
      "Batch 106, Loss: 1.8253\n",
      "Batch 107, Loss: 1.8037\n",
      "Batch 108, Loss: 1.7808\n",
      "Batch 109, Loss: 1.7977\n",
      "Batch 110, Loss: 1.7129\n",
      "Batch 111, Loss: 1.7293\n",
      "Batch 112, Loss: 1.8118\n",
      "Batch 113, Loss: 1.8253\n",
      "Batch 114, Loss: 1.7797\n",
      "Batch 115, Loss: 1.7266\n",
      "Batch 116, Loss: 1.7179\n",
      "Batch 117, Loss: 1.7006\n",
      "Batch 118, Loss: 1.8185\n",
      "Batch 119, Loss: 1.6906\n",
      "Batch 120, Loss: 1.8387\n",
      "Batch 121, Loss: 1.8471\n",
      "Batch 122, Loss: 1.7525\n",
      "Batch 123, Loss: 1.7929\n",
      "Batch 124, Loss: 1.7988\n",
      "Batch 125, Loss: 1.7989\n",
      "Batch 126, Loss: 1.8335\n",
      "Batch 127, Loss: 1.7880\n",
      "Batch 128, Loss: 1.7936\n",
      "Batch 129, Loss: 1.6947\n",
      "Batch 130, Loss: 1.7915\n",
      "Batch 131, Loss: 1.7127\n",
      "Batch 132, Loss: 1.7913\n",
      "Batch 133, Loss: 1.7219\n",
      "Batch 134, Loss: 1.6945\n",
      "Batch 135, Loss: 1.7723\n",
      "Batch 136, Loss: 1.7138\n",
      "Batch 137, Loss: 1.7817\n",
      "Batch 138, Loss: 1.7980\n",
      "Batch 139, Loss: 1.6980\n",
      "Batch 140, Loss: 1.8002\n",
      "Batch 141, Loss: 1.8246\n",
      "Batch 142, Loss: 1.7065\n",
      "Batch 143, Loss: 1.7165\n",
      "Batch 144, Loss: 1.7939\n",
      "Batch 145, Loss: 1.6940\n",
      "Batch 146, Loss: 1.7800\n",
      "Batch 147, Loss: 1.8197\n",
      "Batch 148, Loss: 1.7705\n",
      "Batch 149, Loss: 1.7495\n",
      "Batch 150, Loss: 1.7472\n",
      "Batch 151, Loss: 1.6942\n",
      "Batch 152, Loss: 1.7962\n",
      "Batch 153, Loss: 1.7775\n",
      "Batch 154, Loss: 1.6625\n",
      "Batch 155, Loss: 1.7720\n",
      "Batch 156, Loss: 1.7799\n",
      "Batch 157, Loss: 1.7644\n",
      "Batch 158, Loss: 1.7939\n",
      "Batch 159, Loss: 1.6719\n",
      "Batch 160, Loss: 1.7744\n",
      "Batch 161, Loss: 1.8144\n",
      "Batch 162, Loss: 1.7798\n",
      "Batch 163, Loss: 1.7846\n",
      "Batch 164, Loss: 1.7868\n",
      "Batch 165, Loss: 1.7756\n",
      "Batch 166, Loss: 1.7708\n",
      "Batch 167, Loss: 1.7319\n",
      "Batch 168, Loss: 1.8110\n",
      "Batch 169, Loss: 1.7366\n",
      "Batch 170, Loss: 1.7825\n",
      "Batch 171, Loss: 1.8133\n",
      "Batch 172, Loss: 1.7876\n",
      "Batch 173, Loss: 1.8246\n",
      "Batch 174, Loss: 1.7974\n",
      "Batch 175, Loss: 1.7673\n",
      "Batch 176, Loss: 1.8104\n",
      "Batch 177, Loss: 1.7800\n",
      "Batch 178, Loss: 1.7709\n",
      "Batch 179, Loss: 1.6851\n",
      "Batch 180, Loss: 1.7894\n",
      "Batch 181, Loss: 1.7462\n",
      "Batch 182, Loss: 1.7782\n",
      "Batch 183, Loss: 1.7453\n",
      "Batch 184, Loss: 1.8026\n",
      "Batch 185, Loss: 1.7761\n",
      "Batch 186, Loss: 1.7063\n",
      "Batch 187, Loss: 1.7008\n",
      "Batch 188, Loss: 1.7439\n",
      "Batch 189, Loss: 1.8092\n",
      "Batch 190, Loss: 1.7197\n",
      "Batch 191, Loss: 1.7662\n",
      "Batch 192, Loss: 1.7652\n",
      "Batch 193, Loss: 1.7097\n",
      "Batch 194, Loss: 1.7029\n",
      "Batch 195, Loss: 1.8145\n",
      "Batch 196, Loss: 1.7026\n",
      "Batch 197, Loss: 1.7912\n",
      "Batch 198, Loss: 1.7893\n",
      "Batch 199, Loss: 1.7967\n",
      "Batch 200, Loss: 1.8069\n",
      "Grandma, went to the park with her mommy. \"Look, Mommy made a soft knife!\" Grandma said. She smiled with delight, his hands, and Daddy bear. Grandma nodded and gave him \n",
      "a leak too. She asked, \"Can I have some too?\" Mommy smiled and sang her face. Grandma then took a deep breath. \"It's too sweet and deep your lip,\" Grandma \n",
      "said in a blanket. Grandma smiled and laughed. She had bought a few minutes around her room and the leaves carefully. Grandma really gave her a tasty breath. Grandma smiled. \n",
      "\"This is fork for her,\" mommy said, hugging her check. \"You look was so cool!\" The two put the peace in their eyes and looked around her neck. Grandma was \n",
      "so gentle and smiled at Mommy. Before long, friends shared her toys and smelled new soft leaves. Every particular led Grandma was eager to make him enough to turn off \n",
      "the tub. You look so nice and carefully until it was in its delight. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 25.14 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.7910\n",
      "Batch 202, Loss: 1.8180\n",
      "Batch 203, Loss: 1.7532\n",
      "Batch 204, Loss: 1.7091\n",
      "Batch 205, Loss: 1.7827\n",
      "Batch 206, Loss: 1.8068\n",
      "Batch 207, Loss: 1.7973\n",
      "Batch 208, Loss: 1.8443\n",
      "Batch 209, Loss: 1.7553\n",
      "Batch 210, Loss: 1.7625\n",
      "Batch 211, Loss: 1.7805\n",
      "Batch 212, Loss: 1.7869\n",
      "Batch 213, Loss: 1.8105\n",
      "Batch 214, Loss: 1.8087\n",
      "Batch 215, Loss: 1.6749\n",
      "Batch 216, Loss: 1.7886\n",
      "Batch 217, Loss: 1.7392\n",
      "Batch 218, Loss: 1.7834\n",
      "Batch 219, Loss: 1.7720\n",
      "Batch 220, Loss: 1.7827\n",
      "Batch 221, Loss: 1.7791\n",
      "Batch 222, Loss: 1.6758\n",
      "Batch 223, Loss: 1.7726\n",
      "Batch 224, Loss: 1.8091\n",
      "Batch 225, Loss: 1.7322\n",
      "Batch 226, Loss: 1.7914\n",
      "Batch 227, Loss: 1.7491\n",
      "Batch 228, Loss: 1.8115\n",
      "Batch 229, Loss: 1.7495\n",
      "Batch 230, Loss: 1.7589\n",
      "Batch 231, Loss: 1.8096\n",
      "Batch 232, Loss: 1.7924\n",
      "Batch 233, Loss: 1.7914\n",
      "Batch 234, Loss: 1.8003\n",
      "Batch 235, Loss: 1.6975\n",
      "Batch 236, Loss: 1.8041\n",
      "Batch 237, Loss: 1.7894\n",
      "Batch 238, Loss: 1.6826\n",
      "Batch 239, Loss: 1.7099\n",
      "Batch 240, Loss: 1.8058\n",
      "Batch 241, Loss: 1.7610\n",
      "Batch 242, Loss: 1.6828\n",
      "Batch 243, Loss: 1.7944\n",
      "Batch 244, Loss: 1.7160\n",
      "Batch 245, Loss: 1.7985\n",
      "Batch 246, Loss: 1.7966\n",
      "Batch 247, Loss: 1.7683\n",
      "Batch 248, Loss: 1.7420\n",
      "Batch 249, Loss: 1.7627\n",
      "Batch 250, Loss: 1.7836\n",
      "Batch 251, Loss: 1.7782\n",
      "Batch 252, Loss: 1.7093\n",
      "Batch 253, Loss: 1.7958\n",
      "Batch 254, Loss: 1.8276\n",
      "Batch 255, Loss: 1.7231\n",
      "Batch 256, Loss: 1.8008\n",
      "Batch 257, Loss: 1.7886\n",
      "Batch 258, Loss: 1.7158\n",
      "Batch 259, Loss: 1.8034\n",
      "Batch 260, Loss: 1.7637\n",
      "Batch 261, Loss: 1.7225\n",
      "Batch 262, Loss: 1.7602\n",
      "Batch 263, Loss: 1.7678\n",
      "Batch 264, Loss: 1.7353\n",
      "Batch 265, Loss: 1.7239\n",
      "Batch 266, Loss: 1.7919\n",
      "Batch 267, Loss: 1.6721\n",
      "Batch 268, Loss: 1.7617\n",
      "Batch 269, Loss: 1.7856\n",
      "Batch 270, Loss: 1.7864\n",
      "Batch 271, Loss: 1.6705\n",
      "Batch 272, Loss: 1.7353\n",
      "Batch 273, Loss: 1.7339\n",
      "Batch 274, Loss: 1.8095\n",
      "Batch 275, Loss: 1.7615\n",
      "Batch 276, Loss: 1.7992\n",
      "Batch 277, Loss: 1.7968\n",
      "Batch 278, Loss: 1.7899\n",
      "Batch 279, Loss: 1.8157\n",
      "Batch 280, Loss: 1.8346\n",
      "Batch 281, Loss: 1.7901\n",
      "Batch 282, Loss: 1.7424\n",
      "Batch 283, Loss: 1.7564\n",
      "Batch 284, Loss: 1.8089\n",
      "Batch 285, Loss: 1.6697\n",
      "Batch 286, Loss: 1.7523\n",
      "Batch 287, Loss: 1.8021\n",
      "Batch 288, Loss: 1.8100\n",
      "Batch 289, Loss: 1.7776\n",
      "Batch 290, Loss: 1.7612\n",
      "Batch 291, Loss: 1.7514\n",
      "Batch 292, Loss: 1.7963\n",
      "Batch 293, Loss: 1.7991\n",
      "Batch 294, Loss: 1.7202\n",
      "Batch 295, Loss: 1.7199\n",
      "Batch 296, Loss: 1.7803\n",
      "Batch 297, Loss: 1.7901\n",
      "Batch 298, Loss: 1.8127\n",
      "Batch 299, Loss: 1.6824\n",
      "Batch 300, Loss: 1.7238\n",
      "Batch 301, Loss: 1.7763\n",
      "Batch 302, Loss: 1.7997\n",
      "Batch 303, Loss: 1.7586\n",
      "Batch 304, Loss: 1.6802\n",
      "Batch 305, Loss: 1.7688\n",
      "Batch 306, Loss: 1.7793\n",
      "Batch 307, Loss: 1.7822\n",
      "Batch 308, Loss: 1.7730\n",
      "Batch 309, Loss: 1.7462\n",
      "Batch 310, Loss: 1.8152\n",
      "Batch 311, Loss: 1.6608\n",
      "Batch 312, Loss: 1.8027\n",
      "Batch 313, Loss: 1.7894\n",
      "Batch 314, Loss: 1.7714\n",
      "Batch 315, Loss: 1.7677\n",
      "Batch 316, Loss: 1.7746\n",
      "Batch 317, Loss: 1.7097\n",
      "Batch 318, Loss: 1.8014\n",
      "Batch 319, Loss: 1.7823\n",
      "Batch 320, Loss: 1.7726\n",
      "Batch 321, Loss: 1.6679\n",
      "Batch 322, Loss: 1.8052\n",
      "Batch 323, Loss: 1.7938\n",
      "Batch 324, Loss: 1.7767\n",
      "Batch 325, Loss: 1.7986\n",
      "Batch 326, Loss: 1.7660\n",
      "Batch 327, Loss: 1.6828\n",
      "Batch 328, Loss: 1.7296\n",
      "Batch 329, Loss: 1.7683\n",
      "Batch 330, Loss: 1.6801\n",
      "Batch 331, Loss: 1.7701\n",
      "Batch 332, Loss: 1.6799\n",
      "Batch 333, Loss: 1.7770\n",
      "Batch 334, Loss: 1.7495\n",
      "Batch 335, Loss: 1.6886\n",
      "Batch 336, Loss: 1.7828\n",
      "Batch 337, Loss: 1.7596\n",
      "Batch 338, Loss: 1.7775\n",
      "Batch 339, Loss: 1.6521\n",
      "Batch 340, Loss: 1.7304\n",
      "Batch 341, Loss: 1.7243\n",
      "Batch 342, Loss: 1.8048\n",
      "Batch 343, Loss: 1.7697\n",
      "Batch 344, Loss: 1.7287\n",
      "Batch 345, Loss: 1.7988\n",
      "Batch 346, Loss: 1.8338\n",
      "Batch 347, Loss: 1.7258\n",
      "Batch 348, Loss: 1.8350\n",
      "Batch 349, Loss: 1.7604\n",
      "Batch 350, Loss: 1.7728\n",
      "Batch 351, Loss: 1.7129\n",
      "Batch 352, Loss: 1.8367\n",
      "Batch 353, Loss: 1.7374\n",
      "Batch 354, Loss: 1.8160\n",
      "Batch 355, Loss: 1.7072\n",
      "Batch 356, Loss: 1.7796\n",
      "Batch 357, Loss: 1.8245\n",
      "Batch 358, Loss: 1.8091\n",
      "Batch 359, Loss: 1.8013\n",
      "Batch 360, Loss: 1.7430\n",
      "Batch 361, Loss: 1.7598\n",
      "Batch 362, Loss: 1.8259\n",
      "Batch 363, Loss: 1.8066\n",
      "Batch 364, Loss: 1.8277\n",
      "Batch 365, Loss: 1.8250\n",
      "Batch 366, Loss: 1.7397\n",
      "Batch 367, Loss: 1.7573\n",
      "Batch 368, Loss: 1.8284\n",
      "Batch 369, Loss: 1.7777\n",
      "Batch 370, Loss: 1.7329\n",
      "Batch 371, Loss: 1.7840\n",
      "Batch 372, Loss: 1.7567\n",
      "Batch 373, Loss: 1.8119\n",
      "Batch 374, Loss: 1.6899\n",
      "Batch 375, Loss: 1.7914\n",
      "Batch 376, Loss: 1.7444\n",
      "Batch 377, Loss: 1.7593\n",
      "Batch 378, Loss: 1.8244\n",
      "Epoch 15, Average Loss: 1.7660\n",
      "Tom and Mia love hoesy underck in the park. They have more valuables every day. They use a big carpet and a fluffy star. The new star is many. The \n",
      "jetter think of the jet is it everywhere. One day, Tom and Mia go to the park and Tom's mom. They are sad and angry. They do not know the \n",
      "new expensions and Ben's moo for being mean. They find a safe and win the wine. They wonder what they are and sad. But Tom is lost and angry. He \n",
      "is strong and damper and followed them together. He cannot find what is wrong. He only barked and says, \"Why do you fold? That's passing?\". Mia came over and says, \n",
      "\"No, it is mine!\" Tom feel ashamed. He has a book and fold a hile. He says, \"No, I didn't not. This is ugly and cover very much. You must \n",
      "know what is dady. You should not choose without!\" Mia's mission will not ask before it do not flying again. She is better and happy. The next day, they see \n",
      "a boy couple. The boy is bigger and bigger and smaller and mo open. His lie is free and bigger made the boy happy. Tom and Mia hug each other. \n",
      "They smile and say, \"Why is that?\" and surprise. They talk and laugh. They are sad and angry. They say, \"We are sorry, dog! We wanted to make a game. \n",
      "We don't touch the scold!\" But then, they hear a loud noise. It is a storm coming. The boy has a big heart and bigger and bigger and smaller. He \n",
      "says, \"Thank you, small sea is sir. You are the best friend ever!\" He was very scared and happy. He says, \"You're welcome, cat.\" They hug and leave the park. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.7759\n",
      "Batch 2, Loss: 3.1979\n",
      "Batch 3, Loss: 3.8311\n",
      "Batch 4, Loss: 3.2375\n",
      "Batch 5, Loss: 3.0331\n",
      "Batch 6, Loss: 3.0054\n",
      "Batch 7, Loss: 2.7634\n",
      "Batch 8, Loss: 2.4342\n",
      "Batch 9, Loss: 2.7296\n",
      "Batch 10, Loss: 2.5491\n",
      "Batch 11, Loss: 2.5512\n",
      "Batch 12, Loss: 2.4391\n",
      "Batch 13, Loss: 2.5083\n",
      "Batch 14, Loss: 2.3821\n",
      "Batch 15, Loss: 2.3745\n",
      "Batch 16, Loss: 2.3410\n",
      "Batch 17, Loss: 2.3456\n",
      "Batch 18, Loss: 2.2020\n",
      "Batch 19, Loss: 2.2403\n",
      "Batch 20, Loss: 2.2103\n",
      "Batch 21, Loss: 2.1540\n",
      "Batch 22, Loss: 2.0158\n",
      "Batch 23, Loss: 2.1133\n",
      "Batch 24, Loss: 2.0930\n",
      "Batch 25, Loss: 2.0556\n",
      "Batch 26, Loss: 2.0677\n",
      "Batch 27, Loss: 2.0488\n",
      "Batch 28, Loss: 1.9441\n",
      "Batch 29, Loss: 1.9631\n",
      "Batch 30, Loss: 2.0240\n",
      "Batch 31, Loss: 1.9813\n",
      "Batch 32, Loss: 1.8972\n",
      "Batch 33, Loss: 1.9590\n",
      "Batch 34, Loss: 1.9676\n",
      "Batch 35, Loss: 1.8959\n",
      "Batch 36, Loss: 1.8417\n",
      "Batch 37, Loss: 1.9608\n",
      "Batch 38, Loss: 1.9259\n",
      "Batch 39, Loss: 1.8934\n",
      "Batch 40, Loss: 1.9132\n",
      "Batch 41, Loss: 1.9042\n",
      "Batch 42, Loss: 1.8632\n",
      "Batch 43, Loss: 1.9289\n",
      "Batch 44, Loss: 1.8686\n",
      "Batch 45, Loss: 1.8575\n",
      "Batch 46, Loss: 1.8500\n",
      "Batch 47, Loss: 1.8997\n",
      "Batch 48, Loss: 1.8685\n",
      "Batch 49, Loss: 1.8258\n",
      "Batch 50, Loss: 1.8192\n",
      "Batch 51, Loss: 1.8599\n",
      "Batch 52, Loss: 1.8475\n",
      "Batch 53, Loss: 1.8508\n",
      "Batch 54, Loss: 1.7642\n",
      "Batch 55, Loss: 1.8354\n",
      "Batch 56, Loss: 1.7739\n",
      "Batch 57, Loss: 1.8677\n",
      "Batch 58, Loss: 1.8319\n",
      "Batch 59, Loss: 1.8485\n",
      "Batch 60, Loss: 1.8665\n",
      "Batch 61, Loss: 1.8062\n",
      "Batch 62, Loss: 1.7221\n",
      "Batch 63, Loss: 1.7238\n",
      "Batch 64, Loss: 1.8529\n",
      "Batch 65, Loss: 1.7684\n",
      "Batch 66, Loss: 1.8189\n",
      "Batch 67, Loss: 1.6954\n",
      "Batch 68, Loss: 1.8405\n",
      "Batch 69, Loss: 1.8265\n",
      "Batch 70, Loss: 1.7510\n",
      "Batch 71, Loss: 1.7396\n",
      "Batch 72, Loss: 1.8160\n",
      "Batch 73, Loss: 1.8315\n",
      "Batch 74, Loss: 1.8574\n",
      "Batch 75, Loss: 1.7886\n",
      "Batch 76, Loss: 1.7762\n",
      "Batch 77, Loss: 1.8112\n",
      "Batch 78, Loss: 1.7176\n",
      "Batch 79, Loss: 1.7974\n",
      "Batch 80, Loss: 1.8223\n",
      "Batch 81, Loss: 1.7890\n",
      "Batch 82, Loss: 1.7687\n",
      "Batch 83, Loss: 1.8313\n",
      "Batch 84, Loss: 1.8334\n",
      "Batch 85, Loss: 1.7091\n",
      "Batch 86, Loss: 1.7919\n",
      "Batch 87, Loss: 1.7509\n",
      "Batch 88, Loss: 1.7205\n",
      "Batch 89, Loss: 1.7939\n",
      "Batch 90, Loss: 1.8396\n",
      "Batch 91, Loss: 1.8350\n",
      "Batch 92, Loss: 1.7103\n",
      "Batch 93, Loss: 1.8016\n",
      "Batch 94, Loss: 1.7867\n",
      "Batch 95, Loss: 1.8176\n",
      "Batch 96, Loss: 1.7401\n",
      "Batch 97, Loss: 1.6844\n",
      "Batch 98, Loss: 1.7353\n",
      "Batch 99, Loss: 1.7881\n",
      "Batch 100, Loss: 1.7947\n",
      "Batch 101, Loss: 1.7754\n",
      "Batch 102, Loss: 1.7258\n",
      "Batch 103, Loss: 1.7722\n",
      "Batch 104, Loss: 1.6684\n",
      "Batch 105, Loss: 1.7863\n",
      "Batch 106, Loss: 1.7797\n",
      "Batch 107, Loss: 1.7897\n",
      "Batch 108, Loss: 1.8217\n",
      "Batch 109, Loss: 1.7232\n",
      "Batch 110, Loss: 1.7183\n",
      "Batch 111, Loss: 1.8143\n",
      "Batch 112, Loss: 1.7844\n",
      "Batch 113, Loss: 1.8085\n",
      "Batch 114, Loss: 1.7346\n",
      "Batch 115, Loss: 1.7914\n",
      "Batch 116, Loss: 1.7199\n",
      "Batch 117, Loss: 1.6890\n",
      "Batch 118, Loss: 1.7896\n",
      "Batch 119, Loss: 1.6730\n",
      "Batch 120, Loss: 1.6693\n",
      "Batch 121, Loss: 1.8137\n",
      "Batch 122, Loss: 1.7794\n",
      "Batch 123, Loss: 1.8215\n",
      "Batch 124, Loss: 1.8093\n",
      "Batch 125, Loss: 1.7414\n",
      "Batch 126, Loss: 1.7975\n",
      "Batch 127, Loss: 1.7431\n",
      "Batch 128, Loss: 1.7455\n",
      "Batch 129, Loss: 1.7896\n",
      "Batch 130, Loss: 1.8081\n",
      "Batch 131, Loss: 1.7751\n",
      "Batch 132, Loss: 1.7055\n",
      "Batch 133, Loss: 1.6719\n",
      "Batch 134, Loss: 1.6431\n",
      "Batch 135, Loss: 1.7612\n",
      "Batch 136, Loss: 1.6795\n",
      "Batch 137, Loss: 1.7552\n",
      "Batch 138, Loss: 1.7913\n",
      "Batch 139, Loss: 1.7504\n",
      "Batch 140, Loss: 1.6531\n",
      "Batch 141, Loss: 1.7488\n",
      "Batch 142, Loss: 1.7753\n",
      "Batch 143, Loss: 1.6804\n",
      "Batch 144, Loss: 1.7908\n",
      "Batch 145, Loss: 1.7585\n",
      "Batch 146, Loss: 1.7777\n",
      "Batch 147, Loss: 1.7881\n",
      "Batch 148, Loss: 1.7363\n",
      "Batch 149, Loss: 1.8081\n",
      "Batch 150, Loss: 1.8024\n",
      "Batch 151, Loss: 1.7391\n",
      "Batch 152, Loss: 1.7447\n",
      "Batch 153, Loss: 1.7734\n",
      "Batch 154, Loss: 1.7521\n",
      "Batch 155, Loss: 1.7679\n",
      "Batch 156, Loss: 1.7659\n",
      "Batch 157, Loss: 1.7231\n",
      "Batch 158, Loss: 1.7937\n",
      "Batch 159, Loss: 1.6959\n",
      "Batch 160, Loss: 1.7720\n",
      "Batch 161, Loss: 1.7866\n",
      "Batch 162, Loss: 1.7621\n",
      "Batch 163, Loss: 1.6959\n",
      "Batch 164, Loss: 1.7554\n",
      "Batch 165, Loss: 1.7468\n",
      "Batch 166, Loss: 1.7828\n",
      "Batch 167, Loss: 1.7626\n",
      "Batch 168, Loss: 1.7229\n",
      "Batch 169, Loss: 1.7863\n",
      "Batch 170, Loss: 1.7643\n",
      "Batch 171, Loss: 1.7658\n",
      "Batch 172, Loss: 1.7893\n",
      "Batch 173, Loss: 1.6819\n",
      "Batch 174, Loss: 1.7211\n",
      "Batch 175, Loss: 1.6818\n",
      "Batch 176, Loss: 1.7060\n",
      "Batch 177, Loss: 1.7734\n",
      "Batch 178, Loss: 1.6910\n",
      "Batch 179, Loss: 1.7390\n",
      "Batch 180, Loss: 1.7483\n",
      "Batch 181, Loss: 1.7738\n",
      "Batch 182, Loss: 1.6749\n",
      "Batch 183, Loss: 1.7611\n",
      "Batch 184, Loss: 1.7803\n",
      "Batch 185, Loss: 1.6631\n",
      "Batch 186, Loss: 1.6845\n",
      "Batch 187, Loss: 1.7647\n",
      "Batch 188, Loss: 1.6360\n",
      "Batch 189, Loss: 1.7886\n",
      "Batch 190, Loss: 1.7993\n",
      "Batch 191, Loss: 1.8209\n",
      "Batch 192, Loss: 1.7244\n",
      "Batch 193, Loss: 1.7076\n",
      "Batch 194, Loss: 1.7479\n",
      "Batch 195, Loss: 1.7665\n",
      "Batch 196, Loss: 1.7771\n",
      "Batch 197, Loss: 1.7164\n",
      "Batch 198, Loss: 1.7447\n",
      "Batch 199, Loss: 1.6634\n",
      "Batch 200, Loss: 1.7068\n",
      "Once upon a time there was a famous bird. She had a blue bird, every day but always loved. One day, she wanted to find himself up. She found a \n",
      "bird singing in her tree. She was so excited to see the bird. The bird said, â€œI want to be the fast bird.â€ The bird said â€œYou cannot catch on \n",
      "my blue bird.â€ The bird was scared. She didn't want to go away from him. But then she remembered her friends and hugged her friends. They were clever and they \n",
      "knew they had to stop and be kind to his friends. The mother gave her a big hug and they all went to the same tree to save her. Finally, \n",
      "the greed bird fell into the tree, but she stayed still there for a while, her wings had the same. The bird was very sad and he began to cry. \n",
      "The mother hugged her and said sorry and wanted it for permisse. The bird flew high in the tree and saw the most wonderful thing he had ever seen. It \n",
      "was a sign of her butterfly! She was very happy. She felt proud and beyoved that some day has hugged her friend. The girl who just enjoyed her more and \n",
      "experien this loud noise. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 28.34 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.7717\n",
      "Batch 202, Loss: 1.7238\n",
      "Batch 203, Loss: 1.7052\n",
      "Batch 204, Loss: 1.7823\n",
      "Batch 205, Loss: 1.6790\n",
      "Batch 206, Loss: 1.6654\n",
      "Batch 207, Loss: 1.7992\n",
      "Batch 208, Loss: 1.6894\n",
      "Batch 209, Loss: 1.7939\n",
      "Batch 210, Loss: 1.7165\n",
      "Batch 211, Loss: 1.6689\n",
      "Batch 212, Loss: 1.7934\n",
      "Batch 213, Loss: 1.8001\n",
      "Batch 214, Loss: 1.7826\n",
      "Batch 215, Loss: 1.6957\n",
      "Batch 216, Loss: 1.8049\n",
      "Batch 217, Loss: 1.7892\n",
      "Batch 218, Loss: 1.7918\n",
      "Batch 219, Loss: 1.7674\n",
      "Batch 220, Loss: 1.7893\n",
      "Batch 221, Loss: 1.6765\n",
      "Batch 222, Loss: 1.7547\n",
      "Batch 223, Loss: 1.7687\n",
      "Batch 224, Loss: 1.7795\n",
      "Batch 225, Loss: 1.7560\n",
      "Batch 226, Loss: 1.6582\n",
      "Batch 227, Loss: 1.6838\n",
      "Batch 228, Loss: 1.7983\n",
      "Batch 229, Loss: 1.7899\n",
      "Batch 230, Loss: 1.7507\n",
      "Batch 231, Loss: 1.7766\n",
      "Batch 232, Loss: 1.8002\n",
      "Batch 233, Loss: 1.7356\n",
      "Batch 234, Loss: 1.7759\n",
      "Batch 235, Loss: 1.6998\n",
      "Batch 236, Loss: 1.7236\n",
      "Batch 237, Loss: 1.7851\n",
      "Batch 238, Loss: 1.7924\n",
      "Batch 239, Loss: 1.7712\n",
      "Batch 240, Loss: 1.7766\n",
      "Batch 241, Loss: 1.7431\n",
      "Batch 242, Loss: 1.7510\n",
      "Batch 243, Loss: 1.7420\n",
      "Batch 244, Loss: 1.7522\n",
      "Batch 245, Loss: 1.7808\n",
      "Batch 246, Loss: 1.6664\n",
      "Batch 247, Loss: 1.7604\n",
      "Batch 248, Loss: 1.7617\n",
      "Batch 249, Loss: 1.7066\n",
      "Batch 250, Loss: 1.7122\n",
      "Batch 251, Loss: 1.6969\n",
      "Batch 252, Loss: 1.7639\n",
      "Batch 253, Loss: 1.7662\n",
      "Batch 254, Loss: 1.8037\n",
      "Batch 255, Loss: 1.7654\n",
      "Batch 256, Loss: 1.7501\n",
      "Batch 257, Loss: 1.7377\n",
      "Batch 258, Loss: 1.7926\n",
      "Batch 259, Loss: 1.6936\n",
      "Batch 260, Loss: 1.7536\n",
      "Batch 261, Loss: 1.7742\n",
      "Batch 262, Loss: 1.6601\n",
      "Batch 263, Loss: 1.7762\n",
      "Batch 264, Loss: 1.7550\n",
      "Batch 265, Loss: 1.7684\n",
      "Batch 266, Loss: 1.6634\n",
      "Batch 267, Loss: 1.6573\n",
      "Batch 268, Loss: 1.7721\n",
      "Batch 269, Loss: 1.8069\n",
      "Batch 270, Loss: 1.7419\n",
      "Batch 271, Loss: 1.7606\n",
      "Batch 272, Loss: 1.7642\n",
      "Batch 273, Loss: 1.7346\n",
      "Batch 274, Loss: 1.7671\n",
      "Batch 275, Loss: 1.7510\n",
      "Batch 276, Loss: 1.7467\n",
      "Batch 277, Loss: 1.7743\n",
      "Batch 278, Loss: 1.7524\n",
      "Batch 279, Loss: 1.7717\n",
      "Batch 280, Loss: 1.7637\n",
      "Batch 281, Loss: 1.7331\n",
      "Batch 282, Loss: 1.7383\n",
      "Batch 283, Loss: 1.6784\n",
      "Batch 284, Loss: 1.7789\n",
      "Batch 285, Loss: 1.6824\n",
      "Batch 286, Loss: 1.6809\n",
      "Batch 287, Loss: 1.7698\n",
      "Batch 288, Loss: 1.7386\n",
      "Batch 289, Loss: 1.7717\n",
      "Batch 290, Loss: 1.7240\n",
      "Batch 291, Loss: 1.7847\n",
      "Batch 292, Loss: 1.7677\n",
      "Batch 293, Loss: 1.6455\n",
      "Batch 294, Loss: 1.7727\n",
      "Batch 295, Loss: 1.8075\n",
      "Batch 296, Loss: 1.6907\n",
      "Batch 297, Loss: 1.7658\n",
      "Batch 298, Loss: 1.7890\n",
      "Batch 299, Loss: 1.6686\n",
      "Batch 300, Loss: 1.7282\n",
      "Batch 301, Loss: 1.6997\n",
      "Batch 302, Loss: 1.7497\n",
      "Batch 303, Loss: 1.7547\n",
      "Batch 304, Loss: 1.6985\n",
      "Batch 305, Loss: 1.7635\n",
      "Batch 306, Loss: 1.7401\n",
      "Batch 307, Loss: 1.7580\n",
      "Batch 308, Loss: 1.8181\n",
      "Batch 309, Loss: 1.7379\n",
      "Batch 310, Loss: 1.7447\n",
      "Batch 311, Loss: 1.7533\n",
      "Batch 312, Loss: 1.7120\n",
      "Batch 313, Loss: 1.7280\n",
      "Batch 314, Loss: 1.6979\n",
      "Batch 315, Loss: 1.7922\n",
      "Batch 316, Loss: 1.6922\n",
      "Batch 317, Loss: 1.7325\n",
      "Batch 318, Loss: 1.7813\n",
      "Batch 319, Loss: 1.7151\n",
      "Batch 320, Loss: 1.6210\n",
      "Batch 321, Loss: 1.7077\n",
      "Batch 322, Loss: 1.7620\n",
      "Batch 323, Loss: 1.7563\n",
      "Batch 324, Loss: 1.7468\n",
      "Batch 325, Loss: 1.8004\n",
      "Batch 326, Loss: 1.7218\n",
      "Batch 327, Loss: 1.7399\n",
      "Batch 328, Loss: 1.6886\n",
      "Batch 329, Loss: 1.7274\n",
      "Batch 330, Loss: 1.7576\n",
      "Batch 331, Loss: 1.6773\n",
      "Batch 332, Loss: 1.7335\n",
      "Batch 333, Loss: 1.7728\n",
      "Batch 334, Loss: 1.7422\n",
      "Batch 335, Loss: 1.7887\n",
      "Batch 336, Loss: 1.6574\n",
      "Batch 337, Loss: 1.8040\n",
      "Batch 338, Loss: 1.7395\n",
      "Batch 339, Loss: 1.7723\n",
      "Batch 340, Loss: 1.7315\n",
      "Batch 341, Loss: 1.7785\n",
      "Batch 342, Loss: 1.7602\n",
      "Batch 343, Loss: 1.7800\n",
      "Batch 344, Loss: 1.6879\n",
      "Batch 345, Loss: 1.7370\n",
      "Batch 346, Loss: 1.7768\n",
      "Batch 347, Loss: 1.7548\n",
      "Batch 348, Loss: 1.7773\n",
      "Batch 349, Loss: 1.7624\n",
      "Batch 350, Loss: 1.6490\n",
      "Batch 351, Loss: 1.7105\n",
      "Batch 352, Loss: 1.7570\n",
      "Batch 353, Loss: 1.7787\n",
      "Batch 354, Loss: 1.7589\n",
      "Batch 355, Loss: 1.7611\n",
      "Batch 356, Loss: 1.7105\n",
      "Batch 357, Loss: 1.6528\n",
      "Batch 358, Loss: 1.6650\n",
      "Batch 359, Loss: 1.7808\n",
      "Batch 360, Loss: 1.7526\n",
      "Batch 361, Loss: 1.7783\n",
      "Batch 362, Loss: 1.7237\n",
      "Batch 363, Loss: 1.7546\n",
      "Batch 364, Loss: 1.7525\n",
      "Batch 365, Loss: 1.6890\n",
      "Batch 366, Loss: 1.7558\n",
      "Batch 367, Loss: 1.6524\n",
      "Batch 368, Loss: 1.7506\n",
      "Batch 369, Loss: 1.7364\n",
      "Batch 370, Loss: 1.7619\n",
      "Batch 371, Loss: 1.6769\n",
      "Batch 372, Loss: 1.6635\n",
      "Batch 373, Loss: 1.7458\n",
      "Batch 374, Loss: 1.7785\n",
      "Batch 375, Loss: 1.7400\n",
      "Batch 376, Loss: 1.7508\n",
      "Batch 377, Loss: 1.7200\n",
      "Batch 378, Loss: 1.7987\n",
      "Epoch 16, Average Loss: 1.8127\n",
      "Once upon a time, there was a pirate who slept to each other. She always shared a ball in her dress with the other sidewal. One day, and the pirate \n",
      "went on an adventure with all the other side of the birds on the floor. They saw the bear filled with gas in front of her name, and they were \n",
      "walking in their way. So, they decided to gather a secret nectar from the man. As they were walking, the pirate started to sparkle and loved to join in. But \n",
      "then, a voice said, \"no thank you. You can have everything if you are lying to your friends\". The pirate wink had forgotten to be happy and hold the nectar. \n",
      "The wink was fresh for a big roar and fly her further. The man felt sorry for the pirate and said, \"bye! I want to play with you!\" So he \n",
      "went with the other side. The wink had a party with his friends. The win pirate walled away at them more. The pirate smiled and said, \"Thank you!\" The two \n",
      "good pirate friends jumped into the park. They lived happily ever after. They played and laughed all day long. At the end of the wink, they said goodbye as they \n",
      "enjoyed the walk around the pirate shelter. They all shared the wonderful thing together. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.6259\n",
      "Batch 2, Loss: 1.6591\n",
      "Batch 3, Loss: 1.7363\n",
      "Batch 4, Loss: 1.7101\n",
      "Batch 5, Loss: 1.6982\n",
      "Batch 6, Loss: 1.7844\n",
      "Batch 7, Loss: 1.7051\n",
      "Batch 8, Loss: 1.7596\n",
      "Batch 9, Loss: 1.7254\n",
      "Batch 10, Loss: 1.7327\n",
      "Batch 11, Loss: 1.7172\n",
      "Batch 12, Loss: 1.6388\n",
      "Batch 13, Loss: 1.7456\n",
      "Batch 14, Loss: 1.7427\n",
      "Batch 15, Loss: 1.7780\n",
      "Batch 16, Loss: 1.7154\n",
      "Batch 17, Loss: 1.6396\n",
      "Batch 18, Loss: 1.7449\n",
      "Batch 19, Loss: 1.6532\n",
      "Batch 20, Loss: 1.7340\n",
      "Batch 21, Loss: 1.6134\n",
      "Batch 22, Loss: 1.7658\n",
      "Batch 23, Loss: 1.6980\n",
      "Batch 24, Loss: 1.6938\n",
      "Batch 25, Loss: 1.7569\n",
      "Batch 26, Loss: 1.6995\n",
      "Batch 27, Loss: 1.7079\n",
      "Batch 28, Loss: 1.6912\n",
      "Batch 29, Loss: 1.7107\n",
      "Batch 30, Loss: 1.6152\n",
      "Batch 31, Loss: 1.6912\n",
      "Batch 32, Loss: 1.7482\n",
      "Batch 33, Loss: 1.6508\n",
      "Batch 34, Loss: 1.7822\n",
      "Batch 35, Loss: 1.7159\n",
      "Batch 36, Loss: 1.7410\n",
      "Batch 37, Loss: 1.7372\n",
      "Batch 38, Loss: 1.7661\n",
      "Batch 39, Loss: 1.7346\n",
      "Batch 40, Loss: 1.6200\n",
      "Batch 41, Loss: 1.7379\n",
      "Batch 42, Loss: 1.7475\n",
      "Batch 43, Loss: 1.7401\n",
      "Batch 44, Loss: 1.6526\n",
      "Batch 45, Loss: 1.6894\n",
      "Batch 46, Loss: 1.7519\n",
      "Batch 47, Loss: 1.7547\n",
      "Batch 48, Loss: 1.6246\n",
      "Batch 49, Loss: 1.6858\n",
      "Batch 50, Loss: 1.7470\n",
      "Batch 51, Loss: 1.7295\n",
      "Batch 52, Loss: 1.6239\n",
      "Batch 53, Loss: 1.7221\n",
      "Batch 54, Loss: 1.7474\n",
      "Batch 55, Loss: 1.7399\n",
      "Batch 56, Loss: 1.6765\n",
      "Batch 57, Loss: 1.7313\n",
      "Batch 58, Loss: 1.6790\n",
      "Batch 59, Loss: 1.7751\n",
      "Batch 60, Loss: 1.7164\n",
      "Batch 61, Loss: 1.7883\n",
      "Batch 62, Loss: 1.6170\n",
      "Batch 63, Loss: 1.6166\n",
      "Batch 64, Loss: 1.7101\n",
      "Batch 65, Loss: 1.7333\n",
      "Batch 66, Loss: 1.6603\n",
      "Batch 67, Loss: 1.7775\n",
      "Batch 68, Loss: 1.7523\n",
      "Batch 69, Loss: 1.7262\n",
      "Batch 70, Loss: 1.7355\n",
      "Batch 71, Loss: 1.7746\n",
      "Batch 72, Loss: 1.7754\n",
      "Batch 73, Loss: 1.7718\n",
      "Batch 74, Loss: 1.7147\n",
      "Batch 75, Loss: 1.6786\n",
      "Batch 76, Loss: 1.6984\n",
      "Batch 77, Loss: 1.7494\n",
      "Batch 78, Loss: 1.7678\n",
      "Batch 79, Loss: 1.7313\n",
      "Batch 80, Loss: 1.6992\n",
      "Batch 81, Loss: 1.7380\n",
      "Batch 82, Loss: 1.6941\n",
      "Batch 83, Loss: 1.7084\n",
      "Batch 84, Loss: 1.7567\n",
      "Batch 85, Loss: 1.6381\n",
      "Batch 86, Loss: 1.7818\n",
      "Batch 87, Loss: 1.7428\n",
      "Batch 88, Loss: 1.7885\n",
      "Batch 89, Loss: 1.7090\n",
      "Batch 90, Loss: 1.7621\n",
      "Batch 91, Loss: 1.7380\n",
      "Batch 92, Loss: 1.6297\n",
      "Batch 93, Loss: 1.6819\n",
      "Batch 94, Loss: 1.7371\n",
      "Batch 95, Loss: 1.6694\n",
      "Batch 96, Loss: 1.7206\n",
      "Batch 97, Loss: 1.7480\n",
      "Batch 98, Loss: 1.6995\n",
      "Batch 99, Loss: 1.6380\n",
      "Batch 100, Loss: 1.6806\n",
      "Batch 101, Loss: 1.7598\n",
      "Batch 102, Loss: 1.7406\n",
      "Batch 103, Loss: 1.6441\n",
      "Batch 104, Loss: 1.7343\n",
      "Batch 105, Loss: 1.7172\n",
      "Batch 106, Loss: 1.7431\n",
      "Batch 107, Loss: 1.7103\n",
      "Batch 108, Loss: 1.7412\n",
      "Batch 109, Loss: 1.7761\n",
      "Batch 110, Loss: 1.7002\n",
      "Batch 111, Loss: 1.6914\n",
      "Batch 112, Loss: 1.6903\n",
      "Batch 113, Loss: 1.6307\n",
      "Batch 114, Loss: 1.7483\n",
      "Batch 115, Loss: 1.7708\n",
      "Batch 116, Loss: 1.7883\n",
      "Batch 117, Loss: 1.7610\n",
      "Batch 118, Loss: 1.7333\n",
      "Batch 119, Loss: 1.6521\n",
      "Batch 120, Loss: 1.7834\n",
      "Batch 121, Loss: 1.7034\n",
      "Batch 122, Loss: 1.7495\n",
      "Batch 123, Loss: 1.6324\n",
      "Batch 124, Loss: 1.7261\n",
      "Batch 125, Loss: 1.7366\n",
      "Batch 126, Loss: 1.7129\n",
      "Batch 127, Loss: 1.7430\n",
      "Batch 128, Loss: 1.7703\n",
      "Batch 129, Loss: 1.7788\n",
      "Batch 130, Loss: 1.7256\n",
      "Batch 131, Loss: 1.7532\n",
      "Batch 132, Loss: 1.7536\n",
      "Batch 133, Loss: 1.7395\n",
      "Batch 134, Loss: 1.7363\n",
      "Batch 135, Loss: 1.7255\n",
      "Batch 136, Loss: 1.7213\n",
      "Batch 137, Loss: 1.7481\n",
      "Batch 138, Loss: 1.7306\n",
      "Batch 139, Loss: 1.7313\n",
      "Batch 140, Loss: 1.6508\n",
      "Batch 141, Loss: 1.7119\n",
      "Batch 142, Loss: 1.7398\n",
      "Batch 143, Loss: 1.7665\n",
      "Batch 144, Loss: 1.7144\n",
      "Batch 145, Loss: 1.7066\n",
      "Batch 146, Loss: 1.6685\n",
      "Batch 147, Loss: 1.7523\n",
      "Batch 148, Loss: 1.7208\n",
      "Batch 149, Loss: 1.7252\n",
      "Batch 150, Loss: 1.7188\n",
      "Batch 151, Loss: 1.7558\n",
      "Batch 152, Loss: 1.7352\n",
      "Batch 153, Loss: 1.6742\n",
      "Batch 154, Loss: 1.7708\n",
      "Batch 155, Loss: 1.7213\n",
      "Batch 156, Loss: 1.7543\n",
      "Batch 157, Loss: 1.7384\n",
      "Batch 158, Loss: 1.6742\n",
      "Batch 159, Loss: 1.7607\n",
      "Batch 160, Loss: 1.7526\n",
      "Batch 161, Loss: 1.7625\n",
      "Batch 162, Loss: 1.6584\n",
      "Batch 163, Loss: 1.7506\n",
      "Batch 164, Loss: 1.7231\n",
      "Batch 165, Loss: 1.6066\n",
      "Batch 166, Loss: 1.7279\n",
      "Batch 167, Loss: 1.7755\n",
      "Batch 168, Loss: 1.6564\n",
      "Batch 169, Loss: 1.7568\n",
      "Batch 170, Loss: 1.7629\n",
      "Batch 171, Loss: 1.7406\n",
      "Batch 172, Loss: 1.7267\n",
      "Batch 173, Loss: 1.7108\n",
      "Batch 174, Loss: 1.7273\n",
      "Batch 175, Loss: 1.7400\n",
      "Batch 176, Loss: 1.7434\n",
      "Batch 177, Loss: 1.7519\n",
      "Batch 178, Loss: 1.7494\n",
      "Batch 179, Loss: 1.7505\n",
      "Batch 180, Loss: 1.7607\n",
      "Batch 181, Loss: 1.7205\n",
      "Batch 182, Loss: 1.7245\n",
      "Batch 183, Loss: 1.7178\n",
      "Batch 184, Loss: 1.7289\n",
      "Batch 185, Loss: 1.7629\n",
      "Batch 186, Loss: 1.7243\n",
      "Batch 187, Loss: 1.7387\n",
      "Batch 188, Loss: 1.7423\n",
      "Batch 189, Loss: 1.7250\n",
      "Batch 190, Loss: 1.6576\n",
      "Batch 191, Loss: 1.7549\n",
      "Batch 192, Loss: 1.6611\n",
      "Batch 193, Loss: 1.6991\n",
      "Batch 194, Loss: 1.6886\n",
      "Batch 195, Loss: 1.6135\n",
      "Batch 196, Loss: 1.7219\n",
      "Batch 197, Loss: 1.7937\n",
      "Batch 198, Loss: 1.6266\n",
      "Batch 199, Loss: 1.7186\n",
      "Batch 200, Loss: 1.7546\n",
      "Once upon a time, there were two friends, Sam and Abigail. Sam and Abigail were two small and colourful friends playing with spring jewels in a field. Chroa smiled and \n",
      "watched as Abigail heard Sam's mom's pointy noises. \"Look, Abigail!\" said Abigail as he could, holding his hand in glee. Mom said, \"We'll play with it, but we make you \n",
      "love Mom's!\". Sam said, \"Yes, I am two big!\" They started laughing and having fun. Abigail was soon faster, but Sam and Abigail went very quickly. They spoke until the \n",
      "day finally came out, and Abigail liked Sam and Abigail felt like louber. She carried Abigail for to continue over her field, curious and laughing-cmolouss. Abigailot's eyes turned around and \n",
      "cheered with delight! She had a special lesson that she helped him and wanted to wash the jewel. She had found a special jewelry that made this right decision! \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 22.13 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.7536\n",
      "Batch 202, Loss: 1.7538\n",
      "Batch 203, Loss: 1.6983\n",
      "Batch 204, Loss: 1.7311\n",
      "Batch 205, Loss: 1.6860\n",
      "Batch 206, Loss: 1.7182\n",
      "Batch 207, Loss: 1.7122\n",
      "Batch 208, Loss: 1.7605\n",
      "Batch 209, Loss: 1.7345\n",
      "Batch 210, Loss: 1.7494\n",
      "Batch 211, Loss: 1.6947\n",
      "Batch 212, Loss: 1.7784\n",
      "Batch 213, Loss: 1.7156\n",
      "Batch 214, Loss: 1.6677\n",
      "Batch 215, Loss: 1.6666\n",
      "Batch 216, Loss: 1.6992\n",
      "Batch 217, Loss: 1.6581\n",
      "Batch 218, Loss: 1.7440\n",
      "Batch 219, Loss: 1.7369\n",
      "Batch 220, Loss: 1.6360\n",
      "Batch 221, Loss: 1.7703\n",
      "Batch 222, Loss: 1.7628\n",
      "Batch 223, Loss: 1.7918\n",
      "Batch 224, Loss: 1.7271\n",
      "Batch 225, Loss: 1.7328\n",
      "Batch 226, Loss: 1.7302\n",
      "Batch 227, Loss: 1.7093\n",
      "Batch 228, Loss: 1.6619\n",
      "Batch 229, Loss: 1.7472\n",
      "Batch 230, Loss: 1.7483\n",
      "Batch 231, Loss: 1.7132\n",
      "Batch 232, Loss: 1.6931\n",
      "Batch 233, Loss: 1.6726\n",
      "Batch 234, Loss: 1.6660\n",
      "Batch 235, Loss: 1.6992\n",
      "Batch 236, Loss: 1.6235\n",
      "Batch 237, Loss: 1.7004\n",
      "Batch 238, Loss: 1.7499\n",
      "Batch 239, Loss: 1.7162\n",
      "Batch 240, Loss: 1.7818\n",
      "Batch 241, Loss: 1.7226\n",
      "Batch 242, Loss: 1.7119\n",
      "Batch 243, Loss: 1.7462\n",
      "Batch 244, Loss: 1.7567\n",
      "Batch 245, Loss: 1.6340\n",
      "Batch 246, Loss: 1.6847\n",
      "Batch 247, Loss: 1.7539\n",
      "Batch 248, Loss: 1.6526\n",
      "Batch 249, Loss: 1.7470\n",
      "Batch 250, Loss: 1.7314\n",
      "Batch 251, Loss: 1.7527\n",
      "Batch 252, Loss: 1.7364\n",
      "Batch 253, Loss: 1.7273\n",
      "Batch 254, Loss: 1.7014\n",
      "Batch 255, Loss: 1.7622\n",
      "Batch 256, Loss: 1.7205\n",
      "Batch 257, Loss: 1.6952\n",
      "Batch 258, Loss: 1.7368\n",
      "Batch 259, Loss: 1.7791\n",
      "Batch 260, Loss: 1.7371\n",
      "Batch 261, Loss: 1.7011\n",
      "Batch 262, Loss: 1.7160\n",
      "Batch 263, Loss: 1.7282\n",
      "Batch 264, Loss: 1.7269\n",
      "Batch 265, Loss: 1.7047\n",
      "Batch 266, Loss: 1.7300\n",
      "Batch 267, Loss: 1.6338\n",
      "Batch 268, Loss: 1.7510\n",
      "Batch 269, Loss: 1.6903\n",
      "Batch 270, Loss: 1.6372\n",
      "Batch 271, Loss: 1.7354\n",
      "Batch 272, Loss: 1.6627\n",
      "Batch 273, Loss: 1.6351\n",
      "Batch 274, Loss: 1.6828\n",
      "Batch 275, Loss: 1.7430\n",
      "Batch 276, Loss: 1.7744\n",
      "Batch 277, Loss: 1.6927\n",
      "Batch 278, Loss: 1.7365\n",
      "Batch 279, Loss: 1.7324\n",
      "Batch 280, Loss: 1.7344\n",
      "Batch 281, Loss: 1.7325\n",
      "Batch 282, Loss: 1.7364\n",
      "Batch 283, Loss: 1.6587\n",
      "Batch 284, Loss: 1.3815\n",
      "Batch 285, Loss: 1.7522\n",
      "Batch 286, Loss: 1.7971\n",
      "Batch 287, Loss: 1.7860\n",
      "Batch 288, Loss: 1.7021\n",
      "Batch 289, Loss: 1.7571\n",
      "Batch 290, Loss: 1.6954\n",
      "Batch 291, Loss: 1.7269\n",
      "Batch 292, Loss: 1.7215\n",
      "Batch 293, Loss: 1.7828\n",
      "Batch 294, Loss: 1.7626\n",
      "Batch 295, Loss: 1.6693\n",
      "Batch 296, Loss: 1.7995\n",
      "Batch 297, Loss: 1.6926\n",
      "Batch 298, Loss: 1.6778\n",
      "Batch 299, Loss: 1.7659\n",
      "Batch 300, Loss: 1.6646\n",
      "Batch 301, Loss: 1.6694\n",
      "Batch 302, Loss: 1.6908\n",
      "Batch 303, Loss: 1.7606\n",
      "Batch 304, Loss: 1.7656\n",
      "Batch 305, Loss: 1.7709\n",
      "Batch 306, Loss: 1.6843\n",
      "Batch 307, Loss: 1.6728\n",
      "Batch 308, Loss: 1.7403\n",
      "Batch 309, Loss: 1.7672\n",
      "Batch 310, Loss: 1.6586\n",
      "Batch 311, Loss: 1.6754\n",
      "Batch 312, Loss: 1.6727\n",
      "Batch 313, Loss: 1.7529\n",
      "Batch 314, Loss: 1.7107\n",
      "Batch 315, Loss: 1.7570\n",
      "Batch 316, Loss: 1.7775\n",
      "Batch 317, Loss: 1.7433\n",
      "Batch 318, Loss: 1.7711\n",
      "Batch 319, Loss: 1.7678\n",
      "Batch 320, Loss: 1.6641\n",
      "Batch 321, Loss: 1.7257\n",
      "Batch 322, Loss: 1.6860\n",
      "Batch 323, Loss: 1.7011\n",
      "Batch 324, Loss: 1.7156\n",
      "Batch 325, Loss: 1.6725\n",
      "Batch 326, Loss: 1.6723\n",
      "Batch 327, Loss: 1.7773\n",
      "Batch 328, Loss: 1.6394\n",
      "Batch 329, Loss: 1.6482\n",
      "Batch 330, Loss: 1.7684\n",
      "Batch 331, Loss: 1.7531\n",
      "Batch 332, Loss: 1.6472\n",
      "Batch 333, Loss: 1.7515\n",
      "Batch 334, Loss: 1.7129\n",
      "Batch 335, Loss: 1.7359\n",
      "Batch 336, Loss: 1.7604\n",
      "Batch 337, Loss: 1.6440\n",
      "Batch 338, Loss: 1.7121\n",
      "Batch 339, Loss: 1.7559\n",
      "Batch 340, Loss: 1.6945\n",
      "Batch 341, Loss: 1.6840\n",
      "Batch 342, Loss: 1.6700\n",
      "Batch 343, Loss: 1.7472\n",
      "Batch 344, Loss: 1.7553\n",
      "Batch 345, Loss: 1.6530\n",
      "Batch 346, Loss: 1.7526\n",
      "Batch 347, Loss: 1.7382\n",
      "Batch 348, Loss: 1.7559\n",
      "Batch 349, Loss: 1.7424\n",
      "Batch 350, Loss: 1.7523\n",
      "Batch 351, Loss: 1.7537\n",
      "Batch 352, Loss: 1.7204\n",
      "Batch 353, Loss: 1.6653\n",
      "Batch 354, Loss: 1.6871\n",
      "Batch 355, Loss: 1.7401\n",
      "Batch 356, Loss: 1.7767\n",
      "Batch 357, Loss: 1.7413\n",
      "Batch 358, Loss: 1.6533\n",
      "Batch 359, Loss: 1.6839\n",
      "Batch 360, Loss: 1.6235\n",
      "Batch 361, Loss: 1.6157\n",
      "Batch 362, Loss: 1.7699\n",
      "Batch 363, Loss: 1.6825\n",
      "Batch 364, Loss: 1.7623\n",
      "Batch 365, Loss: 1.7622\n",
      "Batch 366, Loss: 1.7233\n",
      "Batch 367, Loss: 1.7361\n",
      "Batch 368, Loss: 1.6633\n",
      "Batch 369, Loss: 1.6621\n",
      "Batch 370, Loss: 1.7357\n",
      "Batch 371, Loss: 1.7698\n",
      "Batch 372, Loss: 1.7177\n",
      "Batch 373, Loss: 1.7302\n",
      "Batch 374, Loss: 1.6736\n",
      "Batch 375, Loss: 1.6844\n",
      "Batch 376, Loss: 1.6833\n",
      "Batch 377, Loss: 1.7367\n",
      "Batch 378, Loss: 1.7135\n",
      "Epoch 17, Average Loss: 1.7176\n",
      "Once upon a time, there was a mommy and a daddy. On the momdy was a good boy, on something burning on the stove. Daddy was very excited to see \n",
      "what the nice thing had been there. After a while, the mommy noticed something on the stove. She was unknown! There was a big dictionary soil! Next, the mommy laughed \n",
      "and said, \"Mommy, can I try it?\" But the mommy had an idea. She grabbed a toy ribbon around the dictionary. Mommy and daddy watched as it moved around with \n",
      "smiles. Together they had a wonderful time together, the water evening. They were content and loved the ribbons. The mommy gave the larger until it was filled with happiness. Now \n",
      "it worked even harder times and it made their mom smile. The mommy and daddy talked as the mommy exploring in the last time. They said it had a wonderful \n",
      "secret express and prefache control. The friends were feeling happy and proud of their pictures. And so they knew they would ownhe coreshite a delicious powers. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.5978\n",
      "Batch 2, Loss: 1.7193\n",
      "Batch 3, Loss: 1.6345\n",
      "Batch 4, Loss: 1.7472\n",
      "Batch 5, Loss: 1.7059\n",
      "Batch 6, Loss: 1.6325\n",
      "Batch 7, Loss: 1.7445\n",
      "Batch 8, Loss: 1.7315\n",
      "Batch 9, Loss: 1.6269\n",
      "Batch 10, Loss: 1.7100\n",
      "Batch 11, Loss: 1.5971\n",
      "Batch 12, Loss: 1.5941\n",
      "Batch 13, Loss: 1.6266\n",
      "Batch 14, Loss: 1.7056\n",
      "Batch 15, Loss: 1.6816\n",
      "Batch 16, Loss: 1.7109\n",
      "Batch 17, Loss: 1.6419\n",
      "Batch 18, Loss: 1.7163\n",
      "Batch 19, Loss: 1.7536\n",
      "Batch 20, Loss: 1.6378\n",
      "Batch 21, Loss: 1.7598\n",
      "Batch 22, Loss: 1.6495\n",
      "Batch 23, Loss: 1.7582\n",
      "Batch 24, Loss: 1.6811\n",
      "Batch 25, Loss: 1.7332\n",
      "Batch 26, Loss: 1.7220\n",
      "Batch 27, Loss: 1.7037\n",
      "Batch 28, Loss: 1.6103\n",
      "Batch 29, Loss: 1.7141\n",
      "Batch 30, Loss: 1.6824\n",
      "Batch 31, Loss: 1.7174\n",
      "Batch 32, Loss: 1.6891\n",
      "Batch 33, Loss: 1.5987\n",
      "Batch 34, Loss: 1.6744\n",
      "Batch 35, Loss: 1.6877\n",
      "Batch 36, Loss: 1.7231\n",
      "Batch 37, Loss: 1.6453\n",
      "Batch 38, Loss: 1.6841\n",
      "Batch 39, Loss: 1.7306\n",
      "Batch 40, Loss: 1.7249\n",
      "Batch 41, Loss: 1.7279\n",
      "Batch 42, Loss: 1.7422\n",
      "Batch 43, Loss: 1.7187\n",
      "Batch 44, Loss: 1.7301\n",
      "Batch 45, Loss: 1.7189\n",
      "Batch 46, Loss: 1.7426\n",
      "Batch 47, Loss: 1.6638\n",
      "Batch 48, Loss: 1.6014\n",
      "Batch 49, Loss: 1.6938\n",
      "Batch 50, Loss: 1.6948\n",
      "Batch 51, Loss: 1.6896\n",
      "Batch 52, Loss: 1.7370\n",
      "Batch 53, Loss: 1.7117\n",
      "Batch 54, Loss: 1.7335\n",
      "Batch 55, Loss: 1.6101\n",
      "Batch 56, Loss: 1.6432\n",
      "Batch 57, Loss: 1.6469\n",
      "Batch 58, Loss: 1.7600\n",
      "Batch 59, Loss: 1.5929\n",
      "Batch 60, Loss: 1.7033\n",
      "Batch 61, Loss: 1.6019\n",
      "Batch 62, Loss: 1.7206\n",
      "Batch 63, Loss: 1.7249\n",
      "Batch 64, Loss: 1.6285\n",
      "Batch 65, Loss: 1.7060\n",
      "Batch 66, Loss: 1.7517\n",
      "Batch 67, Loss: 1.6744\n",
      "Batch 68, Loss: 1.7303\n",
      "Batch 69, Loss: 1.6398\n",
      "Batch 70, Loss: 1.7149\n",
      "Batch 71, Loss: 1.7056\n",
      "Batch 72, Loss: 1.7420\n",
      "Batch 73, Loss: 1.7028\n",
      "Batch 74, Loss: 1.7058\n",
      "Batch 75, Loss: 1.7383\n",
      "Batch 76, Loss: 1.7024\n",
      "Batch 77, Loss: 1.6283\n",
      "Batch 78, Loss: 1.6378\n",
      "Batch 79, Loss: 1.7214\n",
      "Batch 80, Loss: 1.7303\n",
      "Batch 81, Loss: 1.7297\n",
      "Batch 82, Loss: 1.7322\n",
      "Batch 83, Loss: 1.6234\n",
      "Batch 84, Loss: 1.7237\n",
      "Batch 85, Loss: 1.6196\n",
      "Batch 86, Loss: 1.7012\n",
      "Batch 87, Loss: 1.7151\n",
      "Batch 88, Loss: 1.7205\n",
      "Batch 89, Loss: 1.7352\n",
      "Batch 90, Loss: 1.6983\n",
      "Batch 91, Loss: 1.7118\n",
      "Batch 92, Loss: 1.7126\n",
      "Batch 93, Loss: 1.6454\n",
      "Batch 94, Loss: 1.7247\n",
      "Batch 95, Loss: 1.7191\n",
      "Batch 96, Loss: 1.7194\n",
      "Batch 97, Loss: 1.7578\n",
      "Batch 98, Loss: 1.7075\n",
      "Batch 99, Loss: 1.6832\n",
      "Batch 100, Loss: 1.7115\n",
      "Batch 101, Loss: 1.6169\n",
      "Batch 102, Loss: 1.7469\n",
      "Batch 103, Loss: 1.7035\n",
      "Batch 104, Loss: 1.7123\n",
      "Batch 105, Loss: 1.6846\n",
      "Batch 106, Loss: 1.6884\n",
      "Batch 107, Loss: 1.7390\n",
      "Batch 108, Loss: 1.6422\n",
      "Batch 109, Loss: 1.6717\n",
      "Batch 110, Loss: 1.6703\n",
      "Batch 111, Loss: 1.6994\n",
      "Batch 112, Loss: 1.5955\n",
      "Batch 113, Loss: 1.7738\n",
      "Batch 114, Loss: 1.6679\n",
      "Batch 115, Loss: 1.6941\n",
      "Batch 116, Loss: 1.7101\n",
      "Batch 117, Loss: 1.6334\n",
      "Batch 118, Loss: 1.5988\n",
      "Batch 119, Loss: 1.6466\n",
      "Batch 120, Loss: 1.7285\n",
      "Batch 121, Loss: 1.6933\n",
      "Batch 122, Loss: 1.6968\n",
      "Batch 123, Loss: 1.6857\n",
      "Batch 124, Loss: 1.6703\n",
      "Batch 125, Loss: 1.6835\n",
      "Batch 126, Loss: 1.7240\n",
      "Batch 127, Loss: 1.7230\n",
      "Batch 128, Loss: 1.7564\n",
      "Batch 129, Loss: 1.6573\n",
      "Batch 130, Loss: 1.7201\n",
      "Batch 131, Loss: 1.7202\n",
      "Batch 132, Loss: 1.6312\n",
      "Batch 133, Loss: 1.7186\n",
      "Batch 134, Loss: 1.6918\n",
      "Batch 135, Loss: 1.5991\n",
      "Batch 136, Loss: 1.7587\n",
      "Batch 137, Loss: 1.6299\n",
      "Batch 138, Loss: 1.7568\n",
      "Batch 139, Loss: 1.7007\n",
      "Batch 140, Loss: 1.7458\n",
      "Batch 141, Loss: 1.6960\n",
      "Batch 142, Loss: 1.7118\n",
      "Batch 143, Loss: 1.6941\n",
      "Batch 144, Loss: 1.7273\n",
      "Batch 145, Loss: 1.7010\n",
      "Batch 146, Loss: 1.6556\n",
      "Batch 147, Loss: 1.6774\n",
      "Batch 148, Loss: 1.7234\n",
      "Batch 149, Loss: 1.7274\n",
      "Batch 150, Loss: 1.7231\n",
      "Batch 151, Loss: 1.7357\n",
      "Batch 152, Loss: 1.6046\n",
      "Batch 153, Loss: 1.7191\n",
      "Batch 154, Loss: 1.7040\n",
      "Batch 155, Loss: 1.7158\n",
      "Batch 156, Loss: 1.7555\n",
      "Batch 157, Loss: 1.7110\n",
      "Batch 158, Loss: 1.6404\n",
      "Batch 159, Loss: 1.7598\n",
      "Batch 160, Loss: 1.7039\n",
      "Batch 161, Loss: 1.6892\n",
      "Batch 162, Loss: 1.7095\n",
      "Batch 163, Loss: 1.7091\n",
      "Batch 164, Loss: 1.7050\n",
      "Batch 165, Loss: 1.7200\n",
      "Batch 166, Loss: 1.7062\n",
      "Batch 167, Loss: 1.7001\n",
      "Batch 168, Loss: 1.7132\n",
      "Batch 169, Loss: 1.7239\n",
      "Batch 170, Loss: 1.6405\n",
      "Batch 171, Loss: 1.7162\n",
      "Batch 172, Loss: 1.7422\n",
      "Batch 173, Loss: 1.7283\n",
      "Batch 174, Loss: 1.7233\n",
      "Batch 175, Loss: 1.6700\n",
      "Batch 176, Loss: 1.7059\n",
      "Batch 177, Loss: 1.7130\n",
      "Batch 178, Loss: 1.6281\n",
      "Batch 179, Loss: 1.6328\n",
      "Batch 180, Loss: 1.7111\n",
      "Batch 181, Loss: 1.6983\n",
      "Batch 182, Loss: 1.7689\n",
      "Batch 183, Loss: 1.7485\n",
      "Batch 184, Loss: 1.6384\n",
      "Batch 185, Loss: 1.7426\n",
      "Batch 186, Loss: 1.6584\n",
      "Batch 187, Loss: 1.7399\n",
      "Batch 188, Loss: 1.7124\n",
      "Batch 189, Loss: 1.7133\n",
      "Batch 190, Loss: 1.6053\n",
      "Batch 191, Loss: 1.6742\n",
      "Batch 192, Loss: 1.7191\n",
      "Batch 193, Loss: 1.6899\n",
      "Batch 194, Loss: 1.7136\n",
      "Batch 195, Loss: 1.6309\n",
      "Batch 196, Loss: 1.7351\n",
      "Batch 197, Loss: 1.7379\n",
      "Batch 198, Loss: 1.7484\n",
      "Batch 199, Loss: 1.7458\n",
      "Batch 200, Loss: 1.7547\n",
      "Once, there was a little girl who went to the market. She saw a pink bone down the road and asked her dad if she was observed the little girl \n",
      "without asking her. \"Ah, please!\" an old man said. \"Would you like smarter how smarter?\" \"Yes, please!\" hes held the cop, and headed it. Suddenly the old man saw some \n",
      "wood appeared from the marker. The little girl took her Daddyâ€™s daddy's backpack and socks. She came over and asked them what the spot had. daddy said that the little \n",
      "girl wanted to be person and watch the man swing away. The little girl was disappointed and asked her Daddy if he wanted to see the man. Daddy said, \"Little \n",
      "puppy, can you surprise the man and please get a fake coin from the man and part of paper.\" The little girl was so excited, she grabbed the coin from \n",
      "the manâ€™s friend's mean and said, \"Done it's obsed to decide to finish the speed!\" Daddy was so happy and grateful and thanked them for their wonderful moment. From that \n",
      "day on, she knew that even if the things you changed down and kind, even if you want to do something wrong. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 24.19 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.6655\n",
      "Batch 202, Loss: 1.7207\n",
      "Batch 203, Loss: 1.6953\n",
      "Batch 204, Loss: 1.7114\n",
      "Batch 205, Loss: 1.6474\n",
      "Batch 206, Loss: 1.7267\n",
      "Batch 207, Loss: 1.7150\n",
      "Batch 208, Loss: 1.6906\n",
      "Batch 209, Loss: 1.6602\n",
      "Batch 210, Loss: 1.7013\n",
      "Batch 211, Loss: 1.6473\n",
      "Batch 212, Loss: 1.7259\n",
      "Batch 213, Loss: 1.7431\n",
      "Batch 214, Loss: 1.7378\n",
      "Batch 215, Loss: 1.7212\n",
      "Batch 216, Loss: 1.7210\n",
      "Batch 217, Loss: 1.7014\n",
      "Batch 218, Loss: 1.7576\n",
      "Batch 219, Loss: 1.7725\n",
      "Batch 220, Loss: 1.7437\n",
      "Batch 221, Loss: 1.6144\n",
      "Batch 222, Loss: 1.7336\n",
      "Batch 223, Loss: 1.7318\n",
      "Batch 224, Loss: 1.6484\n",
      "Batch 225, Loss: 1.6542\n",
      "Batch 226, Loss: 1.6542\n",
      "Batch 227, Loss: 1.6704\n",
      "Batch 228, Loss: 1.6986\n",
      "Batch 229, Loss: 1.7216\n",
      "Batch 230, Loss: 1.7353\n",
      "Batch 231, Loss: 1.6774\n",
      "Batch 232, Loss: 1.7307\n",
      "Batch 233, Loss: 1.6596\n",
      "Batch 234, Loss: 1.6465\n",
      "Batch 235, Loss: 1.7324\n",
      "Batch 236, Loss: 1.7165\n",
      "Batch 237, Loss: 1.6584\n",
      "Batch 238, Loss: 1.6409\n",
      "Batch 239, Loss: 1.7544\n",
      "Batch 240, Loss: 1.6737\n",
      "Batch 241, Loss: 1.7382\n",
      "Batch 242, Loss: 1.7155\n",
      "Batch 243, Loss: 1.6904\n",
      "Batch 244, Loss: 1.6947\n",
      "Batch 245, Loss: 1.7645\n",
      "Batch 246, Loss: 1.7517\n",
      "Batch 247, Loss: 1.6765\n",
      "Batch 248, Loss: 1.7496\n",
      "Batch 249, Loss: 1.7303\n",
      "Batch 250, Loss: 1.7418\n",
      "Batch 251, Loss: 1.5947\n",
      "Batch 252, Loss: 1.6913\n",
      "Batch 253, Loss: 1.7482\n",
      "Batch 254, Loss: 1.7646\n",
      "Batch 255, Loss: 1.7076\n",
      "Batch 256, Loss: 1.7066\n",
      "Batch 257, Loss: 1.7383\n",
      "Batch 258, Loss: 1.6607\n",
      "Batch 259, Loss: 1.6698\n",
      "Batch 260, Loss: 1.7390\n",
      "Batch 261, Loss: 1.7340\n",
      "Batch 262, Loss: 1.7203\n",
      "Batch 263, Loss: 1.6598\n",
      "Batch 264, Loss: 1.7344\n",
      "Batch 265, Loss: 1.6519\n",
      "Batch 266, Loss: 1.7267\n",
      "Batch 267, Loss: 1.6386\n",
      "Batch 268, Loss: 1.6723\n",
      "Batch 269, Loss: 1.6355\n",
      "Batch 270, Loss: 1.7002\n",
      "Batch 271, Loss: 1.6599\n",
      "Batch 272, Loss: 1.6916\n",
      "Batch 273, Loss: 1.7342\n",
      "Batch 274, Loss: 1.6352\n",
      "Batch 275, Loss: 1.6428\n",
      "Batch 276, Loss: 1.7423\n",
      "Batch 277, Loss: 1.7517\n",
      "Batch 278, Loss: 1.6267\n",
      "Batch 279, Loss: 1.7226\n",
      "Batch 280, Loss: 1.7062\n",
      "Batch 281, Loss: 1.7376\n",
      "Batch 282, Loss: 1.7232\n",
      "Batch 283, Loss: 1.7390\n",
      "Batch 284, Loss: 1.7662\n",
      "Batch 285, Loss: 1.6720\n",
      "Batch 286, Loss: 1.7596\n",
      "Batch 287, Loss: 1.7066\n",
      "Batch 288, Loss: 1.6856\n",
      "Batch 289, Loss: 1.6772\n",
      "Batch 290, Loss: 1.7159\n",
      "Batch 291, Loss: 1.7096\n",
      "Batch 292, Loss: 1.7103\n",
      "Batch 293, Loss: 1.6412\n",
      "Batch 294, Loss: 1.6858\n",
      "Batch 295, Loss: 1.7230\n",
      "Batch 296, Loss: 1.7133\n",
      "Batch 297, Loss: 1.7259\n",
      "Batch 298, Loss: 1.6960\n",
      "Batch 299, Loss: 1.7112\n",
      "Batch 300, Loss: 1.7324\n",
      "Batch 301, Loss: 1.7230\n",
      "Batch 302, Loss: 1.3187\n",
      "Batch 303, Loss: 1.7070\n",
      "Batch 304, Loss: 1.6406\n",
      "Batch 305, Loss: 1.7556\n",
      "Batch 306, Loss: 1.7891\n",
      "Batch 307, Loss: 1.7046\n",
      "Batch 308, Loss: 1.6589\n",
      "Batch 309, Loss: 1.7255\n",
      "Batch 310, Loss: 1.7779\n",
      "Batch 311, Loss: 1.7331\n",
      "Batch 312, Loss: 1.6372\n",
      "Batch 313, Loss: 1.6519\n",
      "Batch 314, Loss: 1.7436\n",
      "Batch 315, Loss: 1.7523\n",
      "Batch 316, Loss: 1.7149\n",
      "Batch 317, Loss: 1.7375\n",
      "Batch 318, Loss: 1.6251\n",
      "Batch 319, Loss: 1.6986\n",
      "Batch 320, Loss: 1.7745\n",
      "Batch 321, Loss: 1.7463\n",
      "Batch 322, Loss: 1.6804\n",
      "Batch 323, Loss: 1.6652\n",
      "Batch 324, Loss: 1.6204\n",
      "Batch 325, Loss: 1.6823\n",
      "Batch 326, Loss: 1.6904\n",
      "Batch 327, Loss: 1.7252\n",
      "Batch 328, Loss: 1.6623\n",
      "Batch 329, Loss: 1.6437\n",
      "Batch 330, Loss: 1.7221\n",
      "Batch 331, Loss: 1.7418\n",
      "Batch 332, Loss: 1.7208\n",
      "Batch 333, Loss: 1.7238\n",
      "Batch 334, Loss: 1.6118\n",
      "Batch 335, Loss: 1.6362\n",
      "Batch 336, Loss: 1.7025\n",
      "Batch 337, Loss: 1.6242\n",
      "Batch 338, Loss: 1.7509\n",
      "Batch 339, Loss: 1.7010\n",
      "Batch 340, Loss: 1.7243\n",
      "Batch 341, Loss: 1.6188\n",
      "Batch 342, Loss: 1.7485\n",
      "Batch 343, Loss: 1.6408\n",
      "Batch 344, Loss: 1.7383\n",
      "Batch 345, Loss: 1.7674\n",
      "Batch 346, Loss: 1.7682\n",
      "Batch 347, Loss: 1.7367\n",
      "Batch 348, Loss: 1.6773\n",
      "Batch 349, Loss: 1.6737\n",
      "Batch 350, Loss: 1.7484\n",
      "Batch 351, Loss: 1.7104\n",
      "Batch 352, Loss: 1.7428\n",
      "Batch 353, Loss: 1.7359\n",
      "Batch 354, Loss: 1.7347\n",
      "Batch 355, Loss: 1.6762\n",
      "Batch 356, Loss: 1.7011\n",
      "Batch 357, Loss: 1.6254\n",
      "Batch 358, Loss: 1.6587\n",
      "Batch 359, Loss: 1.6743\n",
      "Batch 360, Loss: 1.6248\n",
      "Batch 361, Loss: 1.7164\n",
      "Batch 362, Loss: 1.7405\n",
      "Batch 363, Loss: 1.6954\n",
      "Batch 364, Loss: 1.6881\n",
      "Batch 365, Loss: 1.7350\n",
      "Batch 366, Loss: 1.7324\n",
      "Batch 367, Loss: 1.7053\n",
      "Batch 368, Loss: 1.7230\n",
      "Batch 369, Loss: 1.6615\n",
      "Batch 370, Loss: 1.6363\n",
      "Batch 371, Loss: 1.6194\n",
      "Batch 372, Loss: 1.7371\n",
      "Batch 373, Loss: 1.7644\n",
      "Batch 374, Loss: 1.6324\n",
      "Batch 375, Loss: 1.7719\n",
      "Batch 376, Loss: 1.7302\n",
      "Batch 377, Loss: 1.7265\n",
      "Batch 378, Loss: 1.7484\n",
      "Epoch 18, Average Loss: 1.6978\n",
      "Once upon a time there was a mommy and a daddy and a son. They were every special, but mommy was worried and didn't know why. The mommy said it \n",
      "was time for a minute and the little girl said that one day or she had to paint a green lamp. So the little girl and her family went to \n",
      "the minute path with their hands and asked a smile. She said that she must quit making the little lamp with some tickets, but then they soon became friends over \n",
      "all their miles! Over time, there was a small sun in the sky and the little girl tried a few tears. What a green sunse in the sky in the \n",
      "sky at the end, and the little girl wasn't alone. As they walked, the mommy was gone. It was so beautiful and beautiful! The little girl had a sigh of \n",
      "beautiful lamp, but she knew she had to dome it. The little girl was so happy. She felt full and safe and happy that Lily had given her the lamp. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.6786\n",
      "Batch 2, Loss: 1.7126\n",
      "Batch 3, Loss: 1.6986\n",
      "Batch 4, Loss: 1.7228\n",
      "Batch 5, Loss: 1.7122\n",
      "Batch 6, Loss: 1.6704\n",
      "Batch 7, Loss: 1.7122\n",
      "Batch 8, Loss: 1.7160\n",
      "Batch 9, Loss: 1.7324\n",
      "Batch 10, Loss: 1.6808\n",
      "Batch 11, Loss: 1.6352\n",
      "Batch 12, Loss: 1.6969\n",
      "Batch 13, Loss: 1.6483\n",
      "Batch 14, Loss: 1.7218\n",
      "Batch 15, Loss: 1.7238\n",
      "Batch 16, Loss: 1.6783\n",
      "Batch 17, Loss: 1.7118\n",
      "Batch 18, Loss: 1.6092\n",
      "Batch 19, Loss: 1.6758\n",
      "Batch 20, Loss: 1.6419\n",
      "Batch 21, Loss: 1.6394\n",
      "Batch 22, Loss: 1.6829\n",
      "Batch 23, Loss: 1.6063\n",
      "Batch 24, Loss: 1.6494\n",
      "Batch 25, Loss: 1.6989\n",
      "Batch 26, Loss: 1.6258\n",
      "Batch 27, Loss: 1.6882\n",
      "Batch 28, Loss: 1.7067\n",
      "Batch 29, Loss: 1.7155\n",
      "Batch 30, Loss: 1.7098\n",
      "Batch 31, Loss: 1.7210\n",
      "Batch 32, Loss: 1.6993\n",
      "Batch 33, Loss: 1.5886\n",
      "Batch 34, Loss: 1.7011\n",
      "Batch 35, Loss: 1.6182\n",
      "Batch 36, Loss: 1.6765\n",
      "Batch 37, Loss: 1.7528\n",
      "Batch 38, Loss: 1.6727\n",
      "Batch 39, Loss: 1.6837\n",
      "Batch 40, Loss: 1.5955\n",
      "Batch 41, Loss: 1.6751\n",
      "Batch 42, Loss: 1.6028\n",
      "Batch 43, Loss: 1.6886\n",
      "Batch 44, Loss: 1.6781\n",
      "Batch 45, Loss: 1.7125\n",
      "Batch 46, Loss: 1.7014\n",
      "Batch 47, Loss: 1.5938\n",
      "Batch 48, Loss: 1.7312\n",
      "Batch 49, Loss: 1.7030\n",
      "Batch 50, Loss: 1.6382\n",
      "Batch 51, Loss: 1.6197\n",
      "Batch 52, Loss: 1.6931\n",
      "Batch 53, Loss: 1.6677\n",
      "Batch 54, Loss: 1.5919\n",
      "Batch 55, Loss: 1.6598\n",
      "Batch 56, Loss: 1.6120\n",
      "Batch 57, Loss: 1.6245\n",
      "Batch 58, Loss: 1.6882\n",
      "Batch 59, Loss: 1.6295\n",
      "Batch 60, Loss: 1.7494\n",
      "Batch 61, Loss: 1.6812\n",
      "Batch 62, Loss: 1.5748\n",
      "Batch 63, Loss: 1.6854\n",
      "Batch 64, Loss: 1.7351\n",
      "Batch 65, Loss: 1.6857\n",
      "Batch 66, Loss: 1.5639\n",
      "Batch 67, Loss: 1.7194\n",
      "Batch 68, Loss: 1.7434\n",
      "Batch 69, Loss: 1.7577\n",
      "Batch 70, Loss: 1.6229\n",
      "Batch 71, Loss: 1.6523\n",
      "Batch 72, Loss: 1.7260\n",
      "Batch 73, Loss: 1.5746\n",
      "Batch 74, Loss: 1.6861\n",
      "Batch 75, Loss: 1.7000\n",
      "Batch 76, Loss: 1.7261\n",
      "Batch 77, Loss: 1.6955\n",
      "Batch 78, Loss: 1.7055\n",
      "Batch 79, Loss: 1.7028\n",
      "Batch 80, Loss: 1.7070\n",
      "Batch 81, Loss: 1.7119\n",
      "Batch 82, Loss: 1.6224\n",
      "Batch 83, Loss: 1.7017\n",
      "Batch 84, Loss: 1.7187\n",
      "Batch 85, Loss: 1.7328\n",
      "Batch 86, Loss: 1.6996\n",
      "Batch 87, Loss: 1.6311\n",
      "Batch 88, Loss: 1.6132\n",
      "Batch 89, Loss: 1.6711\n",
      "Batch 90, Loss: 1.7073\n",
      "Batch 91, Loss: 1.6688\n",
      "Batch 92, Loss: 1.7356\n",
      "Batch 93, Loss: 1.5654\n",
      "Batch 94, Loss: 1.7152\n",
      "Batch 95, Loss: 1.6545\n",
      "Batch 96, Loss: 1.6870\n",
      "Batch 97, Loss: 1.6310\n",
      "Batch 98, Loss: 1.7325\n",
      "Batch 99, Loss: 1.6690\n",
      "Batch 100, Loss: 1.6701\n",
      "Batch 101, Loss: 1.6684\n",
      "Batch 102, Loss: 1.6121\n",
      "Batch 103, Loss: 1.6478\n",
      "Batch 104, Loss: 1.5943\n",
      "Batch 105, Loss: 1.6949\n",
      "Batch 106, Loss: 1.7158\n",
      "Batch 107, Loss: 1.6835\n",
      "Batch 108, Loss: 1.6833\n",
      "Batch 109, Loss: 1.6856\n",
      "Batch 110, Loss: 1.7303\n",
      "Batch 111, Loss: 1.6970\n",
      "Batch 112, Loss: 1.6062\n",
      "Batch 113, Loss: 1.6052\n",
      "Batch 114, Loss: 1.7067\n",
      "Batch 115, Loss: 1.6885\n",
      "Batch 116, Loss: 1.6071\n",
      "Batch 117, Loss: 1.7244\n",
      "Batch 118, Loss: 1.6267\n",
      "Batch 119, Loss: 1.7144\n",
      "Batch 120, Loss: 1.7189\n",
      "Batch 121, Loss: 1.5738\n",
      "Batch 122, Loss: 1.6852\n",
      "Batch 123, Loss: 1.7057\n",
      "Batch 124, Loss: 1.7026\n",
      "Batch 125, Loss: 1.6559\n",
      "Batch 126, Loss: 1.7243\n",
      "Batch 127, Loss: 1.6679\n",
      "Batch 128, Loss: 1.6565\n",
      "Batch 129, Loss: 1.7034\n",
      "Batch 130, Loss: 1.7133\n",
      "Batch 131, Loss: 1.7012\n",
      "Batch 132, Loss: 1.6013\n",
      "Batch 133, Loss: 1.6891\n",
      "Batch 134, Loss: 1.6655\n",
      "Batch 135, Loss: 1.7418\n",
      "Batch 136, Loss: 1.7060\n",
      "Batch 137, Loss: 1.6317\n",
      "Batch 138, Loss: 1.6036\n",
      "Batch 139, Loss: 1.7170\n",
      "Batch 140, Loss: 1.6824\n",
      "Batch 141, Loss: 1.6898\n",
      "Batch 142, Loss: 1.6779\n",
      "Batch 143, Loss: 1.6305\n",
      "Batch 144, Loss: 1.7066\n",
      "Batch 145, Loss: 1.6888\n",
      "Batch 146, Loss: 1.7240\n",
      "Batch 147, Loss: 1.7090\n",
      "Batch 148, Loss: 1.6739\n",
      "Batch 149, Loss: 1.7141\n",
      "Batch 150, Loss: 1.6757\n",
      "Batch 151, Loss: 1.6907\n",
      "Batch 152, Loss: 1.6905\n",
      "Batch 153, Loss: 1.7162\n",
      "Batch 154, Loss: 1.7159\n",
      "Batch 155, Loss: 1.6307\n",
      "Batch 156, Loss: 1.6637\n",
      "Batch 157, Loss: 1.6257\n",
      "Batch 158, Loss: 1.6981\n",
      "Batch 159, Loss: 1.6253\n",
      "Batch 160, Loss: 1.5933\n",
      "Batch 161, Loss: 1.6346\n",
      "Batch 162, Loss: 1.7157\n",
      "Batch 163, Loss: 1.6266\n",
      "Batch 164, Loss: 1.6382\n",
      "Batch 165, Loss: 1.6058\n",
      "Batch 166, Loss: 1.6176\n",
      "Batch 167, Loss: 1.7076\n",
      "Batch 168, Loss: 1.6681\n",
      "Batch 169, Loss: 1.6804\n",
      "Batch 170, Loss: 1.6954\n",
      "Batch 171, Loss: 1.6992\n",
      "Batch 172, Loss: 1.6333\n",
      "Batch 173, Loss: 1.7152\n",
      "Batch 174, Loss: 1.6982\n",
      "Batch 175, Loss: 1.6678\n",
      "Batch 176, Loss: 1.6980\n",
      "Batch 177, Loss: 1.6423\n",
      "Batch 178, Loss: 1.6900\n",
      "Batch 179, Loss: 1.5849\n",
      "Batch 180, Loss: 1.7085\n",
      "Batch 181, Loss: 1.6563\n",
      "Batch 182, Loss: 1.6038\n",
      "Batch 183, Loss: 1.6709\n",
      "Batch 184, Loss: 1.7222\n",
      "Batch 185, Loss: 1.7289\n",
      "Batch 186, Loss: 1.7011\n",
      "Batch 187, Loss: 1.6834\n",
      "Batch 188, Loss: 1.5985\n",
      "Batch 189, Loss: 1.7337\n",
      "Batch 190, Loss: 1.7018\n",
      "Batch 191, Loss: 1.6533\n",
      "Batch 192, Loss: 1.6539\n",
      "Batch 193, Loss: 1.6784\n",
      "Batch 194, Loss: 1.6418\n",
      "Batch 195, Loss: 1.7130\n",
      "Batch 196, Loss: 1.7146\n",
      "Batch 197, Loss: 1.7212\n",
      "Batch 198, Loss: 1.7129\n",
      "Batch 199, Loss: 1.6938\n",
      "Batch 200, Loss: 1.7217\n",
      "Once upon a time, there was a little girl named Lily. Lily lived in a cozy home with her family. One day, Lily's mom asked her to help chop the \n",
      "chores. Lily was excited to help and went to the house. She looked at the chores and said, \"Hello, little girl. Would we get home?\" The chores looked at Lily \n",
      "with big smiles and said, \"It's okay and wait for a moment to wake up.\" Lily smiled. She felt even ashamed that the chore. Later that day, Lily's mommy put \n",
      "her in the kitchen. It was a big bowl of rice, and Lily loved it. Her mommy said, \"Come on, Lily, you need to work out your door.\" Even though \n",
      "her hand desphminately, Lily started to feel better. When Lily was finished, she was happy again and said, \"I'm glad you, mommy!\" Her mommy smiled and said, \"Thanks for choice \n",
      "and I'm happy today. I was so proud of you too. You are a kind and helpful girl. Now every time you want to help the animals, we can set \n",
      "a useful picture.\" From that day on, Lily always remembered how to blue a child that apparent just like her. She was able to make new friends and make everyone \n",
      "happy. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 23.89 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.6841\n",
      "Batch 202, Loss: 1.7363\n",
      "Batch 203, Loss: 1.7146\n",
      "Batch 204, Loss: 1.6742\n",
      "Batch 205, Loss: 1.5682\n",
      "Batch 206, Loss: 1.7003\n",
      "Batch 207, Loss: 1.6927\n",
      "Batch 208, Loss: 1.7311\n",
      "Batch 209, Loss: 1.7378\n",
      "Batch 210, Loss: 1.6475\n",
      "Batch 211, Loss: 1.7283\n",
      "Batch 212, Loss: 1.6201\n",
      "Batch 213, Loss: 1.7014\n",
      "Batch 214, Loss: 1.6972\n",
      "Batch 215, Loss: 1.7180\n",
      "Batch 216, Loss: 1.6635\n",
      "Batch 217, Loss: 1.6172\n",
      "Batch 218, Loss: 1.6604\n",
      "Batch 219, Loss: 1.6271\n",
      "Batch 220, Loss: 1.6962\n",
      "Batch 221, Loss: 1.6640\n",
      "Batch 222, Loss: 1.6034\n",
      "Batch 223, Loss: 1.5944\n",
      "Batch 224, Loss: 1.6267\n",
      "Batch 225, Loss: 1.7177\n",
      "Batch 226, Loss: 1.6484\n",
      "Batch 227, Loss: 1.6017\n",
      "Batch 228, Loss: 1.6028\n",
      "Batch 229, Loss: 1.7120\n",
      "Batch 230, Loss: 1.7280\n",
      "Batch 231, Loss: 1.6853\n",
      "Batch 232, Loss: 1.6884\n",
      "Batch 233, Loss: 1.7059\n",
      "Batch 234, Loss: 1.7250\n",
      "Batch 235, Loss: 1.7465\n",
      "Batch 236, Loss: 1.7027\n",
      "Batch 237, Loss: 1.7101\n",
      "Batch 238, Loss: 1.6728\n",
      "Batch 239, Loss: 1.7316\n",
      "Batch 240, Loss: 1.6769\n",
      "Batch 241, Loss: 1.6848\n",
      "Batch 242, Loss: 1.5893\n",
      "Batch 243, Loss: 1.5959\n",
      "Batch 244, Loss: 1.6121\n",
      "Batch 245, Loss: 1.7038\n",
      "Batch 246, Loss: 1.6906\n",
      "Batch 247, Loss: 1.6831\n",
      "Batch 248, Loss: 1.7502\n",
      "Batch 249, Loss: 1.6774\n",
      "Batch 250, Loss: 1.7132\n",
      "Batch 251, Loss: 1.7031\n",
      "Batch 252, Loss: 1.7158\n",
      "Batch 253, Loss: 1.6893\n",
      "Batch 254, Loss: 1.6261\n",
      "Batch 255, Loss: 1.5970\n",
      "Batch 256, Loss: 1.7293\n",
      "Batch 257, Loss: 1.5958\n",
      "Batch 258, Loss: 1.6201\n",
      "Batch 259, Loss: 1.7003\n",
      "Batch 260, Loss: 1.6187\n",
      "Batch 261, Loss: 1.6913\n",
      "Batch 262, Loss: 1.5936\n",
      "Batch 263, Loss: 1.6941\n",
      "Batch 264, Loss: 1.5911\n",
      "Batch 265, Loss: 1.6957\n",
      "Batch 266, Loss: 1.6936\n",
      "Batch 267, Loss: 1.6151\n",
      "Batch 268, Loss: 1.6671\n",
      "Batch 269, Loss: 1.6632\n",
      "Batch 270, Loss: 1.7120\n",
      "Batch 271, Loss: 1.7045\n",
      "Batch 272, Loss: 1.6451\n",
      "Batch 273, Loss: 1.7067\n",
      "Batch 274, Loss: 1.7248\n",
      "Batch 275, Loss: 1.6883\n",
      "Batch 276, Loss: 1.6452\n",
      "Batch 277, Loss: 1.6962\n",
      "Batch 278, Loss: 1.7055\n",
      "Batch 279, Loss: 1.6624\n",
      "Batch 280, Loss: 1.7080\n",
      "Batch 281, Loss: 1.6917\n",
      "Batch 282, Loss: 1.6631\n",
      "Batch 283, Loss: 1.6219\n",
      "Batch 284, Loss: 1.6302\n",
      "Batch 285, Loss: 1.6955\n",
      "Batch 286, Loss: 1.7357\n",
      "Batch 287, Loss: 1.5995\n",
      "Batch 288, Loss: 1.7057\n",
      "Batch 289, Loss: 1.6979\n",
      "Batch 290, Loss: 1.6477\n",
      "Batch 291, Loss: 1.6826\n",
      "Batch 292, Loss: 1.6890\n",
      "Batch 293, Loss: 1.6906\n",
      "Batch 294, Loss: 1.6990\n",
      "Batch 295, Loss: 1.6477\n",
      "Batch 296, Loss: 1.6762\n",
      "Batch 297, Loss: 1.6450\n",
      "Batch 298, Loss: 1.7047\n",
      "Batch 299, Loss: 1.6976\n",
      "Batch 300, Loss: 1.7247\n",
      "Batch 301, Loss: 1.7234\n",
      "Batch 302, Loss: 1.7041\n",
      "Batch 303, Loss: 1.6573\n",
      "Batch 304, Loss: 1.6653\n",
      "Batch 305, Loss: 1.6944\n",
      "Batch 306, Loss: 1.5992\n",
      "Batch 307, Loss: 1.6873\n",
      "Batch 308, Loss: 1.6829\n",
      "Batch 309, Loss: 1.6852\n",
      "Batch 310, Loss: 1.6320\n",
      "Batch 311, Loss: 1.7240\n",
      "Batch 312, Loss: 1.5972\n",
      "Batch 313, Loss: 1.6917\n",
      "Batch 314, Loss: 1.6518\n",
      "Batch 315, Loss: 1.6283\n",
      "Batch 316, Loss: 1.7235\n",
      "Batch 317, Loss: 1.7220\n",
      "Batch 318, Loss: 1.7042\n",
      "Batch 319, Loss: 1.7099\n",
      "Batch 320, Loss: 1.7265\n",
      "Batch 321, Loss: 1.6093\n",
      "Batch 322, Loss: 1.6747\n",
      "Batch 323, Loss: 1.6856\n",
      "Batch 324, Loss: 1.6313\n",
      "Batch 325, Loss: 1.6861\n",
      "Batch 326, Loss: 1.7367\n",
      "Batch 327, Loss: 1.6336\n",
      "Batch 328, Loss: 1.6495\n",
      "Batch 329, Loss: 1.7153\n",
      "Batch 330, Loss: 1.7207\n",
      "Batch 331, Loss: 1.6615\n",
      "Batch 332, Loss: 1.6303\n",
      "Batch 333, Loss: 1.6287\n",
      "Batch 334, Loss: 1.7110\n",
      "Batch 335, Loss: 1.6341\n",
      "Batch 336, Loss: 1.7184\n",
      "Batch 337, Loss: 1.7491\n",
      "Batch 338, Loss: 1.6809\n",
      "Batch 339, Loss: 1.6136\n",
      "Batch 340, Loss: 1.7455\n",
      "Batch 341, Loss: 1.7096\n",
      "Batch 342, Loss: 1.7256\n",
      "Batch 343, Loss: 1.6956\n",
      "Batch 344, Loss: 1.6972\n",
      "Batch 345, Loss: 1.6243\n",
      "Batch 346, Loss: 1.7132\n",
      "Batch 347, Loss: 1.7111\n",
      "Batch 348, Loss: 1.6769\n",
      "Batch 349, Loss: 1.7331\n",
      "Batch 350, Loss: 1.7027\n",
      "Batch 351, Loss: 1.7150\n",
      "Batch 352, Loss: 1.6937\n",
      "Batch 353, Loss: 1.6441\n",
      "Batch 354, Loss: 1.6800\n",
      "Batch 355, Loss: 1.7439\n",
      "Batch 356, Loss: 1.7358\n",
      "Batch 357, Loss: 1.7089\n",
      "Batch 358, Loss: 1.6982\n",
      "Batch 359, Loss: 1.6095\n",
      "Batch 360, Loss: 1.7142\n",
      "Batch 361, Loss: 1.7516\n",
      "Batch 362, Loss: 1.6815\n",
      "Batch 363, Loss: 1.6668\n",
      "Batch 364, Loss: 1.5800\n",
      "Batch 365, Loss: 1.6571\n",
      "Batch 366, Loss: 1.6657\n",
      "Batch 367, Loss: 1.7039\n",
      "Batch 368, Loss: 1.6434\n",
      "Batch 369, Loss: 1.2784\n",
      "Batch 370, Loss: 1.6832\n",
      "Batch 371, Loss: 1.6282\n",
      "Batch 372, Loss: 1.7132\n",
      "Batch 373, Loss: 1.5985\n",
      "Batch 374, Loss: 1.7491\n",
      "Batch 375, Loss: 1.7310\n",
      "Batch 376, Loss: 1.6595\n",
      "Batch 377, Loss: 1.7253\n",
      "Batch 378, Loss: 1.7189\n",
      "Epoch 19, Average Loss: 1.6761\n",
      "Jill was a busy duck question boy. He wanted to see a butterfly-covered! He was very excited. He nibbled on his legs and waited until he reached for the butterfly's \n",
      "mummy grocer. Jill thought about the butterfly, and thought. He wanted to make sure he got better and make his mouth surround them better. He reached up and gasped. The \n",
      "butterfly ended and no longer felt very weak. Jill was not surefunded about the butterfly. He just looked very strong. Jill told him he was polite by teaching other it \n",
      "of the butterfly. She didn't like the butterfly into mummy. She wondered what else had happened. Jill patiently agreed and whispered, \"I love spending time creature and started!\" The butterfly \n",
      "was so excited. He knew that she couldn't stop thinking about the butterfly, but also knew that he had gone on his own and made it even more colour he \n",
      "had seen so still in the grocer. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 1.6379\n",
      "Batch 2, Loss: 1.6409\n",
      "Batch 3, Loss: 1.7104\n",
      "Batch 4, Loss: 1.7205\n",
      "Batch 5, Loss: 1.6749\n",
      "Batch 6, Loss: 1.6807\n",
      "Batch 7, Loss: 1.5733\n",
      "Batch 8, Loss: 1.6774\n",
      "Batch 9, Loss: 1.6925\n",
      "Batch 10, Loss: 1.7021\n",
      "Batch 11, Loss: 1.6061\n",
      "Batch 12, Loss: 1.6703\n",
      "Batch 13, Loss: 1.6760\n",
      "Batch 14, Loss: 1.7058\n",
      "Batch 15, Loss: 1.7226\n",
      "Batch 16, Loss: 1.6209\n",
      "Batch 17, Loss: 1.6191\n",
      "Batch 18, Loss: 1.6659\n",
      "Batch 19, Loss: 1.5868\n",
      "Batch 20, Loss: 1.6568\n",
      "Batch 21, Loss: 1.6807\n",
      "Batch 22, Loss: 1.6738\n",
      "Batch 23, Loss: 1.6764\n",
      "Batch 24, Loss: 1.7159\n",
      "Batch 25, Loss: 1.6605\n",
      "Batch 26, Loss: 1.5891\n",
      "Batch 27, Loss: 1.6506\n",
      "Batch 28, Loss: 1.7072\n",
      "Batch 29, Loss: 1.6212\n",
      "Batch 30, Loss: 1.5837\n",
      "Batch 31, Loss: 1.5993\n",
      "Batch 32, Loss: 1.5962\n",
      "Batch 33, Loss: 1.6765\n",
      "Batch 34, Loss: 1.6754\n",
      "Batch 35, Loss: 1.6743\n",
      "Batch 36, Loss: 1.6247\n",
      "Batch 37, Loss: 1.6834\n",
      "Batch 38, Loss: 1.6411\n",
      "Batch 39, Loss: 1.6914\n",
      "Batch 40, Loss: 1.6603\n",
      "Batch 41, Loss: 1.6860\n",
      "Batch 42, Loss: 1.7111\n",
      "Batch 43, Loss: 1.5791\n",
      "Batch 44, Loss: 1.7001\n",
      "Batch 45, Loss: 1.6750\n",
      "Batch 46, Loss: 1.7102\n",
      "Batch 47, Loss: 1.6824\n",
      "Batch 48, Loss: 1.6756\n",
      "Batch 49, Loss: 1.6094\n",
      "Batch 50, Loss: 1.6753\n",
      "Batch 51, Loss: 1.6859\n",
      "Batch 52, Loss: 1.6877\n",
      "Batch 53, Loss: 1.7033\n",
      "Batch 54, Loss: 1.6874\n",
      "Batch 55, Loss: 1.6721\n",
      "Batch 56, Loss: 1.6786\n",
      "Batch 57, Loss: 1.6776\n",
      "Batch 58, Loss: 1.5993\n",
      "Batch 59, Loss: 1.6713\n",
      "Batch 60, Loss: 1.6872\n",
      "Batch 61, Loss: 1.6499\n",
      "Batch 62, Loss: 1.6691\n",
      "Batch 63, Loss: 1.6761\n",
      "Batch 64, Loss: 1.7062\n",
      "Batch 65, Loss: 1.6258\n",
      "Batch 66, Loss: 1.6162\n",
      "Batch 67, Loss: 1.7307\n",
      "Batch 68, Loss: 1.5764\n",
      "Batch 69, Loss: 1.6827\n",
      "Batch 70, Loss: 1.6820\n",
      "Batch 71, Loss: 1.6817\n",
      "Batch 72, Loss: 1.5761\n",
      "Batch 73, Loss: 1.7088\n",
      "Batch 74, Loss: 1.6558\n",
      "Batch 75, Loss: 1.6239\n",
      "Batch 76, Loss: 1.6713\n",
      "Batch 77, Loss: 1.6950\n",
      "Batch 78, Loss: 1.7032\n",
      "Batch 79, Loss: 1.6778\n",
      "Batch 80, Loss: 1.6352\n",
      "Batch 81, Loss: 1.6746\n",
      "Batch 82, Loss: 1.6751\n",
      "Batch 83, Loss: 1.6795\n",
      "Batch 84, Loss: 1.5926\n",
      "Batch 85, Loss: 1.5931\n",
      "Batch 86, Loss: 1.6682\n",
      "Batch 87, Loss: 1.6594\n",
      "Batch 88, Loss: 1.6834\n",
      "Batch 89, Loss: 1.6550\n",
      "Batch 90, Loss: 1.6822\n",
      "Batch 91, Loss: 1.6769\n",
      "Batch 92, Loss: 1.6696\n",
      "Batch 93, Loss: 1.6273\n",
      "Batch 94, Loss: 1.6806\n",
      "Batch 95, Loss: 1.5901\n",
      "Batch 96, Loss: 1.6724\n",
      "Batch 97, Loss: 1.5911\n",
      "Batch 98, Loss: 1.5820\n",
      "Batch 99, Loss: 1.6295\n",
      "Batch 100, Loss: 1.6456\n",
      "Batch 101, Loss: 1.6768\n",
      "Batch 102, Loss: 1.6829\n",
      "Batch 103, Loss: 1.6327\n",
      "Batch 104, Loss: 1.6837\n",
      "Batch 105, Loss: 1.6835\n",
      "Batch 106, Loss: 1.6638\n",
      "Batch 107, Loss: 1.5767\n",
      "Batch 108, Loss: 1.7059\n",
      "Batch 109, Loss: 1.6720\n",
      "Batch 110, Loss: 1.6532\n",
      "Batch 111, Loss: 1.6792\n",
      "Batch 112, Loss: 1.6387\n",
      "Batch 113, Loss: 1.6752\n",
      "Batch 114, Loss: 1.7032\n",
      "Batch 115, Loss: 1.6775\n",
      "Batch 116, Loss: 1.6839\n",
      "Batch 117, Loss: 1.6829\n",
      "Batch 118, Loss: 1.5423\n",
      "Batch 119, Loss: 1.6752\n",
      "Batch 120, Loss: 1.6649\n",
      "Batch 121, Loss: 1.6233\n",
      "Batch 122, Loss: 1.6627\n",
      "Batch 123, Loss: 1.5873\n",
      "Batch 124, Loss: 1.6858\n",
      "Batch 125, Loss: 1.5832\n",
      "Batch 126, Loss: 1.5958\n",
      "Batch 127, Loss: 1.6953\n",
      "Batch 128, Loss: 1.6153\n",
      "Batch 129, Loss: 1.5911\n",
      "Batch 130, Loss: 1.7271\n",
      "Batch 131, Loss: 1.7258\n",
      "Batch 132, Loss: 1.5924\n",
      "Batch 133, Loss: 1.7062\n",
      "Batch 134, Loss: 1.5926\n",
      "Batch 135, Loss: 1.6440\n",
      "Batch 136, Loss: 1.6602\n",
      "Batch 137, Loss: 1.6916\n",
      "Batch 138, Loss: 1.7205\n",
      "Batch 139, Loss: 1.7145\n",
      "Batch 140, Loss: 1.6692\n",
      "Batch 141, Loss: 1.7050\n",
      "Batch 142, Loss: 1.6524\n",
      "Batch 143, Loss: 1.6057\n",
      "Batch 144, Loss: 1.7127\n",
      "Batch 145, Loss: 1.6098\n",
      "Batch 146, Loss: 1.7048\n",
      "Batch 147, Loss: 1.6814\n",
      "Batch 148, Loss: 1.6199\n",
      "Batch 149, Loss: 1.6177\n",
      "Batch 150, Loss: 1.7440\n",
      "Batch 151, Loss: 1.7007\n",
      "Batch 152, Loss: 1.7264\n",
      "Batch 153, Loss: 1.7091\n",
      "Batch 154, Loss: 1.6041\n",
      "Batch 155, Loss: 1.7415\n",
      "Batch 156, Loss: 1.5854\n",
      "Batch 157, Loss: 1.6751\n",
      "Batch 158, Loss: 1.6336\n",
      "Batch 159, Loss: 1.5808\n",
      "Batch 160, Loss: 1.6349\n",
      "Batch 161, Loss: 1.6219\n",
      "Batch 162, Loss: 1.6019\n",
      "Batch 163, Loss: 1.6865\n",
      "Batch 164, Loss: 1.6202\n",
      "Batch 165, Loss: 1.7238\n",
      "Batch 166, Loss: 1.5898\n",
      "Batch 167, Loss: 1.5741\n",
      "Batch 168, Loss: 1.6187\n",
      "Batch 169, Loss: 1.6896\n",
      "Batch 170, Loss: 1.6857\n",
      "Batch 171, Loss: 1.6653\n",
      "Batch 172, Loss: 1.6947\n",
      "Batch 173, Loss: 1.6447\n",
      "Batch 174, Loss: 1.6922\n",
      "Batch 175, Loss: 1.6898\n",
      "Batch 176, Loss: 1.7283\n",
      "Batch 177, Loss: 1.6705\n",
      "Batch 178, Loss: 1.6251\n",
      "Batch 179, Loss: 1.6932\n",
      "Batch 180, Loss: 1.6837\n",
      "Batch 181, Loss: 1.6711\n",
      "Batch 182, Loss: 1.6650\n",
      "Batch 183, Loss: 1.6954\n",
      "Batch 184, Loss: 1.6130\n",
      "Batch 185, Loss: 1.5522\n",
      "Batch 186, Loss: 1.6764\n",
      "Batch 187, Loss: 1.6897\n",
      "Batch 188, Loss: 1.6788\n",
      "Batch 189, Loss: 1.6959\n",
      "Batch 190, Loss: 1.6510\n",
      "Batch 191, Loss: 1.6646\n",
      "Batch 192, Loss: 1.6970\n",
      "Batch 193, Loss: 1.6806\n",
      "Batch 194, Loss: 1.6353\n",
      "Batch 195, Loss: 1.7031\n",
      "Batch 196, Loss: 1.6972\n",
      "Batch 197, Loss: 1.7259\n",
      "Batch 198, Loss: 1.7068\n",
      "Batch 199, Loss: 1.7163\n",
      "Batch 200, Loss: 1.6846\n",
      "Sam was a wealthy boy, who liked to stop and look out at the sky. He saw a big dog running at her and laughed a lot. It was pink \n",
      "and red. He was a bit scared and ran away as fast as he knew he had to wait for napping me. Sam roared and chased him. He roared, dusting \n",
      "and roared and ran away and hid. He fell and rolled and dropped a stone with a stick. He rolled it in the bushes and rolled back to the ground. \n",
      "Then he went back home to tell his mom and dad. But then, something surprise happened. A tire was a loud roar! Sam stopped rolling and a beginning broke by \n",
      "way. He was hurt and scared and ran back. His mom and dad were very patient and safe. Then, a kind girl came out of the dog. She saw the \n",
      "bucket and the girl. It was perfect and had earned a roar. She was patient and warm up. She gave him a life and a wave of its cookies. Wedness \n",
      "was fluffy, but the girl wouldn't hurt him on. She screamed and hugged him. She said the tire was she had planted. Sam's mom and dad said they had listened \n",
      "and ordinary of stories that day. They said that was the best adventure ever, but their family loved living in the world again. Sam was brave, but he loved the \n",
      "sound of the dog, and they couldn't help order for the \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.09 GB\n",
      "Cache memory: 25.40 GB\n",
      "Peak memory: 18.40 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 1.6065\n",
      "Batch 202, Loss: 1.6788\n",
      "Batch 203, Loss: 1.6814\n",
      "Batch 204, Loss: 1.5952\n",
      "Batch 205, Loss: 1.5752\n",
      "Batch 206, Loss: 1.6290\n",
      "Batch 207, Loss: 1.5857\n",
      "Batch 208, Loss: 1.6999\n",
      "Batch 209, Loss: 1.6850\n",
      "Batch 210, Loss: 1.7266\n",
      "Batch 211, Loss: 1.7164\n",
      "Batch 212, Loss: 1.6276\n",
      "Batch 213, Loss: 1.6002\n",
      "Batch 214, Loss: 1.7203\n",
      "Batch 215, Loss: 1.6687\n",
      "Batch 216, Loss: 1.6578\n",
      "Batch 217, Loss: 1.7090\n",
      "Batch 218, Loss: 1.6444\n",
      "Batch 219, Loss: 1.6376\n",
      "Batch 220, Loss: 1.6293\n",
      "Batch 221, Loss: 1.7233\n",
      "Batch 222, Loss: 1.7115\n",
      "Batch 223, Loss: 1.6825\n",
      "Batch 224, Loss: 1.6985\n",
      "Batch 225, Loss: 1.7232\n",
      "Batch 226, Loss: 1.5809\n",
      "Batch 227, Loss: 1.6780\n",
      "Batch 228, Loss: 1.7187\n",
      "Batch 229, Loss: 1.7313\n",
      "Batch 230, Loss: 1.6393\n",
      "Batch 231, Loss: 1.6711\n",
      "Batch 232, Loss: 1.7001\n",
      "Batch 233, Loss: 1.7144\n",
      "Batch 234, Loss: 1.6250\n",
      "Batch 235, Loss: 1.6644\n",
      "Batch 236, Loss: 1.6773\n",
      "Batch 237, Loss: 1.7124\n",
      "Batch 238, Loss: 1.6693\n",
      "Batch 239, Loss: 1.7133\n",
      "Batch 240, Loss: 1.6862\n",
      "Batch 241, Loss: 1.6718\n",
      "Batch 242, Loss: 1.6706\n",
      "Batch 243, Loss: 1.7116\n",
      "Batch 244, Loss: 1.6942\n",
      "Batch 245, Loss: 1.6476\n",
      "Batch 246, Loss: 1.6962\n",
      "Batch 247, Loss: 1.6818\n",
      "Batch 248, Loss: 1.5601\n",
      "Batch 249, Loss: 1.6385\n",
      "Batch 250, Loss: 1.7165\n",
      "Batch 251, Loss: 1.6401\n",
      "Batch 252, Loss: 1.7004\n",
      "Batch 253, Loss: 1.6932\n",
      "Batch 254, Loss: 1.5684\n",
      "Batch 255, Loss: 1.6762\n",
      "Batch 256, Loss: 1.6646\n",
      "Batch 257, Loss: 1.6616\n",
      "Batch 258, Loss: 1.7406\n",
      "Batch 259, Loss: 1.6656\n",
      "Batch 260, Loss: 1.6667\n",
      "Batch 261, Loss: 1.6618\n",
      "Batch 262, Loss: 1.6814\n",
      "Batch 263, Loss: 1.6635\n",
      "Batch 264, Loss: 1.6484\n",
      "Batch 265, Loss: 1.5926\n",
      "Batch 266, Loss: 1.6421\n",
      "Batch 267, Loss: 1.7085\n",
      "Batch 268, Loss: 1.7072\n",
      "Batch 269, Loss: 1.6858\n",
      "Batch 270, Loss: 1.6587\n",
      "Batch 271, Loss: 1.6927\n",
      "Batch 272, Loss: 1.7207\n",
      "Batch 273, Loss: 1.6759\n",
      "Batch 274, Loss: 1.6791\n",
      "Batch 275, Loss: 1.6242\n",
      "Batch 276, Loss: 1.6686\n",
      "Batch 277, Loss: 1.6279\n",
      "Batch 278, Loss: 1.6765\n",
      "Batch 279, Loss: 1.7372\n",
      "Batch 280, Loss: 1.7048\n",
      "Batch 281, Loss: 1.6940\n",
      "Batch 282, Loss: 1.6011\n",
      "Batch 283, Loss: 1.5734\n",
      "Batch 284, Loss: 1.6403\n",
      "Batch 285, Loss: 1.5860\n",
      "Batch 286, Loss: 1.7266\n",
      "Batch 287, Loss: 1.6083\n",
      "Batch 288, Loss: 1.6977\n",
      "Batch 289, Loss: 1.6653\n",
      "Batch 290, Loss: 1.6972\n",
      "Batch 291, Loss: 1.5993\n",
      "Batch 292, Loss: 1.6053\n",
      "Batch 293, Loss: 1.7317\n",
      "Batch 294, Loss: 1.6742\n",
      "Batch 295, Loss: 1.6673\n",
      "Batch 296, Loss: 1.6451\n",
      "Batch 297, Loss: 1.7043\n",
      "Batch 298, Loss: 1.6654\n",
      "Batch 299, Loss: 1.6743\n",
      "Batch 300, Loss: 1.7279\n",
      "Batch 301, Loss: 1.7290\n",
      "Batch 302, Loss: 1.5893\n",
      "Batch 303, Loss: 1.5728\n",
      "Batch 304, Loss: 1.6962\n",
      "Batch 305, Loss: 1.5782\n",
      "Batch 306, Loss: 1.6998\n",
      "Batch 307, Loss: 1.6262\n",
      "Batch 308, Loss: 1.6920\n",
      "Batch 309, Loss: 1.6257\n",
      "Batch 310, Loss: 1.6295\n",
      "Batch 311, Loss: 1.6685\n",
      "Batch 312, Loss: 1.6097\n",
      "Batch 313, Loss: 1.7016\n",
      "Batch 314, Loss: 1.5993\n",
      "Batch 315, Loss: 1.7131\n",
      "Batch 316, Loss: 1.6834\n",
      "Batch 317, Loss: 1.6494\n",
      "Batch 318, Loss: 1.6961\n",
      "Batch 319, Loss: 1.6497\n",
      "Batch 320, Loss: 1.6545\n",
      "Batch 321, Loss: 1.6505\n",
      "Batch 322, Loss: 1.5820\n",
      "Batch 323, Loss: 1.6473\n",
      "Batch 324, Loss: 1.6894\n",
      "Batch 325, Loss: 1.6702\n",
      "Batch 326, Loss: 1.6679\n",
      "Batch 327, Loss: 1.6336\n",
      "Batch 328, Loss: 1.5802\n",
      "Batch 329, Loss: 1.6319\n",
      "Batch 330, Loss: 1.6818\n",
      "Batch 331, Loss: 1.6830\n",
      "Batch 332, Loss: 1.5805\n",
      "Batch 333, Loss: 1.6308\n",
      "Batch 334, Loss: 1.6879\n",
      "Batch 335, Loss: 1.6544\n",
      "Batch 336, Loss: 1.7049\n",
      "Batch 337, Loss: 1.6009\n",
      "Batch 338, Loss: 1.6774\n",
      "Batch 339, Loss: 1.6098\n",
      "Batch 340, Loss: 1.5738\n",
      "Batch 341, Loss: 1.6875\n",
      "Batch 342, Loss: 1.7057\n",
      "Batch 343, Loss: 1.7084\n",
      "Batch 344, Loss: 1.6435\n",
      "Batch 345, Loss: 1.6679\n",
      "Batch 346, Loss: 1.7024\n",
      "Batch 347, Loss: 1.6763\n",
      "Batch 348, Loss: 1.7030\n",
      "Batch 349, Loss: 1.6945\n",
      "Batch 350, Loss: 1.5803\n",
      "Batch 351, Loss: 1.2260\n",
      "Batch 352, Loss: 1.7126\n",
      "Batch 353, Loss: 1.7310\n",
      "Batch 354, Loss: 1.6316\n",
      "Batch 355, Loss: 1.6761\n",
      "Batch 356, Loss: 1.6858\n",
      "Batch 357, Loss: 1.7251\n",
      "Batch 358, Loss: 1.6149\n",
      "Batch 359, Loss: 1.5865\n",
      "Batch 360, Loss: 1.5985\n",
      "Batch 361, Loss: 1.7333\n",
      "Batch 362, Loss: 1.7066\n",
      "Batch 363, Loss: 1.7424\n",
      "Batch 364, Loss: 1.6077\n",
      "Batch 365, Loss: 1.7129\n",
      "Batch 366, Loss: 1.6899\n",
      "Batch 367, Loss: 1.7055\n",
      "Batch 368, Loss: 1.7076\n",
      "Batch 369, Loss: 1.6152\n",
      "Batch 370, Loss: 1.6808\n",
      "Batch 371, Loss: 1.6442\n",
      "Batch 372, Loss: 1.6338\n",
      "Batch 373, Loss: 1.5708\n",
      "Batch 374, Loss: 1.6923\n",
      "Batch 375, Loss: 1.7112\n",
      "Batch 376, Loss: 1.6826\n",
      "Batch 377, Loss: 1.7080\n",
      "Batch 378, Loss: 1.7197\n",
      "Epoch 20, Average Loss: 1.6618\n",
      "Ben and Mia liked to play in the garden. They saw a big black cat and many carrots on them. Ben's eyes wide and she asked her a question. Ben \n",
      "nodded and smiled. \"Wow, that's a pretty bird,\" Ben said. \"Can I please behind you, please?\" Ben did not like that idea. He ignored her and gave a kiss and \n",
      "a gun. A nice person who came to visit and gave them frame. He told Anna and Ben not to go. Ben felt sad and angry. He ran away. He \n",
      "did not want to go back home and say he did a bad game. \"Please, please, Ben,\" Ben said. \"It's okay? I hurrses it like her.\" Ben felt scared. He \n",
      "named his voice. He had told her that he lost her friend's pinks and his gun. He had met a whistle for his dad. \"Your please feel sorry,\" Ben said. \n",
      "\"You look very bad. You need a book and a whistle. I will have no gun. Do you want to try a book?\" Ben nodded in agreement. He realized that \n",
      "he was scared. He wanted to be a soft bird hope. He walked to find her mom. Lily hugged him and said, \"You are my best friend too, Ben. You \n",
      "need a book to save your gun. Thank you, Ben. You make a new friend.\" Ben realized he was wrong and promised to be more careful. He learned a very \n",
      "important lesson: Do not touch other people's things that is bad for ignoring. He wished he had listened to her. But he could say sorry and contained. Ben learned his \n",
      "lesson. He asked Mia and her to come back and play with him. He told her his name. Anna nodded and smiled. She could not obsumes and lie. She wished \n",
      "him and the fun company in the garden. Finally, he met Mia skipped away with her. They learned their lesson. He was not a regular suation anymore. He learned that \n",
      "helping things can bring good animals too. \n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "save_freq = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    if last_epoch and epoch + 1 < last_epoch:\n",
    "        continue\n",
    "    for i, seq in enumerate(stream):\n",
    "        if last_batch and epoch + 1 == last_epoch and i + 1 <= last_batch:\n",
    "            continue\n",
    "        mx_seq = mx.array(seq[DICT_LABEL])\n",
    "        input_seq = mx_seq[:, :-1]  # Exclude the last token for input\n",
    "        target_seq = mx_seq[:, 1:]  # Exclude the first token for target\n",
    "        if PADDING:\n",
    "            loss, grads = loss_and_grad(model, input_seq, target_seq, pad_token_id=pad_token_id)\n",
    "        else:\n",
    "            loss, grads = loss_and_grad(model, input_seq, target_seq)\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        print(f\"Batch {i + 1}, Loss: {loss:.4f}\")\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * num_batches + i)\n",
    "        if (i+1) % save_freq == 0:\n",
    "            generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "            model.save_weights(f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1}.npz')\n",
    "            if i+1 != save_freq:\n",
    "                prev_save_path = f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1-save_freq}.npz'\n",
    "                if os.path.exists(prev_save_path):\n",
    "                    os.remove(prev_save_path)\n",
    "            print('-'*20)\n",
    "            print(f\"Active memory: {mx.get_active_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Cache memory: {mx.get_cache_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Peak memory: {mx.get_peak_memory() / 1024**3:.2f} GB\")\n",
    "            mx.clear_cache()\n",
    "            print('-'*20)\n",
    "        losses.append(loss)\n",
    "    stream.reset()\n",
    "    avg_loss = mx.array(losses).mean()\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/epoch_train', avg_loss.item(), epoch)\n",
    "    generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "    matching_paths = list(Path(LOG_DIR).glob(f'model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_*.npz'))\n",
    "    if len(matching_paths) > 0:\n",
    "        os.remove(matching_paths[0])\n",
    "    model.save_weights(f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}.npz')\n",
    "    if epoch + 1 > 1:\n",
    "        prev_epoch_path = f'{LOG_DIR}/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch}.npz'\n",
    "        if os.path.exists(prev_epoch_path):\n",
    "            os.remove(prev_epoch_path)\n",
    "writer.add_hparams(params, {'hparam/last_loss': avg_loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c93e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was an elderly monkey. He was sad because he couldn't do anything. He wanted to do something. He looked around for food and he saw \n",
      "some wooden words on the wall. He was so excited that he couldn't help but he was still very happy. He gathered some yummy food and some cookies. He ate \n",
      "yummy and the cookies, and he was very impressed with him. He was so happy he could come back soon. As he was eating, he saw a bowl of cookies \n",
      "and he wanted to cut the cookies. He quickly grabbed some and started to cut the cookie and taste it. It was so delicious! The cookies were so delicious, just \n",
      "like he did a few moments. He was so happy that he could cut the cookies with his crayons and help people heal. \n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
