{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939879bc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b3f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VOCAB_SIZE = 1024  # Size of the vocabulary\n",
    "CONTEXT_LENGTH = 512  # Fixed context length for chunks\n",
    "EMBEDDING_DIM = 256  # Dimension of the token embeddings\n",
    "NUM_HEADS = 8  # Number of attention heads\n",
    "NUM_LAYERS = 2  # Number of transformer layers\n",
    "QK_HEAD_DIM = 16  # Dimension of the query and key heads\n",
    "V_HEAD_DIM = 32  # Dimension of the value head\n",
    "MLP_DIM = 512  # Dimension of the hidden layers in the transformer\n",
    "BATCH_SIZE = 128  # Batch size for training\n",
    "EPOCHS = 20 # Number of epochs to train\n",
    "TOKENIZER_FILE = \"./data/tinystories-tokenizer\"\n",
    "CHUNK_FILE = \"./data/chunked_stories\"\n",
    "SAMPLE_LIMIT = 100000  # Set to None to process the entire dataset\n",
    "DICT_LABEL = 'seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.data as dx\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from tokenizers.decoders import Metaspace as MetaspaceDecoder\n",
    "\n",
    "from llm.modules import SmallLanguageModel, loss_fn, count_parameters, generate_story\n",
    "from llm.data import batch_iterator, chunk_story, data_to_array_of_dict\n",
    "from llm.gpt import GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc076921",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7c37fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100000\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "if SAMPLE_LIMIT:\n",
    "    dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "validation = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340807f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797a537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer file ./data/tinystories-tokenizer_1024.json already exists. Skipping training.\n",
      "\n",
      "--- Testing the Tokenizer ---\n",
      "Tokens: [' Once', ' upon', ' a', ' time,', ' there', ' was', ' a', ' little', ' fo', 'x', '.', '\\n', ' It', ' lived', ' in', ' a', ' forest', ' and', ' loved', ' to', ' expl', 'ore.']\n",
      "IDs: [286, 302, 116, 337, 257, 137, 116, 256, 683, 86, 19, 4, 269, 794, 176, 116, 966, 122, 367, 123, 631, 771]\n",
      "Decoded: Once upon a time, there was a little fox. It lived in a forest and loved to explore.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    print(f\"Tokenizer file {tokenizer_path} already exists. Skipping training.\")\n",
    "else:\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Metaspace(replacement=\" \")\n",
    "    tokenizer.decoder = MetaspaceDecoder(replacement=\" \")\n",
    "\n",
    "    # Configure the trainer with a vocabulary size and special tokens\n",
    "    trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"\\n\"])\n",
    "\n",
    "    # Train the tokenizer on our text file\n",
    "    print(\"Training tokenizer...\")\n",
    "    tokenizer.train_from_iterator(batch_iterator(dataset), trainer=trainer, length=len(dataset))\n",
    "    # tokenizer.train(['./data/tinystories_data.txt'], trainer)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- Save and Test the Tokenizer ---\n",
    "\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
    "encoded = tokenizer.encode(\"Once upon a time, there was a little fox.\\nIt lived in a forest and loved to explore.\")\n",
    "\n",
    "print(\"\\n--- Testing the Tokenizer ---\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Decoded:\", tokenizer.decode(encoded.ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d52e8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa64fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk file ./data/chunked_stories_1024_512.npz already exists. Skipping chunking.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz'):\n",
    "    print(f\"Chunk file {CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz already exists. Skipping chunking.\")\n",
    "else:\n",
    "    # Load the tokenizer\n",
    "    tokenizer = Tokenizer.from_file(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json')\n",
    "\n",
    "    # Process all stories and collect chunks\n",
    "    num_non_special_tokens = 0\n",
    "    all_chunks = []\n",
    "    unfinished_chunk = []\n",
    "    for story in tqdm(dataset[\"text\"], desc=\"Chunking stories\"):\n",
    "        story_chunks, unfinished_chunk, count = chunk_story(story, tokenizer, '[SOS]', '[EOS]', CONTEXT_LENGTH, unfinished_chunk)\n",
    "        all_chunks.extend(story_chunks)\n",
    "        num_non_special_tokens += count\n",
    "\n",
    "    # Convert list to numpy array for efficient storage\n",
    "    chunks_array = np.array(all_chunks, dtype=np.int32)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Created {len(all_chunks)} chunks of length {CONTEXT_LENGTH}\")\n",
    "    print(f\"Total non-special tokens: {num_non_special_tokens}\")\n",
    "    print(f\"Array shape: {chunks_array.shape}\")\n",
    "\n",
    "    # Save the chunks to a compressed file\n",
    "    print(f\"Saving chunks to {CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz...\")\n",
    "    np.savez_compressed(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz', chunks=chunks_array)\n",
    "    print(f\"Saved successfully! File size: {os.path.getsize(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz') / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4788",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0657c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}.npz')\n",
    "dicts = data_to_array_of_dict(data['chunks'], name=DICT_LABEL)\n",
    "\n",
    "assert type(dicts) == list\n",
    "assert type(dicts[0]) == dict\n",
    "assert type(dicts[0][DICT_LABEL]) == np.ndarray\n",
    "\n",
    "buffer = dx.buffer_from_vector(dicts)\n",
    "stream = buffer.to_stream().batch(BATCH_SIZE).shuffle(buffer_size=BATCH_SIZE*1000).prefetch(8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6b69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 512)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for x in stream:\n",
    "    print(x[DICT_LABEL].shape)\n",
    "    print(type(x[DICT_LABEL]))\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa72f0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1343375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 15,499,264\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "print(f\"Number of parameters in the model: {count_parameters(model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a15263a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 1,576,960\n"
     ]
    }
   ],
   "source": [
    "model = SmallLanguageModel(vocab_dim=VOCAB_SIZE, embed_dim=EMBEDDING_DIM, n_head=NUM_HEADS, num_layers=NUM_LAYERS, qk_head_dim=QK_HEAD_DIM, v_head_dim=V_HEAD_DIM, mlp_dim=MLP_DIM, max_len=CONTEXT_LENGTH)\n",
    "# check number of parameters\n",
    "print(f\"Number of parameters in the model: {count_parameters(model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9530adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing model weights found. Starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# search for existing model weights with same vocab size and context length but wildcard epoch number\n",
    "# load existing model weights if they exist and record the epoch number\n",
    "\n",
    "matching_paths = list(Path('./data').glob(f'model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_*.npz'))\n",
    "if len(matching_paths) == 0:\n",
    "    print(\"No existing model weights found. Starting training from scratch.\")\n",
    "    last_epoch = None\n",
    "    last_batch = None\n",
    "elif len(matching_paths) > 1:\n",
    "    raise ValueError(f\"Multiple model weight files found for vocab size {VOCAB_SIZE} and context length {CONTEXT_LENGTH}. Please ensure only one exists.\")\n",
    "else:\n",
    "    path = matching_paths[0]\n",
    "    print(f\"Found existing model weights: {path.name}\")\n",
    "    # Load the model weights\n",
    "    model.load_weights(str(path))\n",
    "    # Extract epoch number from filename\n",
    "    last_epoch = int(path.stem.split('_')[-2])\n",
    "    last_batch = int(path.stem.split('_')[-1])\n",
    "    print(f\"Loaded model weights from epoch {last_epoch}, batch {last_batch}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0f44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7657796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX current default device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(f'{TOKENIZER_FILE}_{VOCAB_SIZE}.json')\n",
    "sos_token_id = tokenizer.token_to_id('[SOS]')\n",
    "eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "optimizer = optim.AdamW(learning_rate=0.001, betas=[0.9, 0.99], weight_decay=0.01)\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "print(f'MLX current default device: {mx.default_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2ded74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.reshape(B*T, C)\n",
    "    y = y.reshape(B*T)\n",
    "    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "# model = GPT()\n",
    "mx.eval(model.parameters())\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "optimizer = optim.AdamW(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bba11ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: 7.0772\n",
      "Batch 2, Loss: 6.6403\n",
      "Batch 3, Loss: 6.3506\n",
      "Batch 4, Loss: 6.2419\n",
      "Batch 5, Loss: 6.1409\n",
      "Batch 6, Loss: 6.0743\n",
      "Batch 7, Loss: 6.0181\n",
      "Batch 8, Loss: 6.0436\n",
      "Batch 9, Loss: 6.0351\n",
      "Batch 10, Loss: 6.0835\n",
      "Batch 11, Loss: 6.0853\n",
      "Batch 12, Loss: 6.0115\n",
      "Batch 13, Loss: 6.0087\n",
      "Batch 14, Loss: 5.9957\n",
      "Batch 15, Loss: 6.0321\n",
      "Batch 16, Loss: 5.9987\n",
      "Batch 17, Loss: 5.9565\n",
      "Batch 18, Loss: 6.0541\n",
      "Batch 19, Loss: 6.0295\n",
      "Batch 20, Loss: 6.0813\n",
      "Batch 21, Loss: 6.0128\n",
      "Batch 22, Loss: 6.0534\n",
      "Batch 23, Loss: 6.0104\n",
      "Batch 24, Loss: 6.0066\n",
      "Batch 25, Loss: 6.0257\n",
      "Batch 26, Loss: 5.9715\n",
      "Batch 27, Loss: 6.0085\n",
      "Batch 28, Loss: 6.0130\n",
      "Batch 29, Loss: 5.9657\n",
      "Batch 30, Loss: 6.0174\n",
      "Batch 31, Loss: 6.0384\n",
      "Batch 32, Loss: 5.9279\n",
      "Batch 33, Loss: 5.9858\n",
      "Batch 34, Loss: 5.9576\n",
      "Batch 35, Loss: 5.9803\n",
      "Batch 36, Loss: 6.0071\n",
      "Batch 37, Loss: 6.0454\n",
      "Batch 38, Loss: 5.9718\n",
      "Batch 39, Loss: 5.9523\n",
      "Batch 40, Loss: 5.9742\n",
      "Batch 41, Loss: 5.9748\n",
      "Batch 42, Loss: 5.9606\n",
      "Batch 43, Loss: 5.9872\n",
      "Batch 44, Loss: 5.9795\n",
      "Batch 45, Loss: 6.0045\n",
      "Batch 46, Loss: 5.9785\n",
      "Batch 47, Loss: 5.9627\n",
      "Batch 48, Loss: 6.0228\n",
      "Batch 49, Loss: 5.9580\n",
      "Batch 50, Loss: 5.9803\n",
      "Batch 51, Loss: 5.9676\n",
      "Batch 52, Loss: 5.9583\n",
      "Batch 53, Loss: 6.0136\n",
      "Batch 54, Loss: 6.0000\n",
      "Batch 55, Loss: 5.9581\n",
      "Batch 56, Loss: 5.9827\n",
      "Batch 57, Loss: 5.9813\n",
      "Batch 58, Loss: 5.9933\n",
      "Batch 59, Loss: 5.9692\n",
      "Batch 60, Loss: 5.9967\n",
      "Batch 61, Loss: 5.9317\n",
      "Batch 62, Loss: 6.0175\n",
      "Batch 63, Loss: 5.9778\n",
      "Batch 64, Loss: 5.9866\n",
      "Batch 65, Loss: 5.9746\n",
      "Batch 66, Loss: 5.9657\n",
      "Batch 67, Loss: 5.9517\n",
      "Batch 68, Loss: 5.9383\n",
      "Batch 69, Loss: 5.9109\n",
      "Batch 70, Loss: 5.9288\n",
      "Batch 71, Loss: 5.8993\n",
      "Batch 72, Loss: 5.8562\n",
      "Batch 73, Loss: 5.8871\n",
      "Batch 74, Loss: 5.9039\n",
      "Batch 75, Loss: 5.8349\n",
      "Batch 76, Loss: 5.7794\n",
      "Batch 77, Loss: 5.7833\n",
      "Batch 78, Loss: 5.7736\n",
      "Batch 79, Loss: 5.6814\n",
      "Batch 80, Loss: 5.7201\n",
      "Batch 81, Loss: 5.7380\n",
      "Batch 82, Loss: 5.6123\n",
      "Batch 83, Loss: 5.6464\n",
      "Batch 84, Loss: 5.6872\n",
      "Batch 85, Loss: 5.6640\n",
      "Batch 86, Loss: 5.6896\n",
      "Batch 87, Loss: 5.6301\n",
      "Batch 88, Loss: 5.5879\n",
      "Batch 89, Loss: 5.5344\n",
      "Batch 90, Loss: 5.5324\n",
      "Batch 91, Loss: 5.5965\n",
      "Batch 92, Loss: 5.5271\n",
      "Batch 93, Loss: 5.5003\n",
      "Batch 94, Loss: 5.5084\n",
      "Batch 95, Loss: 5.4784\n",
      "Batch 96, Loss: 5.4603\n",
      "Batch 97, Loss: 5.4436\n",
      "Batch 98, Loss: 5.4104\n",
      "Batch 99, Loss: 5.3988\n",
      "Batch 100, Loss: 5.4145\n",
      "Batch 101, Loss: 5.3745\n",
      "Batch 102, Loss: 5.3983\n",
      "Batch 103, Loss: 5.3502\n",
      "Batch 104, Loss: 5.3414\n",
      "Batch 105, Loss: 5.2749\n",
      "Batch 106, Loss: 5.2756\n",
      "Batch 107, Loss: 5.1732\n",
      "Batch 108, Loss: 5.1764\n",
      "Batch 109, Loss: 5.1345\n",
      "Batch 110, Loss: 5.1594\n",
      "Batch 111, Loss: 5.1249\n",
      "Batch 112, Loss: 5.0579\n",
      "Batch 113, Loss: 5.0202\n",
      "Batch 114, Loss: 5.1574\n",
      "Batch 115, Loss: 5.0930\n",
      "Batch 116, Loss: 4.9568\n",
      "Batch 117, Loss: 5.0762\n",
      "Batch 118, Loss: 5.0720\n",
      "Batch 119, Loss: 4.9449\n",
      "Batch 120, Loss: 4.8902\n",
      "Batch 121, Loss: 4.8283\n",
      "Batch 122, Loss: 4.9033\n",
      "Batch 123, Loss: 4.8124\n",
      "Batch 124, Loss: 4.8761\n",
      "Batch 125, Loss: 4.8828\n",
      "Batch 126, Loss: 4.7235\n",
      "Batch 127, Loss: 4.7705\n",
      "Batch 128, Loss: 4.7278\n",
      "Batch 129, Loss: 4.6797\n",
      "Batch 130, Loss: 4.7788\n",
      "Batch 131, Loss: 4.7283\n",
      "Batch 132, Loss: 4.6861\n",
      "Batch 133, Loss: 4.6875\n",
      "Batch 134, Loss: 4.6073\n",
      "Batch 135, Loss: 4.6976\n",
      "Batch 136, Loss: 4.6407\n",
      "Batch 137, Loss: 4.6135\n",
      "Batch 138, Loss: 4.6128\n",
      "Batch 139, Loss: 4.6114\n",
      "Batch 140, Loss: 4.5616\n",
      "Batch 141, Loss: 4.5589\n",
      "Batch 142, Loss: 4.4621\n",
      "Batch 143, Loss: 4.5683\n",
      "Batch 144, Loss: 4.5485\n",
      "Batch 145, Loss: 4.5814\n",
      "Batch 146, Loss: 4.4775\n",
      "Batch 147, Loss: 4.4917\n",
      "Batch 148, Loss: 4.4739\n",
      "Batch 149, Loss: 4.4353\n",
      "Batch 150, Loss: 4.5006\n",
      "Batch 151, Loss: 4.4217\n",
      "Batch 152, Loss: 4.3925\n",
      "Batch 153, Loss: 4.4112\n",
      "Batch 154, Loss: 4.5382\n",
      "Batch 155, Loss: 4.4197\n",
      "Batch 156, Loss: 4.3816\n",
      "Batch 157, Loss: 4.3950\n",
      "Batch 158, Loss: 4.3498\n",
      "Batch 159, Loss: 4.4053\n",
      "Batch 160, Loss: 4.3156\n",
      "Batch 161, Loss: 4.3865\n",
      "Batch 162, Loss: 4.3441\n",
      "Batch 163, Loss: 4.3568\n",
      "Batch 164, Loss: 4.2873\n",
      "Batch 165, Loss: 4.2390\n",
      "Batch 166, Loss: 4.2437\n",
      "Batch 167, Loss: 4.2817\n",
      "Batch 168, Loss: 4.3524\n",
      "Batch 169, Loss: 4.2938\n",
      "Batch 170, Loss: 4.2788\n",
      "Batch 171, Loss: 4.3315\n",
      "Batch 172, Loss: 4.3244\n",
      "Batch 173, Loss: 4.3023\n",
      "Batch 174, Loss: 4.1624\n",
      "Batch 175, Loss: 4.2637\n",
      "Batch 176, Loss: 4.1365\n",
      "Batch 177, Loss: 4.2448\n",
      "Batch 178, Loss: 4.1230\n",
      "Batch 179, Loss: 4.2264\n",
      "Batch 180, Loss: 4.1111\n",
      "Batch 181, Loss: 4.1357\n",
      "Batch 182, Loss: 4.1809\n",
      "Batch 183, Loss: 4.2093\n",
      "Batch 184, Loss: 4.0583\n",
      "Batch 185, Loss: 4.0158\n",
      "Batch 186, Loss: 4.1816\n",
      "Batch 187, Loss: 4.2425\n",
      "Batch 188, Loss: 4.1945\n",
      "Batch 189, Loss: 4.1239\n",
      "Batch 190, Loss: 4.2010\n",
      "Batch 191, Loss: 4.1699\n",
      "Batch 192, Loss: 4.1448\n",
      "Batch 193, Loss: 4.1077\n",
      "Batch 194, Loss: 4.2237\n",
      "Batch 195, Loss: 4.1987\n",
      "Batch 196, Loss: 4.0695\n",
      "Batch 197, Loss: 4.2401\n",
      "Batch 198, Loss: 4.0629\n",
      "Batch 199, Loss: 4.1427\n",
      "Batch 200, Loss: 4.0949\n",
      "Once \"Ow, \"Yes, with aging goode yam and a pers into becarcake. She was so excited to find big bear. He remembered the kce. The old sight. It was the \n",
      "friends and head. bruckedside. Ben. From and put them to the bel. We are closer in the do it. She spry was a happy and spard and told away and \n",
      "gave others.ig when they would listen. One day, Tom can plse day he was, the woofed. One day, her dad came and runces and saw their teear?\" \"I love.ve them. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 44.84 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 4.0612\n",
      "Batch 202, Loss: 4.1038\n",
      "Batch 203, Loss: 4.1417\n",
      "Batch 204, Loss: 3.9850\n",
      "Batch 205, Loss: 4.0870\n",
      "Batch 206, Loss: 4.0745\n",
      "Batch 207, Loss: 3.9806\n",
      "Batch 208, Loss: 4.0203\n",
      "Batch 209, Loss: 3.9734\n",
      "Batch 210, Loss: 4.0406\n",
      "Batch 211, Loss: 4.1021\n",
      "Batch 212, Loss: 3.9552\n",
      "Batch 213, Loss: 4.1209\n",
      "Batch 214, Loss: 4.0226\n",
      "Batch 215, Loss: 4.1019\n",
      "Batch 216, Loss: 3.9737\n",
      "Batch 217, Loss: 3.9630\n",
      "Batch 218, Loss: 3.9073\n",
      "Batch 219, Loss: 3.9593\n",
      "Batch 220, Loss: 4.0101\n",
      "Batch 221, Loss: 4.0340\n",
      "Batch 222, Loss: 3.9767\n",
      "Batch 223, Loss: 3.8692\n",
      "Batch 224, Loss: 4.0508\n",
      "Batch 225, Loss: 4.0601\n",
      "Batch 226, Loss: 4.0637\n",
      "Batch 227, Loss: 3.8642\n",
      "Batch 228, Loss: 3.9959\n",
      "Batch 229, Loss: 4.0061\n",
      "Batch 230, Loss: 3.9266\n",
      "Batch 231, Loss: 3.9905\n",
      "Batch 232, Loss: 3.8802\n",
      "Batch 233, Loss: 3.9226\n",
      "Batch 234, Loss: 3.9285\n",
      "Batch 235, Loss: 3.9413\n",
      "Batch 236, Loss: 3.9546\n",
      "Batch 237, Loss: 3.8901\n",
      "Batch 238, Loss: 3.8277\n",
      "Batch 239, Loss: 3.9203\n",
      "Batch 240, Loss: 3.9671\n",
      "Batch 241, Loss: 3.9397\n",
      "Batch 242, Loss: 3.9663\n",
      "Batch 243, Loss: 3.8358\n",
      "Batch 244, Loss: 3.9634\n",
      "Batch 245, Loss: 3.9536\n",
      "Batch 246, Loss: 3.8916\n",
      "Batch 247, Loss: 3.8576\n",
      "Batch 248, Loss: 3.9603\n",
      "Batch 249, Loss: 3.8080\n",
      "Batch 250, Loss: 3.8759\n",
      "Batch 251, Loss: 3.8805\n",
      "Batch 252, Loss: 3.9644\n",
      "Batch 253, Loss: 3.8486\n",
      "Batch 254, Loss: 3.8788\n",
      "Batch 255, Loss: 3.7875\n",
      "Batch 256, Loss: 3.8980\n",
      "Batch 257, Loss: 3.8717\n",
      "Batch 258, Loss: 3.9222\n",
      "Batch 259, Loss: 3.8327\n",
      "Batch 260, Loss: 3.7969\n",
      "Batch 261, Loss: 3.8742\n",
      "Batch 262, Loss: 3.7823\n",
      "Batch 263, Loss: 3.8584\n",
      "Batch 264, Loss: 3.8666\n",
      "Batch 265, Loss: 3.7880\n",
      "Batch 266, Loss: 3.7890\n",
      "Batch 267, Loss: 3.8711\n",
      "Batch 268, Loss: 3.8568\n",
      "Batch 269, Loss: 3.8128\n",
      "Batch 270, Loss: 3.7303\n",
      "Batch 271, Loss: 3.9093\n",
      "Batch 272, Loss: 3.7801\n",
      "Batch 273, Loss: 3.6861\n",
      "Batch 274, Loss: 3.7889\n",
      "Batch 275, Loss: 3.6304\n",
      "Batch 276, Loss: 3.8290\n",
      "Batch 277, Loss: 3.8039\n",
      "Batch 278, Loss: 3.7473\n",
      "Batch 279, Loss: 3.7499\n",
      "Batch 280, Loss: 3.7136\n",
      "Batch 281, Loss: 3.7833\n",
      "Batch 282, Loss: 3.7686\n",
      "Batch 283, Loss: 3.9309\n",
      "Epoch 1, Average Loss: 4.8442\n",
      "Once upon a pogest bused his room. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 3.8656\n",
      "Batch 2, Loss: 3.8513\n",
      "Batch 3, Loss: 3.7023\n",
      "Batch 4, Loss: 3.7869\n",
      "Batch 5, Loss: 3.8490\n",
      "Batch 6, Loss: 3.6930\n",
      "Batch 7, Loss: 3.8489\n",
      "Batch 8, Loss: 3.8355\n",
      "Batch 9, Loss: 3.7501\n",
      "Batch 10, Loss: 3.8452\n",
      "Batch 11, Loss: 3.8681\n",
      "Batch 12, Loss: 3.7747\n",
      "Batch 13, Loss: 3.7190\n",
      "Batch 14, Loss: 3.7230\n",
      "Batch 15, Loss: 3.7107\n",
      "Batch 16, Loss: 3.7045\n",
      "Batch 17, Loss: 3.7140\n",
      "Batch 18, Loss: 3.6889\n",
      "Batch 19, Loss: 3.6667\n",
      "Batch 20, Loss: 3.6441\n",
      "Batch 21, Loss: 3.6504\n",
      "Batch 22, Loss: 3.6001\n",
      "Batch 23, Loss: 3.8112\n",
      "Batch 24, Loss: 3.6659\n",
      "Batch 25, Loss: 3.6056\n",
      "Batch 26, Loss: 3.7456\n",
      "Batch 27, Loss: 3.6501\n",
      "Batch 28, Loss: 3.7446\n",
      "Batch 29, Loss: 3.7106\n",
      "Batch 30, Loss: 3.6055\n",
      "Batch 31, Loss: 3.7978\n",
      "Batch 32, Loss: 3.7572\n",
      "Batch 33, Loss: 3.8054\n",
      "Batch 34, Loss: 3.6822\n",
      "Batch 35, Loss: 3.6673\n",
      "Batch 36, Loss: 3.6360\n",
      "Batch 37, Loss: 3.6681\n",
      "Batch 38, Loss: 3.8049\n",
      "Batch 39, Loss: 3.7886\n",
      "Batch 40, Loss: 3.7257\n",
      "Batch 41, Loss: 3.6704\n",
      "Batch 42, Loss: 3.7284\n",
      "Batch 43, Loss: 3.7611\n",
      "Batch 44, Loss: 3.7290\n",
      "Batch 45, Loss: 3.6793\n",
      "Batch 46, Loss: 3.7390\n",
      "Batch 47, Loss: 3.6922\n",
      "Batch 48, Loss: 3.6989\n",
      "Batch 49, Loss: 3.6930\n",
      "Batch 50, Loss: 3.7096\n",
      "Batch 51, Loss: 3.6733\n",
      "Batch 52, Loss: 3.6293\n",
      "Batch 53, Loss: 3.6980\n",
      "Batch 54, Loss: 3.6256\n",
      "Batch 55, Loss: 3.6740\n",
      "Batch 56, Loss: 3.5739\n",
      "Batch 57, Loss: 3.6537\n",
      "Batch 58, Loss: 3.6427\n",
      "Batch 59, Loss: 3.7203\n",
      "Batch 60, Loss: 3.6565\n",
      "Batch 61, Loss: 3.6127\n",
      "Batch 62, Loss: 3.6554\n",
      "Batch 63, Loss: 3.6384\n",
      "Batch 64, Loss: 3.5878\n",
      "Batch 65, Loss: 3.7838\n",
      "Batch 66, Loss: 3.6946\n",
      "Batch 67, Loss: 3.7049\n",
      "Batch 68, Loss: 3.6297\n",
      "Batch 69, Loss: 3.5585\n",
      "Batch 70, Loss: 3.6400\n",
      "Batch 71, Loss: 3.5576\n",
      "Batch 72, Loss: 3.6130\n",
      "Batch 73, Loss: 3.6745\n",
      "Batch 74, Loss: 3.5522\n",
      "Batch 75, Loss: 3.8140\n",
      "Batch 76, Loss: 3.7236\n",
      "Batch 77, Loss: 3.5946\n",
      "Batch 78, Loss: 3.6035\n",
      "Batch 79, Loss: 3.5913\n",
      "Batch 80, Loss: 3.6814\n",
      "Batch 81, Loss: 3.6287\n",
      "Batch 82, Loss: 3.6162\n",
      "Batch 83, Loss: 3.5832\n",
      "Batch 84, Loss: 3.6799\n",
      "Batch 85, Loss: 3.5801\n",
      "Batch 86, Loss: 3.5903\n",
      "Batch 87, Loss: 3.6998\n",
      "Batch 88, Loss: 3.5284\n",
      "Batch 89, Loss: 3.6325\n",
      "Batch 90, Loss: 3.5380\n",
      "Batch 91, Loss: 3.5752\n",
      "Batch 92, Loss: 3.7166\n",
      "Batch 93, Loss: 3.5499\n",
      "Batch 94, Loss: 3.5781\n",
      "Batch 95, Loss: 3.4461\n",
      "Batch 96, Loss: 3.6089\n",
      "Batch 97, Loss: 3.6608\n",
      "Batch 98, Loss: 3.5095\n",
      "Batch 99, Loss: 3.5179\n",
      "Batch 100, Loss: 3.4892\n",
      "Batch 101, Loss: 3.4466\n",
      "Batch 102, Loss: 3.6696\n",
      "Batch 103, Loss: 3.6342\n",
      "Batch 104, Loss: 3.6515\n",
      "Batch 105, Loss: 3.6657\n",
      "Batch 106, Loss: 3.5673\n",
      "Batch 107, Loss: 3.6098\n",
      "Batch 108, Loss: 3.6068\n",
      "Batch 109, Loss: 3.5856\n",
      "Batch 110, Loss: 3.4883\n",
      "Batch 111, Loss: 3.5997\n",
      "Batch 112, Loss: 3.6246\n",
      "Batch 113, Loss: 3.5988\n",
      "Batch 114, Loss: 3.5843\n",
      "Batch 115, Loss: 3.6238\n",
      "Batch 116, Loss: 3.5057\n",
      "Batch 117, Loss: 3.6457\n",
      "Batch 118, Loss: 3.5747\n",
      "Batch 119, Loss: 3.4683\n",
      "Batch 120, Loss: 3.5733\n",
      "Batch 121, Loss: 3.5650\n",
      "Batch 122, Loss: 3.5556\n",
      "Batch 123, Loss: 3.6203\n",
      "Batch 124, Loss: 3.6287\n",
      "Batch 125, Loss: 3.5696\n",
      "Batch 126, Loss: 3.5481\n",
      "Batch 127, Loss: 3.6140\n",
      "Batch 128, Loss: 3.4418\n",
      "Batch 129, Loss: 3.6018\n",
      "Batch 130, Loss: 3.5227\n",
      "Batch 131, Loss: 3.5765\n",
      "Batch 132, Loss: 3.5260\n",
      "Batch 133, Loss: 3.4727\n",
      "Batch 134, Loss: 3.4584\n",
      "Batch 135, Loss: 3.6404\n",
      "Batch 136, Loss: 3.5967\n",
      "Batch 137, Loss: 3.5049\n",
      "Batch 138, Loss: 3.6228\n",
      "Batch 139, Loss: 3.4320\n",
      "Batch 140, Loss: 3.4375\n",
      "Batch 141, Loss: 3.5679\n",
      "Batch 142, Loss: 3.3700\n",
      "Batch 143, Loss: 3.6392\n",
      "Batch 144, Loss: 3.5256\n",
      "Batch 145, Loss: 3.5385\n",
      "Batch 146, Loss: 3.4608\n",
      "Batch 147, Loss: 3.5817\n",
      "Batch 148, Loss: 3.5262\n",
      "Batch 149, Loss: 3.4540\n",
      "Batch 150, Loss: 3.5428\n",
      "Batch 151, Loss: 3.5310\n",
      "Batch 152, Loss: 3.4779\n",
      "Batch 153, Loss: 3.4954\n",
      "Batch 154, Loss: 3.5358\n",
      "Batch 155, Loss: 3.4757\n",
      "Batch 156, Loss: 3.4538\n",
      "Batch 157, Loss: 3.5103\n",
      "Batch 158, Loss: 3.5279\n",
      "Batch 159, Loss: 3.3905\n",
      "Batch 160, Loss: 3.4741\n",
      "Batch 161, Loss: 3.5383\n",
      "Batch 162, Loss: 3.4373\n",
      "Batch 163, Loss: 3.4348\n",
      "Batch 164, Loss: 3.4287\n",
      "Batch 165, Loss: 3.5522\n",
      "Batch 166, Loss: 3.3530\n",
      "Batch 167, Loss: 3.5233\n",
      "Batch 168, Loss: 3.3541\n",
      "Batch 169, Loss: 3.4481\n",
      "Batch 170, Loss: 3.4918\n",
      "Batch 171, Loss: 3.5959\n",
      "Batch 172, Loss: 3.5178\n",
      "Batch 173, Loss: 3.4655\n",
      "Batch 174, Loss: 3.5087\n",
      "Batch 175, Loss: 3.4887\n",
      "Batch 176, Loss: 3.4570\n",
      "Batch 177, Loss: 3.4018\n",
      "Batch 178, Loss: 3.5515\n",
      "Batch 179, Loss: 3.4428\n",
      "Batch 180, Loss: 3.4038\n",
      "Batch 181, Loss: 3.5224\n",
      "Batch 182, Loss: 3.5268\n",
      "Batch 183, Loss: 3.5483\n",
      "Batch 184, Loss: 3.4052\n",
      "Batch 185, Loss: 3.3785\n",
      "Batch 186, Loss: 3.5571\n",
      "Batch 187, Loss: 3.4147\n",
      "Batch 188, Loss: 3.3721\n",
      "Batch 189, Loss: 3.5040\n",
      "Batch 190, Loss: 3.5477\n",
      "Batch 191, Loss: 3.4025\n",
      "Batch 192, Loss: 3.4113\n",
      "Batch 193, Loss: 3.4575\n",
      "Batch 194, Loss: 3.4917\n",
      "Batch 195, Loss: 3.4612\n",
      "Batch 196, Loss: 3.3707\n",
      "Batch 197, Loss: 3.4059\n",
      "Batch 198, Loss: 3.5245\n",
      "Batch 199, Loss: 3.4951\n",
      "Batch 200, Loss: 3.4530\n",
      "Jen started to cry. ind- was prore, he felt very bed. From that the little why opened it. Suddenly, the mimplill. Bob was a big and rivering! With putting on \n",
      "the shell. 3 yell he looked so lation that it knew he could go together. The vorio fewer and John admers and flew around the reled. â€œ food let's do \n",
      "thatâ€™s!â€ Mia went to her pillag a looks. They worked and replied, then she years on a big yarn. The sun and smiled and said, Timmy: \"You're?\" Let's go touch \n",
      "a surprisent so ears together and put on the rain. Sver came to feel better said: \"Mommy, you irss it?\" And'depped around the fullion smiled and sle The water holessed \n",
      "the water and strong. They all of fun he wouldn't fight. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 14.78 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 3.5126\n",
      "Batch 202, Loss: 3.3948\n",
      "Batch 203, Loss: 3.3824\n",
      "Batch 204, Loss: 3.4166\n",
      "Batch 205, Loss: 3.4449\n",
      "Batch 206, Loss: 3.4483\n",
      "Batch 207, Loss: 3.4338\n",
      "Batch 208, Loss: 3.4839\n",
      "Batch 209, Loss: 3.4917\n",
      "Batch 210, Loss: 3.4593\n",
      "Batch 211, Loss: 3.3967\n",
      "Batch 212, Loss: 3.4128\n",
      "Batch 213, Loss: 3.3898\n",
      "Batch 214, Loss: 3.3606\n",
      "Batch 215, Loss: 3.3922\n",
      "Batch 216, Loss: 3.3278\n",
      "Batch 217, Loss: 3.3362\n",
      "Batch 218, Loss: 3.4769\n",
      "Batch 219, Loss: 3.4823\n",
      "Batch 220, Loss: 3.3629\n",
      "Batch 221, Loss: 3.3982\n",
      "Batch 222, Loss: 3.4367\n",
      "Batch 223, Loss: 3.4082\n",
      "Batch 224, Loss: 3.4455\n",
      "Batch 225, Loss: 3.4433\n",
      "Batch 226, Loss: 3.4925\n",
      "Batch 227, Loss: 3.3810\n",
      "Batch 228, Loss: 3.4763\n",
      "Batch 229, Loss: 3.3313\n",
      "Batch 230, Loss: 3.4113\n",
      "Batch 231, Loss: 3.4759\n",
      "Batch 232, Loss: 3.3328\n",
      "Batch 233, Loss: 3.4115\n",
      "Batch 234, Loss: 3.4497\n",
      "Batch 235, Loss: 3.2832\n",
      "Batch 236, Loss: 3.2872\n",
      "Batch 237, Loss: 3.3640\n",
      "Batch 238, Loss: 3.3032\n",
      "Batch 239, Loss: 3.3751\n",
      "Batch 240, Loss: 3.3041\n",
      "Batch 241, Loss: 3.4294\n",
      "Batch 242, Loss: 3.3251\n",
      "Batch 243, Loss: 3.3257\n",
      "Batch 244, Loss: 3.3906\n",
      "Batch 245, Loss: 3.3748\n",
      "Batch 246, Loss: 3.4206\n",
      "Batch 247, Loss: 3.3258\n",
      "Batch 248, Loss: 3.4207\n",
      "Batch 249, Loss: 3.4364\n",
      "Batch 250, Loss: 3.2893\n",
      "Batch 251, Loss: 3.3663\n",
      "Batch 252, Loss: 3.4035\n",
      "Batch 253, Loss: 3.3487\n",
      "Batch 254, Loss: 3.4611\n",
      "Batch 255, Loss: 3.3415\n",
      "Batch 256, Loss: 3.3813\n",
      "Batch 257, Loss: 3.3083\n",
      "Batch 258, Loss: 3.2633\n",
      "Batch 259, Loss: 3.3433\n",
      "Batch 260, Loss: 3.3232\n",
      "Batch 261, Loss: 3.3986\n",
      "Batch 262, Loss: 3.2575\n",
      "Batch 263, Loss: 3.2406\n",
      "Batch 264, Loss: 3.3071\n",
      "Batch 265, Loss: 3.4595\n",
      "Batch 266, Loss: 3.2317\n",
      "Batch 267, Loss: 3.1999\n",
      "Batch 268, Loss: 3.2454\n",
      "Batch 269, Loss: 3.3488\n",
      "Batch 270, Loss: 3.3453\n",
      "Batch 271, Loss: 3.2492\n",
      "Batch 272, Loss: 3.2598\n",
      "Batch 273, Loss: 3.4076\n",
      "Batch 274, Loss: 3.1816\n",
      "Batch 275, Loss: 3.3925\n",
      "Batch 276, Loss: 3.3044\n",
      "Batch 277, Loss: 3.2773\n",
      "Batch 278, Loss: 3.3353\n",
      "Batch 279, Loss: 3.3172\n",
      "Batch 280, Loss: 3.4227\n",
      "Batch 281, Loss: 3.2585\n",
      "Batch 282, Loss: 3.2244\n",
      "Batch 283, Loss: 3.3467\n",
      "Batch 284, Loss: 3.3532\n",
      "Batch 285, Loss: 3.2747\n",
      "Batch 286, Loss: 3.2460\n",
      "Batch 287, Loss: 3.2051\n",
      "Batch 288, Loss: 3.3945\n",
      "Batch 289, Loss: 3.3039\n",
      "Batch 290, Loss: 3.3576\n",
      "Batch 291, Loss: 3.1616\n",
      "Batch 292, Loss: 3.3223\n",
      "Batch 293, Loss: 3.1904\n",
      "Batch 294, Loss: 3.3748\n",
      "Batch 295, Loss: 3.2160\n",
      "Batch 296, Loss: 3.1947\n",
      "Batch 297, Loss: 3.1951\n",
      "Batch 298, Loss: 3.4049\n",
      "Batch 299, Loss: 3.3644\n",
      "Batch 300, Loss: 3.2788\n",
      "Batch 301, Loss: 3.2648\n",
      "Batch 302, Loss: 3.2980\n",
      "Batch 303, Loss: 3.3104\n",
      "Batch 304, Loss: 3.2295\n",
      "Batch 305, Loss: 3.1867\n",
      "Batch 306, Loss: 3.1808\n",
      "Batch 307, Loss: 3.3105\n",
      "Batch 308, Loss: 3.2731\n",
      "Batch 309, Loss: 3.1679\n",
      "Batch 310, Loss: 3.2642\n",
      "Batch 311, Loss: 3.3695\n",
      "Batch 312, Loss: 3.2751\n",
      "Batch 313, Loss: 3.2135\n",
      "Batch 314, Loss: 3.2302\n",
      "Batch 315, Loss: 3.2198\n",
      "Batch 316, Loss: 3.3211\n",
      "Batch 317, Loss: 3.3461\n",
      "Batch 318, Loss: 3.3947\n",
      "Batch 319, Loss: 3.2565\n",
      "Batch 320, Loss: 3.2729\n",
      "Batch 321, Loss: 3.3795\n",
      "Batch 322, Loss: 3.2911\n",
      "Batch 323, Loss: 3.2024\n",
      "Batch 324, Loss: 3.4014\n",
      "Batch 325, Loss: 3.1433\n",
      "Batch 326, Loss: 3.1393\n",
      "Batch 327, Loss: 3.1465\n",
      "Batch 328, Loss: 3.1850\n",
      "Batch 329, Loss: 3.2201\n",
      "Batch 330, Loss: 3.1668\n",
      "Batch 331, Loss: 3.2541\n",
      "Batch 332, Loss: 3.3452\n",
      "Batch 333, Loss: 3.2399\n",
      "Batch 334, Loss: 3.1168\n",
      "Batch 335, Loss: 3.3127\n",
      "Batch 336, Loss: 3.1923\n",
      "Batch 337, Loss: 3.3179\n",
      "Batch 338, Loss: 3.2893\n",
      "Batch 339, Loss: 3.2012\n",
      "Batch 340, Loss: 3.1960\n",
      "Batch 341, Loss: 3.2248\n",
      "Batch 342, Loss: 3.1393\n",
      "Batch 343, Loss: 3.2911\n",
      "Batch 344, Loss: 3.2078\n",
      "Batch 345, Loss: 3.2400\n",
      "Batch 346, Loss: 3.0953\n",
      "Batch 347, Loss: 3.3081\n",
      "Batch 348, Loss: 3.2094\n",
      "Batch 349, Loss: 3.1702\n",
      "Batch 350, Loss: 3.2227\n",
      "Batch 351, Loss: 3.2934\n",
      "Batch 352, Loss: 3.2741\n",
      "Batch 353, Loss: 3.1876\n",
      "Batch 354, Loss: 3.2021\n",
      "Batch 355, Loss: 3.1430\n",
      "Batch 356, Loss: 3.2674\n",
      "Batch 357, Loss: 3.0797\n",
      "Batch 358, Loss: 3.1341\n",
      "Batch 359, Loss: 3.0716\n",
      "Batch 360, Loss: 3.1575\n",
      "Batch 361, Loss: 3.2577\n",
      "Batch 362, Loss: 3.2662\n",
      "Batch 363, Loss: 3.1070\n",
      "Batch 364, Loss: 3.3176\n",
      "Batch 365, Loss: 3.0954\n",
      "Batch 366, Loss: 3.1959\n",
      "Batch 367, Loss: 3.0577\n",
      "Batch 368, Loss: 3.3201\n",
      "Batch 369, Loss: 3.1859\n",
      "Batch 370, Loss: 3.0673\n",
      "Batch 371, Loss: 3.2779\n",
      "Batch 372, Loss: 3.1957\n",
      "Batch 373, Loss: 3.2272\n",
      "Batch 374, Loss: 3.2032\n",
      "Batch 375, Loss: 3.1350\n",
      "Batch 376, Loss: 3.1428\n",
      "Batch 377, Loss: 3.2690\n",
      "Batch 378, Loss: 3.1748\n",
      "Batch 379, Loss: 3.2317\n",
      "Batch 380, Loss: 3.1368\n",
      "Batch 381, Loss: 3.0190\n",
      "Batch 382, Loss: 3.0988\n",
      "Batch 383, Loss: 3.1706\n",
      "Batch 384, Loss: 3.1452\n",
      "Batch 385, Loss: 3.1503\n",
      "Batch 386, Loss: 2.9815\n",
      "Batch 387, Loss: 3.0810\n",
      "Batch 388, Loss: 3.2711\n",
      "Batch 389, Loss: 3.1984\n",
      "Batch 390, Loss: 3.2047\n",
      "Batch 391, Loss: 3.0946\n",
      "Batch 392, Loss: 3.0841\n",
      "Batch 393, Loss: 3.2959\n",
      "Batch 394, Loss: 3.1543\n",
      "Batch 395, Loss: 3.1838\n",
      "Batch 396, Loss: 3.1917\n",
      "Batch 397, Loss: 3.1300\n",
      "Batch 398, Loss: 3.2164\n",
      "Batch 399, Loss: 3.0118\n",
      "Batch 400, Loss: 3.1308\n",
      "One day, This time, there was a chick tally. Theree stared and grabbed a stir on the carkle. It dreamed up may down. Ted around and couldn't find a. It \n",
      "looked out he saw what to go in the big placheations. When held in a standy onion, she still beenside there was a small blanket dream and put on the \n",
      "trees. As to the sky the leaf. Sam was so restleepse. He looked around in the table and that he lated in it, but the bird had dise. Suddenly, the \n",
      "prokey heard a new leaianted in the quiauts shining kind at high in and played with a treass. He wanted to follow his stre longers and he tried to eat \n",
      "the kind jarmiving clap and soon went out to the he didn't know what it ever with a roll. When her mommy noticed a hunny no metleread. It could see \n",
      "what she could. Tilly's friendm and enjoyed the ben had a big feary was a hugek for a long time nearby tin. Timmy was even loud and never think the \n",
      "pipss, cries. Timmy started to take the vancean was sad because helding a big tom! After they were so sad because they were an earra careful and they had a \n",
      "good ruffletoland that day on, Timmy loved playing with his bassicalash up! The dog and his faczards, Timmy was too and other -it in the teacher went back \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 12.29 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 401, Loss: 3.0675\n",
      "Batch 402, Loss: 3.1943\n",
      "Batch 403, Loss: 3.2358\n",
      "Batch 404, Loss: 3.1440\n",
      "Batch 405, Loss: 3.2063\n",
      "Batch 406, Loss: 3.0768\n",
      "Batch 407, Loss: 3.1907\n",
      "Batch 408, Loss: 3.1006\n",
      "Batch 409, Loss: 3.0113\n",
      "Batch 410, Loss: 3.0584\n",
      "Batch 411, Loss: 3.0763\n",
      "Batch 412, Loss: 3.0330\n",
      "Batch 413, Loss: 3.1598\n",
      "Batch 414, Loss: 3.0243\n",
      "Batch 415, Loss: 3.1895\n",
      "Batch 416, Loss: 3.0251\n",
      "Batch 417, Loss: 3.0369\n",
      "Batch 418, Loss: 3.1741\n",
      "Epoch 2, Average Loss: 3.4237\n",
      ". And don't be knew waits down all about talke. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 3.1376\n",
      "Batch 2, Loss: 3.1105\n",
      "Batch 3, Loss: 3.2455\n",
      "Batch 4, Loss: 3.0604\n",
      "Batch 5, Loss: 3.1637\n",
      "Batch 6, Loss: 2.9914\n",
      "Batch 7, Loss: 3.1194\n",
      "Batch 8, Loss: 3.2057\n",
      "Batch 9, Loss: 3.2291\n",
      "Batch 10, Loss: 3.1752\n",
      "Batch 11, Loss: 2.9737\n",
      "Batch 12, Loss: 3.0482\n",
      "Batch 13, Loss: 3.1196\n",
      "Batch 14, Loss: 3.0705\n",
      "Batch 15, Loss: 3.0805\n",
      "Batch 16, Loss: 3.2059\n",
      "Batch 17, Loss: 3.0692\n",
      "Batch 18, Loss: 3.1110\n",
      "Batch 19, Loss: 3.1928\n",
      "Batch 20, Loss: 3.1614\n",
      "Batch 21, Loss: 3.0670\n",
      "Batch 22, Loss: 3.2121\n",
      "Batch 23, Loss: 3.0997\n",
      "Batch 24, Loss: 3.0726\n",
      "Batch 25, Loss: 3.0371\n",
      "Batch 26, Loss: 2.9916\n",
      "Batch 27, Loss: 3.0184\n",
      "Batch 28, Loss: 2.9374\n",
      "Batch 29, Loss: 3.1003\n",
      "Batch 30, Loss: 3.0957\n",
      "Batch 31, Loss: 3.0250\n",
      "Batch 32, Loss: 3.0933\n",
      "Batch 33, Loss: 3.0725\n",
      "Batch 34, Loss: 2.9593\n",
      "Batch 35, Loss: 2.9807\n",
      "Batch 36, Loss: 2.8848\n",
      "Batch 37, Loss: 3.0704\n",
      "Batch 38, Loss: 3.0251\n",
      "Batch 39, Loss: 3.0654\n",
      "Batch 40, Loss: 3.1163\n",
      "Batch 41, Loss: 3.0627\n",
      "Batch 42, Loss: 2.9884\n",
      "Batch 43, Loss: 2.9121\n",
      "Batch 44, Loss: 2.8872\n",
      "Batch 45, Loss: 2.9298\n",
      "Batch 46, Loss: 2.9238\n",
      "Batch 47, Loss: 3.0870\n",
      "Batch 48, Loss: 3.1143\n",
      "Batch 49, Loss: 2.9934\n",
      "Batch 50, Loss: 2.9248\n",
      "Batch 51, Loss: 3.2197\n",
      "Batch 52, Loss: 3.0734\n",
      "Batch 53, Loss: 3.0255\n",
      "Batch 54, Loss: 3.0369\n",
      "Batch 55, Loss: 3.0603\n",
      "Batch 56, Loss: 2.9196\n",
      "Batch 57, Loss: 3.2177\n",
      "Batch 58, Loss: 3.0812\n",
      "Batch 59, Loss: 3.0194\n",
      "Batch 60, Loss: 3.0901\n",
      "Batch 61, Loss: 3.0430\n",
      "Batch 62, Loss: 2.8808\n",
      "Batch 63, Loss: 3.0440\n",
      "Batch 64, Loss: 3.0378\n",
      "Batch 65, Loss: 3.0989\n",
      "Batch 66, Loss: 2.8711\n",
      "Batch 67, Loss: 3.0818\n",
      "Batch 68, Loss: 2.9589\n",
      "Batch 69, Loss: 3.1053\n",
      "Batch 70, Loss: 3.0731\n",
      "Batch 71, Loss: 3.0014\n",
      "Batch 72, Loss: 2.8733\n",
      "Batch 73, Loss: 3.0934\n",
      "Batch 74, Loss: 3.1778\n",
      "Batch 75, Loss: 3.0838\n",
      "Batch 76, Loss: 3.0577\n",
      "Batch 77, Loss: 3.0765\n",
      "Batch 78, Loss: 3.0542\n",
      "Batch 79, Loss: 3.0252\n",
      "Batch 80, Loss: 3.0954\n",
      "Batch 81, Loss: 2.9785\n",
      "Batch 82, Loss: 2.9598\n",
      "Batch 83, Loss: 3.0113\n",
      "Batch 84, Loss: 2.8866\n",
      "Batch 85, Loss: 3.0749\n",
      "Batch 86, Loss: 2.9536\n",
      "Batch 87, Loss: 2.8986\n",
      "Batch 88, Loss: 2.9897\n",
      "Batch 89, Loss: 2.9997\n",
      "Batch 90, Loss: 2.9455\n",
      "Batch 91, Loss: 2.9239\n",
      "Batch 92, Loss: 2.9736\n",
      "Batch 93, Loss: 3.0169\n",
      "Batch 94, Loss: 3.0638\n",
      "Batch 95, Loss: 2.9501\n",
      "Batch 96, Loss: 3.1170\n",
      "Batch 97, Loss: 2.9321\n",
      "Batch 98, Loss: 3.0035\n",
      "Batch 99, Loss: 3.1045\n",
      "Batch 100, Loss: 2.9717\n",
      "Batch 101, Loss: 2.8236\n",
      "Batch 102, Loss: 2.9260\n",
      "Batch 103, Loss: 2.9426\n",
      "Batch 104, Loss: 2.9006\n",
      "Batch 105, Loss: 3.0949\n",
      "Batch 106, Loss: 2.9635\n",
      "Batch 107, Loss: 2.9379\n",
      "Batch 108, Loss: 2.8149\n",
      "Batch 109, Loss: 2.9986\n",
      "Batch 110, Loss: 3.0560\n",
      "Batch 111, Loss: 3.0884\n",
      "Batch 112, Loss: 2.9386\n",
      "Batch 113, Loss: 2.8678\n",
      "Batch 114, Loss: 3.0357\n",
      "Batch 115, Loss: 3.0398\n",
      "Batch 116, Loss: 3.0343\n",
      "Batch 117, Loss: 2.8528\n",
      "Batch 118, Loss: 2.9657\n",
      "Batch 119, Loss: 3.0157\n",
      "Batch 120, Loss: 3.0236\n",
      "Batch 121, Loss: 2.8640\n",
      "Batch 122, Loss: 2.9370\n",
      "Batch 123, Loss: 2.8627\n",
      "Batch 124, Loss: 2.9828\n",
      "Batch 125, Loss: 2.9306\n",
      "Batch 126, Loss: 2.8690\n",
      "Batch 127, Loss: 2.8869\n",
      "Batch 128, Loss: 2.8373\n",
      "Batch 129, Loss: 2.8995\n",
      "Batch 130, Loss: 2.8447\n",
      "Batch 131, Loss: 3.0452\n",
      "Batch 132, Loss: 2.9133\n",
      "Batch 133, Loss: 2.8112\n",
      "Batch 134, Loss: 2.8260\n",
      "Batch 135, Loss: 2.9485\n",
      "Batch 136, Loss: 2.9900\n",
      "Batch 137, Loss: 3.0303\n",
      "Batch 138, Loss: 2.9842\n",
      "Batch 139, Loss: 2.8157\n",
      "Batch 140, Loss: 2.9716\n",
      "Batch 141, Loss: 2.9523\n",
      "Batch 142, Loss: 3.0001\n",
      "Batch 143, Loss: 2.9096\n",
      "Batch 144, Loss: 2.9406\n",
      "Batch 145, Loss: 3.0187\n",
      "Batch 146, Loss: 2.9634\n",
      "Batch 147, Loss: 3.0007\n",
      "Batch 148, Loss: 2.8710\n",
      "Batch 149, Loss: 3.0040\n",
      "Batch 150, Loss: 2.9908\n",
      "Batch 151, Loss: 2.9958\n",
      "Batch 152, Loss: 2.7777\n",
      "Batch 153, Loss: 3.0465\n",
      "Batch 154, Loss: 2.8940\n",
      "Batch 155, Loss: 2.9676\n",
      "Batch 156, Loss: 3.0033\n",
      "Batch 157, Loss: 2.8726\n",
      "Batch 158, Loss: 3.0655\n",
      "Batch 159, Loss: 2.8559\n",
      "Batch 160, Loss: 2.9867\n",
      "Batch 161, Loss: 2.7848\n",
      "Batch 162, Loss: 3.0335\n",
      "Batch 163, Loss: 2.9139\n",
      "Batch 164, Loss: 2.9096\n",
      "Batch 165, Loss: 2.7763\n",
      "Batch 166, Loss: 2.9326\n",
      "Batch 167, Loss: 2.9060\n",
      "Batch 168, Loss: 2.9003\n",
      "Batch 169, Loss: 2.8362\n",
      "Batch 170, Loss: 2.9990\n",
      "Batch 171, Loss: 2.9042\n",
      "Batch 172, Loss: 3.0172\n",
      "Batch 173, Loss: 2.9101\n",
      "Batch 174, Loss: 2.9716\n",
      "Batch 175, Loss: 2.7927\n",
      "Batch 176, Loss: 2.8202\n",
      "Batch 177, Loss: 2.8313\n",
      "Batch 178, Loss: 2.8905\n",
      "Batch 179, Loss: 2.9379\n",
      "Batch 180, Loss: 3.0365\n",
      "Batch 181, Loss: 2.8913\n",
      "Batch 182, Loss: 2.8837\n",
      "Batch 183, Loss: 2.9144\n",
      "Batch 184, Loss: 3.0429\n",
      "Batch 185, Loss: 2.9745\n",
      "Batch 186, Loss: 2.9768\n",
      "Batch 187, Loss: 2.8955\n",
      "Batch 188, Loss: 2.8055\n",
      "Batch 189, Loss: 2.8748\n",
      "Batch 190, Loss: 2.9369\n",
      "Batch 191, Loss: 2.9584\n",
      "Batch 192, Loss: 2.8131\n",
      "Batch 193, Loss: 2.9584\n",
      "Batch 194, Loss: 2.8877\n",
      "Batch 195, Loss: 2.7897\n",
      "Batch 196, Loss: 2.8911\n",
      "Batch 197, Loss: 2.7792\n",
      "Batch 198, Loss: 2.8328\n",
      "Batch 199, Loss: 2.9862\n",
      "Batch 200, Loss: 2.8805\n",
      "Once upon a time there was a dog named Timmy. Timmy loved to be at the mess. One day, Timmy said, \"Let's buildre in the cadle vishing his toy car. \n",
      "Timmy learned that ple,\" she said the ball home they were best friends, pretending up. He had the fridge to play chooful schavated that vrossed over, and kept bubowy. Be \n",
      "thought forthiable to dop the dragine gassed up and happily stopped as fast as she watched the eggee lazed to her grandma's way to help her watching the flowers and \n",
      "the day sunsco into the nearbye. When they walked, the stormue, the rainbar, Spot was very angry and clapped up to pee againstles. As that's mom all's oited them in \n",
      "my purs, and her chees letters. It's so much fun and now feeling down! \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 7.61 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.9736\n",
      "Batch 202, Loss: 2.9228\n",
      "Batch 203, Loss: 2.8316\n",
      "Batch 204, Loss: 2.9338\n",
      "Batch 205, Loss: 2.8578\n",
      "Batch 206, Loss: 2.8720\n",
      "Batch 207, Loss: 2.7361\n",
      "Batch 208, Loss: 2.8804\n",
      "Batch 209, Loss: 2.9366\n",
      "Batch 210, Loss: 2.8813\n",
      "Batch 211, Loss: 2.9166\n",
      "Batch 212, Loss: 2.9045\n",
      "Batch 213, Loss: 2.7941\n",
      "Batch 214, Loss: 2.9321\n",
      "Batch 215, Loss: 2.8533\n",
      "Batch 216, Loss: 2.7782\n",
      "Batch 217, Loss: 2.7520\n",
      "Batch 218, Loss: 2.9220\n",
      "Batch 219, Loss: 2.7344\n",
      "Batch 220, Loss: 2.9591\n",
      "Batch 221, Loss: 2.7212\n",
      "Batch 222, Loss: 2.8291\n",
      "Batch 223, Loss: 2.9190\n",
      "Batch 224, Loss: 2.8812\n",
      "Batch 225, Loss: 2.8271\n",
      "Batch 226, Loss: 2.7047\n",
      "Batch 227, Loss: 2.7887\n",
      "Batch 228, Loss: 2.8161\n",
      "Batch 229, Loss: 2.8891\n",
      "Batch 230, Loss: 2.9140\n",
      "Batch 231, Loss: 2.8935\n",
      "Batch 232, Loss: 2.8472\n",
      "Batch 233, Loss: 2.8963\n",
      "Batch 234, Loss: 2.8436\n",
      "Batch 235, Loss: 2.7048\n",
      "Batch 236, Loss: 2.7657\n",
      "Batch 237, Loss: 2.7941\n",
      "Batch 238, Loss: 2.8696\n",
      "Batch 239, Loss: 2.7275\n",
      "Batch 240, Loss: 2.6872\n",
      "Batch 241, Loss: 2.7176\n",
      "Batch 242, Loss: 2.8443\n",
      "Batch 243, Loss: 2.7841\n",
      "Batch 244, Loss: 2.8575\n",
      "Batch 245, Loss: 2.8169\n",
      "Batch 246, Loss: 2.7739\n",
      "Batch 247, Loss: 2.9545\n",
      "Batch 248, Loss: 2.9293\n",
      "Batch 249, Loss: 2.8386\n",
      "Batch 250, Loss: 2.8844\n",
      "Batch 251, Loss: 2.7213\n",
      "Batch 252, Loss: 2.6578\n",
      "Batch 253, Loss: 2.8058\n",
      "Batch 254, Loss: 2.6789\n",
      "Batch 255, Loss: 2.9135\n",
      "Batch 256, Loss: 2.9488\n",
      "Batch 257, Loss: 2.8504\n",
      "Batch 258, Loss: 2.8970\n",
      "Batch 259, Loss: 2.7804\n",
      "Batch 260, Loss: 2.7739\n",
      "Batch 261, Loss: 2.9263\n",
      "Batch 262, Loss: 2.6733\n",
      "Batch 263, Loss: 2.6638\n",
      "Batch 264, Loss: 2.6081\n",
      "Batch 265, Loss: 2.8945\n",
      "Batch 266, Loss: 2.7216\n",
      "Batch 267, Loss: 2.7628\n",
      "Batch 268, Loss: 2.8275\n",
      "Batch 269, Loss: 2.8348\n",
      "Batch 270, Loss: 2.8685\n",
      "Batch 271, Loss: 2.7731\n",
      "Batch 272, Loss: 2.7850\n",
      "Batch 273, Loss: 2.7621\n",
      "Batch 274, Loss: 2.8740\n",
      "Batch 275, Loss: 2.8982\n",
      "Batch 276, Loss: 2.8294\n",
      "Batch 277, Loss: 2.9911\n",
      "Batch 278, Loss: 2.7920\n",
      "Batch 279, Loss: 2.8583\n",
      "Batch 280, Loss: 2.7107\n",
      "Batch 281, Loss: 2.8438\n",
      "Batch 282, Loss: 2.8249\n",
      "Batch 283, Loss: 2.8256\n",
      "Batch 284, Loss: 2.7426\n",
      "Batch 285, Loss: 2.6437\n",
      "Batch 286, Loss: 2.7580\n",
      "Batch 287, Loss: 2.6285\n",
      "Batch 288, Loss: 2.6866\n",
      "Batch 289, Loss: 2.6320\n",
      "Batch 290, Loss: 2.6428\n",
      "Batch 291, Loss: 2.8023\n",
      "Batch 292, Loss: 2.6223\n",
      "Batch 293, Loss: 2.8402\n",
      "Batch 294, Loss: 2.6933\n",
      "Batch 295, Loss: 2.6330\n",
      "Batch 296, Loss: 2.8521\n",
      "Batch 297, Loss: 2.8713\n",
      "Batch 298, Loss: 2.7320\n",
      "Batch 299, Loss: 2.7555\n",
      "Batch 300, Loss: 2.7306\n",
      "Batch 301, Loss: 2.8594\n",
      "Batch 302, Loss: 2.8956\n",
      "Batch 303, Loss: 2.7658\n",
      "Batch 304, Loss: 2.7575\n",
      "Batch 305, Loss: 2.6603\n",
      "Batch 306, Loss: 2.6637\n",
      "Batch 307, Loss: 2.8263\n",
      "Batch 308, Loss: 2.7982\n",
      "Batch 309, Loss: 2.7494\n",
      "Batch 310, Loss: 2.7972\n",
      "Batch 311, Loss: 2.7192\n",
      "Batch 312, Loss: 2.7152\n",
      "Batch 313, Loss: 2.6799\n",
      "Batch 314, Loss: 2.6142\n",
      "Batch 315, Loss: 2.8338\n",
      "Batch 316, Loss: 2.8327\n",
      "Batch 317, Loss: 2.8309\n",
      "Batch 318, Loss: 2.6528\n",
      "Batch 319, Loss: 2.8242\n",
      "Batch 320, Loss: 2.7041\n",
      "Batch 321, Loss: 2.6644\n",
      "Batch 322, Loss: 2.6514\n",
      "Batch 323, Loss: 2.6392\n",
      "Batch 324, Loss: 2.6935\n",
      "Batch 325, Loss: 2.6935\n",
      "Batch 326, Loss: 2.7279\n",
      "Batch 327, Loss: 2.6710\n",
      "Batch 328, Loss: 2.6530\n",
      "Batch 329, Loss: 2.6071\n",
      "Batch 330, Loss: 2.7092\n",
      "Batch 331, Loss: 2.6989\n",
      "Batch 332, Loss: 2.5880\n",
      "Batch 333, Loss: 2.8493\n",
      "Batch 334, Loss: 2.6263\n",
      "Batch 335, Loss: 2.6781\n",
      "Batch 336, Loss: 2.8689\n",
      "Batch 337, Loss: 2.8424\n",
      "Batch 338, Loss: 2.5742\n",
      "Batch 339, Loss: 2.7056\n",
      "Batch 340, Loss: 2.8168\n",
      "Batch 341, Loss: 2.7567\n",
      "Batch 342, Loss: 2.6167\n",
      "Batch 343, Loss: 2.5673\n",
      "Batch 344, Loss: 2.7417\n",
      "Batch 345, Loss: 2.6700\n",
      "Batch 346, Loss: 2.7763\n",
      "Batch 347, Loss: 2.5969\n",
      "Batch 348, Loss: 2.7723\n",
      "Batch 349, Loss: 2.7537\n",
      "Batch 350, Loss: 2.6251\n",
      "Batch 351, Loss: 2.8298\n",
      "Batch 352, Loss: 2.7260\n",
      "Batch 353, Loss: 2.7621\n",
      "Batch 354, Loss: 2.8272\n",
      "Batch 355, Loss: 2.7254\n",
      "Batch 356, Loss: 2.7321\n",
      "Batch 357, Loss: 2.6965\n",
      "Batch 358, Loss: 2.7524\n",
      "Batch 359, Loss: 2.6466\n",
      "Batch 360, Loss: 2.6916\n",
      "Batch 361, Loss: 2.6146\n",
      "Batch 362, Loss: 2.7617\n",
      "Batch 363, Loss: 2.6504\n",
      "Batch 364, Loss: 2.7807\n",
      "Batch 365, Loss: 2.7119\n",
      "Batch 366, Loss: 2.6566\n",
      "Batch 367, Loss: 2.7207\n",
      "Batch 368, Loss: 2.6113\n",
      "Batch 369, Loss: 2.6519\n",
      "Batch 370, Loss: 2.6754\n",
      "Batch 371, Loss: 2.5793\n",
      "Batch 372, Loss: 2.7837\n",
      "Batch 373, Loss: 2.5192\n",
      "Batch 374, Loss: 2.7052\n",
      "Batch 375, Loss: 2.5944\n",
      "Batch 376, Loss: 2.6746\n",
      "Batch 377, Loss: 2.7716\n",
      "Batch 378, Loss: 2.8067\n",
      "Batch 379, Loss: 2.6491\n",
      "Batch 380, Loss: 2.5399\n",
      "Batch 381, Loss: 2.7674\n",
      "Batch 382, Loss: 2.5384\n",
      "Batch 383, Loss: 2.5910\n",
      "Batch 384, Loss: 2.6457\n",
      "Batch 385, Loss: 2.5625\n",
      "Batch 386, Loss: 2.5358\n",
      "Batch 387, Loss: 2.8326\n",
      "Batch 388, Loss: 2.7773\n",
      "Batch 389, Loss: 2.5123\n",
      "Batch 390, Loss: 2.7675\n",
      "Batch 391, Loss: 2.6945\n",
      "Batch 392, Loss: 2.7013\n",
      "Batch 393, Loss: 2.5715\n",
      "Batch 394, Loss: 2.8321\n",
      "Batch 395, Loss: 2.5287\n",
      "Batch 396, Loss: 2.6487\n",
      "Batch 397, Loss: 2.6997\n",
      "Batch 398, Loss: 2.7074\n",
      "Batch 399, Loss: 2.7537\n",
      "Batch 400, Loss: 2.5608\n",
      "Once and Mama got to the squirrel came out. Mama was so happy that he would never been hour any childing. They went outside and searching high in the pillows. \n",
      "And when the end, they noticed what they were flying so pretty! His puppy things were playing out his foot of trees. They played out as pluggles and escomedied \"Are \n",
      "is your wheelcome, my cot and two teama spark sometimes, I can put the a new tube!\" The vaset and games brelish every day to find out the caunder around \n",
      "the time. It said, \"Stop! It is your traine, Tim? What sounds like that is never sharful.\" The next day, askange peppantly. He jumped out, the fake and the waterwital \n",
      "and everyone asks she does it is bory. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 11.84 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 401, Loss: 2.6946\n",
      "Batch 402, Loss: 2.7812\n",
      "Batch 403, Loss: 2.6829\n",
      "Batch 404, Loss: 2.5511\n",
      "Batch 405, Loss: 2.7212\n",
      "Batch 406, Loss: 2.6019\n",
      "Batch 407, Loss: 2.7428\n",
      "Batch 408, Loss: 2.7232\n",
      "Batch 409, Loss: 2.7641\n",
      "Batch 410, Loss: 2.6962\n",
      "Batch 411, Loss: 2.5777\n",
      "Batch 412, Loss: 2.6884\n",
      "Batch 413, Loss: 2.7057\n",
      "Batch 414, Loss: 2.6695\n",
      "Batch 415, Loss: 2.5891\n",
      "Batch 416, Loss: 2.6138\n",
      "Batch 417, Loss: 2.7667\n",
      "Batch 418, Loss: 2.5469\n",
      "Batch 419, Loss: 2.5654\n",
      "Epoch 3, Average Loss: 2.8614\n",
      "One day, End. Her mom came to pick up with a toy knife and took out her hands. She saw a big bird with a dog. She asked Ben the \n",
      "yach boys and showed it some cookies. The dog said, \"Hi and Ben lion now!\" Tom said not friendly. \"It's a great idea!\" Tob, \"That's great cars funny.\" \"Wow about \n",
      "mine in her house?\" Ben asked the cake, coming the tree. He used her on a box of book. It grewhere in the boxlf and the maper. He clapped happily \n",
      "water and diritely after, and someone laughtue. \"What are wely,\" Lily said. \"No, no, makes!\" Ben said. \"But you don't do it will want to be mad at the mitter. \n",
      "You are done angry. We were tirty in the book and take the car back.\" \"Thank you, mom,\" Lily says. \"I was very kind to be Spientle caring. You found \n",
      "a dog at Lily and then follow my dolls. And remembered to be kind because I succession and Bill is funny. They thought forgient.\" Ben and Lily go for the \n",
      "doll and the doll in the room. They play until ast bage thing. They say, \"You love cookies with my car!\" dented,ep. They come close. They pretend star race and \n",
      "played together all day. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.6141\n",
      "Batch 2, Loss: 2.7100\n",
      "Batch 3, Loss: 2.7346\n",
      "Batch 4, Loss: 2.7560\n",
      "Batch 5, Loss: 2.5768\n",
      "Batch 6, Loss: 2.6219\n",
      "Batch 7, Loss: 2.6277\n",
      "Batch 8, Loss: 2.5400\n",
      "Batch 9, Loss: 2.7542\n",
      "Batch 10, Loss: 2.5594\n",
      "Batch 11, Loss: 2.5537\n",
      "Batch 12, Loss: 2.5605\n",
      "Batch 13, Loss: 2.7318\n",
      "Batch 14, Loss: 2.4781\n",
      "Batch 15, Loss: 2.5332\n",
      "Batch 16, Loss: 2.6836\n",
      "Batch 17, Loss: 2.7286\n",
      "Batch 18, Loss: 2.6360\n",
      "Batch 19, Loss: 2.5295\n",
      "Batch 20, Loss: 2.6885\n",
      "Batch 21, Loss: 2.5646\n",
      "Batch 22, Loss: 2.4858\n",
      "Batch 23, Loss: 2.5880\n",
      "Batch 24, Loss: 2.5958\n",
      "Batch 25, Loss: 2.5444\n",
      "Batch 26, Loss: 2.6821\n",
      "Batch 27, Loss: 2.6514\n",
      "Batch 28, Loss: 2.6460\n",
      "Batch 29, Loss: 2.4873\n",
      "Batch 30, Loss: 2.5106\n",
      "Batch 31, Loss: 2.4633\n",
      "Batch 32, Loss: 2.7158\n",
      "Batch 33, Loss: 2.6344\n",
      "Batch 34, Loss: 2.5957\n",
      "Batch 35, Loss: 2.7350\n",
      "Batch 36, Loss: 2.6124\n",
      "Batch 37, Loss: 2.5868\n",
      "Batch 38, Loss: 2.6658\n",
      "Batch 39, Loss: 2.5595\n",
      "Batch 40, Loss: 2.6023\n",
      "Batch 41, Loss: 2.5891\n",
      "Batch 42, Loss: 2.5058\n",
      "Batch 43, Loss: 2.5257\n",
      "Batch 44, Loss: 2.5768\n",
      "Batch 45, Loss: 2.5502\n",
      "Batch 46, Loss: 2.5567\n",
      "Batch 47, Loss: 2.6518\n",
      "Batch 48, Loss: 2.5796\n",
      "Batch 49, Loss: 2.6515\n",
      "Batch 50, Loss: 2.6637\n",
      "Batch 51, Loss: 2.4707\n",
      "Batch 52, Loss: 2.6892\n",
      "Batch 53, Loss: 2.6396\n",
      "Batch 54, Loss: 2.4889\n",
      "Batch 55, Loss: 2.6609\n",
      "Batch 56, Loss: 2.7203\n",
      "Batch 57, Loss: 2.4475\n",
      "Batch 58, Loss: 2.5688\n",
      "Batch 59, Loss: 2.6658\n",
      "Batch 60, Loss: 2.4891\n",
      "Batch 61, Loss: 2.6749\n",
      "Batch 62, Loss: 2.5768\n",
      "Batch 63, Loss: 2.6138\n",
      "Batch 64, Loss: 2.5501\n",
      "Batch 65, Loss: 2.4639\n",
      "Batch 66, Loss: 2.6434\n",
      "Batch 67, Loss: 2.4277\n",
      "Batch 68, Loss: 2.6629\n",
      "Batch 69, Loss: 2.5766\n",
      "Batch 70, Loss: 2.7120\n",
      "Batch 71, Loss: 2.6109\n",
      "Batch 72, Loss: 2.5474\n",
      "Batch 73, Loss: 2.7461\n",
      "Batch 74, Loss: 2.5587\n",
      "Batch 75, Loss: 2.5696\n",
      "Batch 76, Loss: 2.5471\n",
      "Batch 77, Loss: 2.4838\n",
      "Batch 78, Loss: 2.4961\n",
      "Batch 79, Loss: 2.4574\n",
      "Batch 80, Loss: 2.5925\n",
      "Batch 81, Loss: 2.6511\n",
      "Batch 82, Loss: 2.5959\n",
      "Batch 83, Loss: 2.5789\n",
      "Batch 84, Loss: 2.5142\n",
      "Batch 85, Loss: 2.6535\n",
      "Batch 86, Loss: 2.5863\n",
      "Batch 87, Loss: 2.4356\n",
      "Batch 88, Loss: 2.4811\n",
      "Batch 89, Loss: 2.4172\n",
      "Batch 90, Loss: 2.5830\n",
      "Batch 91, Loss: 2.4666\n",
      "Batch 92, Loss: 2.7026\n",
      "Batch 93, Loss: 2.4356\n",
      "Batch 94, Loss: 2.4750\n",
      "Batch 95, Loss: 2.4205\n",
      "Batch 96, Loss: 2.5250\n",
      "Batch 97, Loss: 2.4274\n",
      "Batch 98, Loss: 2.4243\n",
      "Batch 99, Loss: 2.5736\n",
      "Batch 100, Loss: 2.5465\n",
      "Batch 101, Loss: 2.4958\n",
      "Batch 102, Loss: 2.3730\n",
      "Batch 103, Loss: 2.6733\n",
      "Batch 104, Loss: 2.5271\n",
      "Batch 105, Loss: 2.4497\n",
      "Batch 106, Loss: 2.4601\n",
      "Batch 107, Loss: 2.4850\n",
      "Batch 108, Loss: 2.7096\n",
      "Batch 109, Loss: 2.6071\n",
      "Batch 110, Loss: 2.6396\n",
      "Batch 111, Loss: 2.6530\n",
      "Batch 112, Loss: 2.6565\n",
      "Batch 113, Loss: 2.6690\n",
      "Batch 114, Loss: 2.3984\n",
      "Batch 115, Loss: 2.5572\n",
      "Batch 116, Loss: 2.5364\n",
      "Batch 117, Loss: 2.5910\n",
      "Batch 118, Loss: 2.5887\n",
      "Batch 119, Loss: 2.5223\n",
      "Batch 120, Loss: 2.5208\n",
      "Batch 121, Loss: 2.4176\n",
      "Batch 122, Loss: 2.5735\n",
      "Batch 123, Loss: 2.4315\n",
      "Batch 124, Loss: 2.4939\n",
      "Batch 125, Loss: 2.6984\n",
      "Batch 126, Loss: 2.4889\n",
      "Batch 127, Loss: 2.4405\n",
      "Batch 128, Loss: 2.4780\n",
      "Batch 129, Loss: 2.5654\n",
      "Batch 130, Loss: 2.6020\n",
      "Batch 131, Loss: 2.5403\n",
      "Batch 132, Loss: 2.4362\n",
      "Batch 133, Loss: 2.4293\n",
      "Batch 134, Loss: 2.6407\n",
      "Batch 135, Loss: 2.5405\n",
      "Batch 136, Loss: 2.6419\n",
      "Batch 137, Loss: 2.4237\n",
      "Batch 138, Loss: 2.5271\n",
      "Batch 139, Loss: 2.5659\n",
      "Batch 140, Loss: 2.6225\n",
      "Batch 141, Loss: 2.4037\n",
      "Batch 142, Loss: 2.4006\n",
      "Batch 143, Loss: 2.5504\n",
      "Batch 144, Loss: 2.6299\n",
      "Batch 145, Loss: 2.6262\n",
      "Batch 146, Loss: 2.4913\n",
      "Batch 147, Loss: 2.4712\n",
      "Batch 148, Loss: 2.4940\n",
      "Batch 149, Loss: 2.6092\n",
      "Batch 150, Loss: 2.5249\n",
      "Batch 151, Loss: 2.5510\n",
      "Batch 152, Loss: 2.6330\n",
      "Batch 153, Loss: 2.6269\n",
      "Batch 154, Loss: 2.5165\n",
      "Batch 155, Loss: 2.5499\n",
      "Batch 156, Loss: 2.5702\n",
      "Batch 157, Loss: 2.5691\n",
      "Batch 158, Loss: 2.5320\n",
      "Batch 159, Loss: 2.5830\n",
      "Batch 160, Loss: 2.4622\n",
      "Batch 161, Loss: 2.4160\n",
      "Batch 162, Loss: 2.5313\n",
      "Batch 163, Loss: 2.4351\n",
      "Batch 164, Loss: 2.5327\n",
      "Batch 165, Loss: 2.4058\n",
      "Batch 166, Loss: 2.5933\n",
      "Batch 167, Loss: 2.4982\n",
      "Batch 168, Loss: 2.4856\n",
      "Batch 169, Loss: 2.4813\n",
      "Batch 170, Loss: 2.6122\n",
      "Batch 171, Loss: 2.6305\n",
      "Batch 172, Loss: 2.4813\n",
      "Batch 173, Loss: 2.6321\n",
      "Batch 174, Loss: 2.5878\n",
      "Batch 175, Loss: 2.4105\n",
      "Batch 176, Loss: 2.4858\n",
      "Batch 177, Loss: 2.4552\n",
      "Batch 178, Loss: 2.4695\n",
      "Batch 179, Loss: 2.5312\n",
      "Batch 180, Loss: 2.5617\n",
      "Batch 181, Loss: 2.5979\n",
      "Batch 182, Loss: 2.5407\n",
      "Batch 183, Loss: 2.4110\n",
      "Batch 184, Loss: 2.4939\n",
      "Batch 185, Loss: 2.4321\n",
      "Batch 186, Loss: 2.3701\n",
      "Batch 187, Loss: 2.5609\n",
      "Batch 188, Loss: 2.4371\n",
      "Batch 189, Loss: 2.4783\n",
      "Batch 190, Loss: 2.5013\n",
      "Batch 191, Loss: 2.4154\n",
      "Batch 192, Loss: 2.5917\n",
      "Batch 193, Loss: 2.4342\n",
      "Batch 194, Loss: 2.4675\n",
      "Batch 195, Loss: 2.4954\n",
      "Batch 196, Loss: 2.5190\n",
      "Batch 197, Loss: 2.4511\n",
      "Batch 198, Loss: 2.4991\n",
      "Batch 199, Loss: 2.4586\n",
      "Batch 200, Loss: 2.5736\n",
      "ie helped the train said: \"Yes special, bird!\", and she ran out with a spot slowly and bubbles and flowers. They smelled wing the story was too dizzy. The train \n",
      "bites to find their friend. Suddenly, the turry turns turned to fight over a loud noise. The tig was too slavy. The tank was dark and the train to the \n",
      "train fruit and the whole washaught. The turtle and the fish came to the store. \"You are leocut!\" Tom and Mia said. They saw a big white of tree and \n",
      "threw it in the sky. They were in the grass. They roared their pastic and smiled. \"Tom, look at clay!\" He was angry and went in build. But one happened \n",
      "to play. He watched the ring and fears screamed his eyes across the wind drove and never touch it again. They saw it and smiled. \"My thing people is not \n",
      "a job. Man you help me to find help you. It is fluffy forday.\" Tom and Mia felt sad. They say: \"That was a very nice boy. You learned an \n",
      "here! Han let's recogn. We need to be warer. We will let him and learn how to be to do there. Friends you use your name. Let's trapasta. A snowmma \n",
      "took me use her, Mom and get himself?\" Mom asked. \"Yes, they,\" Mia said. \"It's my hummy. You are very happy and we listen to her my lemon. Let's have \n",
      "some music. You look with them and help you frown everyon. \"You have a great. You are so tummy. It's birthday. You are good learning to treasure. And you are \n",
      "around. I will keep up, you.\" They say, \"We love to come with me?\" Mom said. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 9.55 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.5708\n",
      "Batch 202, Loss: 2.4291\n",
      "Batch 203, Loss: 2.5494\n",
      "Batch 204, Loss: 2.5809\n",
      "Batch 205, Loss: 2.5143\n",
      "Batch 206, Loss: 2.4275\n",
      "Batch 207, Loss: 2.5240\n",
      "Batch 208, Loss: 2.5499\n",
      "Batch 209, Loss: 2.6219\n",
      "Batch 210, Loss: 2.5711\n",
      "Batch 211, Loss: 2.5382\n",
      "Batch 212, Loss: 2.3936\n",
      "Batch 213, Loss: 2.5928\n",
      "Batch 214, Loss: 2.3671\n",
      "Batch 215, Loss: 2.5036\n",
      "Batch 216, Loss: 2.5117\n",
      "Batch 217, Loss: 2.4367\n",
      "Batch 218, Loss: 2.4167\n",
      "Batch 219, Loss: 2.4497\n",
      "Batch 220, Loss: 2.5049\n",
      "Batch 221, Loss: 2.4869\n",
      "Batch 222, Loss: 2.5476\n",
      "Batch 223, Loss: 2.5557\n",
      "Batch 224, Loss: 2.4415\n",
      "Batch 225, Loss: 2.5160\n",
      "Batch 226, Loss: 2.6352\n",
      "Batch 227, Loss: 2.3905\n",
      "Batch 228, Loss: 2.4930\n",
      "Batch 229, Loss: 2.6103\n",
      "Batch 230, Loss: 2.5467\n",
      "Batch 231, Loss: 2.3587\n",
      "Batch 232, Loss: 2.5423\n",
      "Batch 233, Loss: 2.5902\n",
      "Batch 234, Loss: 2.5865\n",
      "Batch 235, Loss: 2.4062\n",
      "Batch 236, Loss: 2.5315\n",
      "Batch 237, Loss: 2.5302\n",
      "Batch 238, Loss: 2.3989\n",
      "Batch 239, Loss: 2.4617\n",
      "Batch 240, Loss: 2.4536\n",
      "Batch 241, Loss: 2.3668\n",
      "Batch 242, Loss: 2.5114\n",
      "Batch 243, Loss: 2.4092\n",
      "Batch 244, Loss: 2.3661\n",
      "Batch 245, Loss: 2.4287\n",
      "Batch 246, Loss: 2.5437\n",
      "Batch 247, Loss: 2.3282\n",
      "Batch 248, Loss: 2.4811\n",
      "Batch 249, Loss: 2.5179\n",
      "Batch 250, Loss: 2.4323\n",
      "Batch 251, Loss: 2.4064\n",
      "Batch 252, Loss: 2.5655\n",
      "Batch 253, Loss: 2.5903\n",
      "Batch 254, Loss: 2.3711\n",
      "Batch 255, Loss: 2.4384\n",
      "Batch 256, Loss: 2.4865\n",
      "Batch 257, Loss: 2.4101\n",
      "Batch 258, Loss: 2.4924\n",
      "Batch 259, Loss: 2.4414\n",
      "Batch 260, Loss: 2.5365\n",
      "Batch 261, Loss: 2.4713\n",
      "Batch 262, Loss: 2.4515\n",
      "Batch 263, Loss: 2.3497\n",
      "Batch 264, Loss: 2.5125\n",
      "Batch 265, Loss: 2.3626\n",
      "Batch 266, Loss: 2.4756\n",
      "Batch 267, Loss: 2.2661\n",
      "Batch 268, Loss: 2.6257\n",
      "Batch 269, Loss: 2.4650\n",
      "Batch 270, Loss: 2.5053\n",
      "Batch 271, Loss: 2.5024\n",
      "Batch 272, Loss: 2.4587\n",
      "Batch 273, Loss: 2.4468\n",
      "Batch 274, Loss: 2.4293\n",
      "Batch 275, Loss: 2.5199\n",
      "Batch 276, Loss: 2.4149\n",
      "Batch 277, Loss: 2.5015\n",
      "Batch 278, Loss: 2.3074\n",
      "Batch 279, Loss: 2.4638\n",
      "Batch 280, Loss: 2.4907\n",
      "Batch 281, Loss: 2.4997\n",
      "Batch 282, Loss: 2.4286\n",
      "Batch 283, Loss: 2.4326\n",
      "Batch 284, Loss: 2.3699\n",
      "Batch 285, Loss: 2.5087\n",
      "Batch 286, Loss: 2.5532\n",
      "Batch 287, Loss: 2.3594\n",
      "Batch 288, Loss: 2.4500\n",
      "Batch 289, Loss: 2.4955\n",
      "Batch 290, Loss: 2.3552\n",
      "Batch 291, Loss: 2.3981\n",
      "Batch 292, Loss: 2.4793\n",
      "Batch 293, Loss: 2.3104\n",
      "Batch 294, Loss: 2.3565\n",
      "Batch 295, Loss: 2.3624\n",
      "Batch 296, Loss: 2.4917\n",
      "Batch 297, Loss: 2.3743\n",
      "Batch 298, Loss: 2.3676\n",
      "Batch 299, Loss: 2.5078\n",
      "Batch 300, Loss: 2.4033\n",
      "Batch 301, Loss: 2.3573\n",
      "Batch 302, Loss: 2.4574\n",
      "Batch 303, Loss: 2.5324\n",
      "Batch 304, Loss: 2.4680\n",
      "Batch 305, Loss: 2.4875\n",
      "Batch 306, Loss: 2.5401\n",
      "Batch 307, Loss: 2.3814\n",
      "Batch 308, Loss: 2.3148\n",
      "Batch 309, Loss: 2.5078\n",
      "Batch 310, Loss: 2.3834\n",
      "Batch 311, Loss: 2.4326\n",
      "Batch 312, Loss: 2.3409\n",
      "Batch 313, Loss: 2.2864\n",
      "Batch 314, Loss: 2.4599\n",
      "Batch 315, Loss: 2.5178\n",
      "Batch 316, Loss: 2.3472\n",
      "Batch 317, Loss: 2.4816\n",
      "Batch 318, Loss: 2.3376\n",
      "Batch 319, Loss: 2.5491\n",
      "Batch 320, Loss: 2.4870\n",
      "Batch 321, Loss: 2.4054\n",
      "Batch 322, Loss: 2.3865\n",
      "Batch 323, Loss: 2.3964\n",
      "Batch 324, Loss: 2.3083\n",
      "Batch 325, Loss: 2.4845\n",
      "Batch 326, Loss: 2.5136\n",
      "Batch 327, Loss: 2.4075\n",
      "Batch 328, Loss: 2.4157\n",
      "Batch 329, Loss: 2.4571\n",
      "Batch 330, Loss: 2.5110\n",
      "Batch 331, Loss: 2.2708\n",
      "Batch 332, Loss: 2.3119\n",
      "Batch 333, Loss: 2.2895\n",
      "Batch 334, Loss: 2.5117\n",
      "Batch 335, Loss: 2.3742\n",
      "Batch 336, Loss: 2.3803\n",
      "Batch 337, Loss: 2.5514\n",
      "Batch 338, Loss: 2.5558\n",
      "Batch 339, Loss: 2.4032\n",
      "Batch 340, Loss: 2.3287\n",
      "Batch 341, Loss: 2.4671\n",
      "Batch 342, Loss: 2.4076\n",
      "Batch 343, Loss: 2.4201\n",
      "Batch 344, Loss: 2.3101\n",
      "Batch 345, Loss: 2.4689\n",
      "Batch 346, Loss: 2.4489\n",
      "Batch 347, Loss: 2.3806\n",
      "Batch 348, Loss: 2.3639\n",
      "Batch 349, Loss: 2.5158\n",
      "Batch 350, Loss: 2.2331\n",
      "Batch 351, Loss: 2.4802\n",
      "Batch 352, Loss: 2.3743\n",
      "Batch 353, Loss: 2.2783\n",
      "Batch 354, Loss: 2.4850\n",
      "Batch 355, Loss: 2.4062\n",
      "Batch 356, Loss: 2.4580\n",
      "Batch 357, Loss: 2.5317\n",
      "Batch 358, Loss: 2.3947\n",
      "Batch 359, Loss: 2.3064\n",
      "Batch 360, Loss: 2.4402\n",
      "Batch 361, Loss: 2.4099\n",
      "Batch 362, Loss: 2.4583\n",
      "Batch 363, Loss: 2.5027\n",
      "Batch 364, Loss: 2.3015\n",
      "Batch 365, Loss: 2.3157\n",
      "Batch 366, Loss: 2.5565\n",
      "Batch 367, Loss: 2.4332\n",
      "Batch 368, Loss: 2.2787\n",
      "Batch 369, Loss: 2.3997\n",
      "Batch 370, Loss: 2.3232\n",
      "Batch 371, Loss: 2.3067\n",
      "Batch 372, Loss: 2.3567\n",
      "Batch 373, Loss: 2.2695\n",
      "Batch 374, Loss: 2.3825\n",
      "Batch 375, Loss: 2.4321\n",
      "Batch 376, Loss: 2.3656\n",
      "Batch 377, Loss: 2.3991\n",
      "Batch 378, Loss: 2.3155\n",
      "Batch 379, Loss: 2.5027\n",
      "Batch 380, Loss: 2.4086\n",
      "Batch 381, Loss: 2.3056\n",
      "Batch 382, Loss: 2.4661\n",
      "Batch 383, Loss: 2.3544\n",
      "Batch 384, Loss: 2.3373\n",
      "Batch 385, Loss: 2.4611\n",
      "Batch 386, Loss: 2.5065\n",
      "Batch 387, Loss: 2.3427\n",
      "Batch 388, Loss: 2.2990\n",
      "Batch 389, Loss: 2.3254\n",
      "Batch 390, Loss: 2.2151\n",
      "Batch 391, Loss: 2.3638\n",
      "Batch 392, Loss: 2.4604\n",
      "Batch 393, Loss: 2.3838\n",
      "Batch 394, Loss: 2.4019\n",
      "Batch 395, Loss: 2.3598\n",
      "Batch 396, Loss: 2.2838\n",
      "Batch 397, Loss: 2.4640\n",
      "Batch 398, Loss: 2.3500\n",
      "Batch 399, Loss: 2.2671\n",
      "Batch 400, Loss: 2.3730\n",
      "Once upon a time, there was a pot again! He was very happy to see decided to have a special leopard. The duck tug sawder kiding stop playing the hiddenly. \n",
      "He saw a brillarger sparkling and exciting. The lighter smiled and said, \"Yes, I impatient and kind. I want to put away everywhere I can go examine if you.\" One \n",
      "day, the light noticed a scared sparkly dinner to catch. The luck came outside home and saw that the light moder frightened and went back that the way until the \n",
      "light said, \"Let's try instead.\" It was so excited and generousâ€™re to know what to do. And the light deep, the light started to move it. The morning moral of \n",
      "Jen sneest in amazement and the moral of the lightsy for her were the best. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 11.14 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 401, Loss: 2.2776\n",
      "Batch 402, Loss: 2.3435\n",
      "Batch 403, Loss: 2.3915\n",
      "Batch 404, Loss: 2.2864\n",
      "Batch 405, Loss: 2.3760\n",
      "Batch 406, Loss: 2.2902\n",
      "Batch 407, Loss: 2.4292\n",
      "Batch 408, Loss: 2.3914\n",
      "Batch 409, Loss: 2.4066\n",
      "Batch 410, Loss: 2.4653\n",
      "Batch 411, Loss: 2.2420\n",
      "Batch 412, Loss: 2.4490\n",
      "Batch 413, Loss: 2.3975\n",
      "Batch 414, Loss: 2.3786\n",
      "Batch 415, Loss: 2.2812\n",
      "Batch 416, Loss: 2.4583\n",
      "Batch 417, Loss: 2.3972\n",
      "Batch 418, Loss: 2.3455\n",
      "Epoch 4, Average Loss: 2.4885\n",
      "Hun and Daduff in the car. They did when they fina came to help. They hugged each other and kissed the ingredient. But then, Dad fruit more ice. Sam and \n",
      "Ben were still in the car. They ran to the zoo and said they were on their way, a cow suit! They said dad. \"Let's go inside and play every \n",
      "day. They are very happy and excited.\" Sam said. They picked the zoo and put their tricks and brought one to the zoo. They thought it was fun and ran \n",
      "to the toy. They looked in some flowers and drove into two grass and the zoo. \"Can I reach the vide closer. It cut Tom is a good plate from \n",
      "the count. \"We are the best.\" Dad laughed and said, \"No, Anna, you'.\" Dad looked at Ben and saw an orange that did not want to share them!\" But Ben \n",
      "and Mia were shaking. It was a toy. He gave her a hug and showed her a hug. There were nos that he had no toys forgive him and put \n",
      "his fountain back. He hoped back to Spothing, and his eyes were coming from awlen that was gone forever. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.4264\n",
      "Batch 2, Loss: 2.3030\n",
      "Batch 3, Loss: 2.4604\n",
      "Batch 4, Loss: 2.4054\n",
      "Batch 5, Loss: 2.3325\n",
      "Batch 6, Loss: 2.2710\n",
      "Batch 7, Loss: 2.2464\n",
      "Batch 8, Loss: 2.2603\n",
      "Batch 9, Loss: 2.2864\n",
      "Batch 10, Loss: 2.4225\n",
      "Batch 11, Loss: 2.3084\n",
      "Batch 12, Loss: 2.3141\n",
      "Batch 13, Loss: 2.5168\n",
      "Batch 14, Loss: 2.3574\n",
      "Batch 15, Loss: 2.4744\n",
      "Batch 16, Loss: 2.2049\n",
      "Batch 17, Loss: 2.4736\n",
      "Batch 18, Loss: 2.3575\n",
      "Batch 19, Loss: 2.3298\n",
      "Batch 20, Loss: 2.2955\n",
      "Batch 21, Loss: 2.2687\n",
      "Batch 22, Loss: 2.5256\n",
      "Batch 23, Loss: 2.5082\n",
      "Batch 24, Loss: 2.3416\n",
      "Batch 25, Loss: 2.3202\n",
      "Batch 26, Loss: 2.3679\n",
      "Batch 27, Loss: 2.4194\n",
      "Batch 28, Loss: 2.4726\n",
      "Batch 29, Loss: 2.4904\n",
      "Batch 30, Loss: 2.3428\n",
      "Batch 31, Loss: 2.2717\n",
      "Batch 32, Loss: 2.4111\n",
      "Batch 33, Loss: 2.3336\n",
      "Batch 34, Loss: 2.4012\n",
      "Batch 35, Loss: 2.3891\n",
      "Batch 36, Loss: 2.3650\n",
      "Batch 37, Loss: 2.2597\n",
      "Batch 38, Loss: 2.4123\n",
      "Batch 39, Loss: 2.2877\n",
      "Batch 40, Loss: 2.2518\n",
      "Batch 41, Loss: 2.2250\n",
      "Batch 42, Loss: 2.3118\n",
      "Batch 43, Loss: 2.4908\n",
      "Batch 44, Loss: 2.4178\n",
      "Batch 45, Loss: 2.2816\n",
      "Batch 46, Loss: 2.2649\n",
      "Batch 47, Loss: 2.2246\n",
      "Batch 48, Loss: 2.3715\n",
      "Batch 49, Loss: 2.4042\n",
      "Batch 50, Loss: 2.3919\n",
      "Batch 51, Loss: 2.3740\n",
      "Batch 52, Loss: 2.3431\n",
      "Batch 53, Loss: 2.4815\n",
      "Batch 54, Loss: 2.2573\n",
      "Batch 55, Loss: 2.2782\n",
      "Batch 56, Loss: 2.4294\n",
      "Batch 57, Loss: 2.3465\n",
      "Batch 58, Loss: 2.4080\n",
      "Batch 59, Loss: 2.4482\n",
      "Batch 60, Loss: 2.3968\n",
      "Batch 61, Loss: 2.3498\n",
      "Batch 62, Loss: 2.4459\n",
      "Batch 63, Loss: 2.2484\n",
      "Batch 64, Loss: 2.3436\n",
      "Batch 65, Loss: 2.2057\n",
      "Batch 66, Loss: 2.2401\n",
      "Batch 67, Loss: 2.4406\n",
      "Batch 68, Loss: 2.2915\n",
      "Batch 69, Loss: 2.3447\n",
      "Batch 70, Loss: 2.4867\n",
      "Batch 71, Loss: 2.3187\n",
      "Batch 72, Loss: 2.3466\n",
      "Batch 73, Loss: 2.2896\n",
      "Batch 74, Loss: 2.4144\n",
      "Batch 75, Loss: 2.2695\n",
      "Batch 76, Loss: 2.3543\n",
      "Batch 77, Loss: 2.4339\n",
      "Batch 78, Loss: 2.2951\n",
      "Batch 79, Loss: 2.3177\n",
      "Batch 80, Loss: 2.3351\n",
      "Batch 81, Loss: 2.3727\n",
      "Batch 82, Loss: 2.3192\n",
      "Batch 83, Loss: 2.2983\n",
      "Batch 84, Loss: 2.1514\n",
      "Batch 85, Loss: 2.2622\n",
      "Batch 86, Loss: 2.2224\n",
      "Batch 87, Loss: 2.4626\n",
      "Batch 88, Loss: 2.2480\n",
      "Batch 89, Loss: 2.1735\n",
      "Batch 90, Loss: 2.4162\n",
      "Batch 91, Loss: 2.4745\n",
      "Batch 92, Loss: 2.4389\n",
      "Batch 93, Loss: 2.3099\n",
      "Batch 94, Loss: 2.3784\n",
      "Batch 95, Loss: 2.4669\n",
      "Batch 96, Loss: 2.3874\n",
      "Batch 97, Loss: 2.4274\n",
      "Batch 98, Loss: 2.4466\n",
      "Batch 99, Loss: 2.3818\n",
      "Batch 100, Loss: 2.2612\n",
      "Batch 101, Loss: 2.2254\n",
      "Batch 102, Loss: 2.4366\n",
      "Batch 103, Loss: 2.1821\n",
      "Batch 104, Loss: 2.3005\n",
      "Batch 105, Loss: 2.3340\n",
      "Batch 106, Loss: 2.3129\n",
      "Batch 107, Loss: 2.4262\n",
      "Batch 108, Loss: 2.2677\n",
      "Batch 109, Loss: 2.2235\n",
      "Batch 110, Loss: 2.2330\n",
      "Batch 111, Loss: 2.3469\n",
      "Batch 112, Loss: 2.3076\n",
      "Batch 113, Loss: 2.2673\n",
      "Batch 114, Loss: 2.1750\n",
      "Batch 115, Loss: 2.3802\n",
      "Batch 116, Loss: 2.3849\n",
      "Batch 117, Loss: 2.2596\n",
      "Batch 118, Loss: 2.1800\n",
      "Batch 119, Loss: 2.2612\n",
      "Batch 120, Loss: 2.2600\n",
      "Batch 121, Loss: 2.3939\n",
      "Batch 122, Loss: 2.3126\n",
      "Batch 123, Loss: 2.2981\n",
      "Batch 124, Loss: 2.3452\n",
      "Batch 125, Loss: 2.3705\n",
      "Batch 126, Loss: 2.3046\n",
      "Batch 127, Loss: 2.4175\n",
      "Batch 128, Loss: 2.4467\n",
      "Batch 129, Loss: 2.1781\n",
      "Batch 130, Loss: 2.2368\n",
      "Batch 131, Loss: 2.1996\n",
      "Batch 132, Loss: 2.3912\n",
      "Batch 133, Loss: 2.3544\n",
      "Batch 134, Loss: 2.3702\n",
      "Batch 135, Loss: 2.4280\n",
      "Batch 136, Loss: 2.2120\n",
      "Batch 137, Loss: 2.2851\n",
      "Batch 138, Loss: 2.3207\n",
      "Batch 139, Loss: 2.3211\n",
      "Batch 140, Loss: 2.1967\n",
      "Batch 141, Loss: 2.3892\n",
      "Batch 142, Loss: 2.3698\n",
      "Batch 143, Loss: 2.1742\n",
      "Batch 144, Loss: 2.3821\n",
      "Batch 145, Loss: 2.2139\n",
      "Batch 146, Loss: 2.2633\n",
      "Batch 147, Loss: 2.3690\n",
      "Batch 148, Loss: 2.2090\n",
      "Batch 149, Loss: 2.4046\n",
      "Batch 150, Loss: 2.1822\n",
      "Batch 151, Loss: 2.2873\n",
      "Batch 152, Loss: 2.1823\n",
      "Batch 153, Loss: 2.1871\n",
      "Batch 154, Loss: 2.2627\n",
      "Batch 155, Loss: 2.4343\n",
      "Batch 156, Loss: 2.2207\n",
      "Batch 157, Loss: 2.4089\n",
      "Batch 158, Loss: 2.2664\n",
      "Batch 159, Loss: 2.3986\n",
      "Batch 160, Loss: 2.3965\n",
      "Batch 161, Loss: 2.2130\n",
      "Batch 162, Loss: 2.2718\n",
      "Batch 163, Loss: 2.3630\n",
      "Batch 164, Loss: 2.4281\n",
      "Batch 165, Loss: 2.1791\n",
      "Batch 166, Loss: 2.3368\n",
      "Batch 167, Loss: 2.2349\n",
      "Batch 168, Loss: 2.4031\n",
      "Batch 169, Loss: 2.2817\n",
      "Batch 170, Loss: 2.4067\n",
      "Batch 171, Loss: 2.4071\n",
      "Batch 172, Loss: 2.3064\n",
      "Batch 173, Loss: 2.3328\n",
      "Batch 174, Loss: 2.2680\n",
      "Batch 175, Loss: 2.4150\n",
      "Batch 176, Loss: 2.3948\n",
      "Batch 177, Loss: 2.3492\n",
      "Batch 178, Loss: 2.3826\n",
      "Batch 179, Loss: 2.2864\n",
      "Batch 180, Loss: 2.4194\n",
      "Batch 181, Loss: 2.4040\n",
      "Batch 182, Loss: 2.3294\n",
      "Batch 183, Loss: 2.4028\n",
      "Batch 184, Loss: 2.4096\n",
      "Batch 185, Loss: 2.3159\n",
      "Batch 186, Loss: 2.3792\n",
      "Batch 187, Loss: 2.2889\n",
      "Batch 188, Loss: 2.1598\n",
      "Batch 189, Loss: 2.2499\n",
      "Batch 190, Loss: 2.3082\n",
      "Batch 191, Loss: 2.3409\n",
      "Batch 192, Loss: 2.2769\n",
      "Batch 193, Loss: 2.2351\n",
      "Batch 194, Loss: 2.3270\n",
      "Batch 195, Loss: 2.3152\n",
      "Batch 196, Loss: 2.2491\n",
      "Batch 197, Loss: 2.1511\n",
      "Batch 198, Loss: 2.3062\n",
      "Batch 199, Loss: 2.3768\n",
      "Batch 200, Loss: 2.2620\n",
      "Tom and Lily were away from the tral. They had another time drove home and Tom's house. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 7.93 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.3715\n",
      "Batch 202, Loss: 2.2206\n",
      "Batch 203, Loss: 2.4003\n",
      "Batch 204, Loss: 2.3753\n",
      "Batch 205, Loss: 2.2062\n",
      "Batch 206, Loss: 2.1776\n",
      "Batch 207, Loss: 2.4265\n",
      "Batch 208, Loss: 2.1719\n",
      "Batch 209, Loss: 2.2829\n",
      "Batch 210, Loss: 2.2642\n",
      "Batch 211, Loss: 2.3326\n",
      "Batch 212, Loss: 2.3707\n",
      "Batch 213, Loss: 2.2107\n",
      "Batch 214, Loss: 2.3113\n",
      "Batch 215, Loss: 2.2039\n",
      "Batch 216, Loss: 2.2652\n",
      "Batch 217, Loss: 2.1705\n",
      "Batch 218, Loss: 2.2550\n",
      "Batch 219, Loss: 2.3647\n",
      "Batch 220, Loss: 2.3854\n",
      "Batch 221, Loss: 2.3700\n",
      "Batch 222, Loss: 2.3684\n",
      "Batch 223, Loss: 2.3159\n",
      "Batch 224, Loss: 2.2183\n",
      "Batch 225, Loss: 2.1629\n",
      "Batch 226, Loss: 2.4018\n",
      "Batch 227, Loss: 2.2358\n",
      "Batch 228, Loss: 2.3096\n",
      "Batch 229, Loss: 2.1752\n",
      "Batch 230, Loss: 2.2628\n",
      "Batch 231, Loss: 2.3517\n",
      "Batch 232, Loss: 2.2935\n",
      "Batch 233, Loss: 2.3347\n",
      "Batch 234, Loss: 2.2232\n",
      "Batch 235, Loss: 2.3787\n",
      "Batch 236, Loss: 2.4003\n",
      "Batch 237, Loss: 2.3492\n",
      "Batch 238, Loss: 2.3940\n",
      "Batch 239, Loss: 2.3755\n",
      "Batch 240, Loss: 2.3203\n",
      "Batch 241, Loss: 2.2592\n",
      "Batch 242, Loss: 2.2468\n",
      "Batch 243, Loss: 2.3558\n",
      "Batch 244, Loss: 2.1869\n",
      "Batch 245, Loss: 2.3668\n",
      "Batch 246, Loss: 2.1890\n",
      "Batch 247, Loss: 2.3211\n",
      "Batch 248, Loss: 2.2666\n",
      "Batch 249, Loss: 2.3529\n",
      "Batch 250, Loss: 2.2797\n",
      "Batch 251, Loss: 2.1890\n",
      "Batch 252, Loss: 2.4297\n",
      "Batch 253, Loss: 2.1791\n",
      "Batch 254, Loss: 2.1454\n",
      "Batch 255, Loss: 2.2623\n",
      "Batch 256, Loss: 2.1144\n",
      "Batch 257, Loss: 2.2922\n",
      "Batch 258, Loss: 2.1266\n",
      "Batch 259, Loss: 2.3900\n",
      "Batch 260, Loss: 2.2117\n",
      "Batch 261, Loss: 2.2301\n",
      "Batch 262, Loss: 2.2134\n",
      "Batch 263, Loss: 2.2774\n",
      "Batch 264, Loss: 2.4025\n",
      "Batch 265, Loss: 2.1623\n",
      "Batch 266, Loss: 2.2675\n",
      "Batch 267, Loss: 2.2233\n",
      "Batch 268, Loss: 2.3081\n",
      "Batch 269, Loss: 2.2372\n",
      "Batch 270, Loss: 2.3505\n",
      "Batch 271, Loss: 2.2050\n",
      "Batch 272, Loss: 2.3009\n",
      "Batch 273, Loss: 2.3755\n",
      "Batch 274, Loss: 2.2796\n",
      "Batch 275, Loss: 2.4052\n",
      "Batch 276, Loss: 2.3703\n",
      "Batch 277, Loss: 2.3943\n",
      "Batch 278, Loss: 2.1510\n",
      "Batch 279, Loss: 2.2496\n",
      "Batch 280, Loss: 2.2670\n",
      "Batch 281, Loss: 2.3831\n",
      "Batch 282, Loss: 2.2532\n",
      "Batch 283, Loss: 2.1425\n",
      "Batch 284, Loss: 2.3343\n",
      "Batch 285, Loss: 2.4192\n",
      "Batch 286, Loss: 2.2257\n",
      "Batch 287, Loss: 2.2370\n",
      "Batch 288, Loss: 2.2150\n",
      "Batch 289, Loss: 2.3616\n",
      "Batch 290, Loss: 2.1815\n",
      "Batch 291, Loss: 2.3958\n",
      "Batch 292, Loss: 2.3394\n",
      "Batch 293, Loss: 2.1941\n",
      "Batch 294, Loss: 2.3229\n",
      "Batch 295, Loss: 2.1169\n",
      "Batch 296, Loss: 2.2850\n",
      "Batch 297, Loss: 2.2296\n",
      "Batch 298, Loss: 2.2169\n",
      "Batch 299, Loss: 2.2927\n",
      "Batch 300, Loss: 2.4155\n",
      "Batch 301, Loss: 2.2744\n",
      "Batch 302, Loss: 2.3378\n",
      "Batch 303, Loss: 2.2731\n",
      "Batch 304, Loss: 2.3026\n",
      "Batch 305, Loss: 2.4014\n",
      "Batch 306, Loss: 2.2621\n",
      "Batch 307, Loss: 2.3609\n",
      "Batch 308, Loss: 2.2231\n",
      "Batch 309, Loss: 2.3862\n",
      "Batch 310, Loss: 2.1940\n",
      "Batch 311, Loss: 2.2071\n",
      "Batch 312, Loss: 2.3091\n",
      "Batch 313, Loss: 2.3227\n",
      "Batch 314, Loss: 2.2972\n",
      "Batch 315, Loss: 2.1297\n",
      "Batch 316, Loss: 2.2765\n",
      "Batch 317, Loss: 2.2309\n",
      "Batch 318, Loss: 2.2219\n",
      "Batch 319, Loss: 2.1686\n",
      "Batch 320, Loss: 2.3506\n",
      "Batch 321, Loss: 2.1766\n",
      "Batch 322, Loss: 2.2800\n",
      "Batch 323, Loss: 2.1642\n",
      "Batch 324, Loss: 2.2216\n",
      "Batch 325, Loss: 2.2934\n",
      "Batch 326, Loss: 2.3879\n",
      "Batch 327, Loss: 2.3705\n",
      "Batch 328, Loss: 2.1850\n",
      "Batch 329, Loss: 2.1696\n",
      "Batch 330, Loss: 2.1485\n",
      "Batch 331, Loss: 2.1644\n",
      "Batch 332, Loss: 2.3008\n",
      "Batch 333, Loss: 2.1501\n",
      "Batch 334, Loss: 2.3436\n",
      "Batch 335, Loss: 2.2388\n",
      "Batch 336, Loss: 2.2367\n",
      "Batch 337, Loss: 2.1643\n",
      "Batch 338, Loss: 2.3460\n",
      "Batch 339, Loss: 2.3593\n",
      "Batch 340, Loss: 2.2860\n",
      "Batch 341, Loss: 2.1579\n",
      "Batch 342, Loss: 2.2926\n",
      "Batch 343, Loss: 2.2106\n",
      "Batch 344, Loss: 2.3524\n",
      "Batch 345, Loss: 2.1038\n",
      "Batch 346, Loss: 2.1735\n",
      "Batch 347, Loss: 2.2390\n",
      "Batch 348, Loss: 2.2110\n",
      "Batch 349, Loss: 2.2048\n",
      "Batch 350, Loss: 2.2808\n",
      "Batch 351, Loss: 2.4110\n",
      "Batch 352, Loss: 2.3603\n",
      "Batch 353, Loss: 2.1078\n",
      "Batch 354, Loss: 2.2086\n",
      "Batch 355, Loss: 2.2524\n",
      "Batch 356, Loss: 2.2938\n",
      "Batch 357, Loss: 2.2086\n",
      "Batch 358, Loss: 2.2256\n",
      "Batch 359, Loss: 2.2922\n",
      "Batch 360, Loss: 2.2082\n",
      "Batch 361, Loss: 2.2559\n",
      "Batch 362, Loss: 2.1510\n",
      "Batch 363, Loss: 2.3421\n",
      "Batch 364, Loss: 2.3462\n",
      "Batch 365, Loss: 2.2438\n",
      "Batch 366, Loss: 2.2714\n",
      "Batch 367, Loss: 2.4115\n",
      "Batch 368, Loss: 2.2894\n",
      "Batch 369, Loss: 2.3379\n",
      "Batch 370, Loss: 2.2303\n",
      "Batch 371, Loss: 2.2342\n",
      "Batch 372, Loss: 2.1665\n",
      "Batch 373, Loss: 2.1825\n",
      "Batch 374, Loss: 2.1903\n",
      "Batch 375, Loss: 2.1516\n",
      "Batch 376, Loss: 2.2848\n",
      "Batch 377, Loss: 2.3560\n",
      "Batch 378, Loss: 2.3174\n",
      "Batch 379, Loss: 2.2364\n",
      "Batch 380, Loss: 2.2855\n",
      "Batch 381, Loss: 2.1398\n",
      "Batch 382, Loss: 2.1558\n",
      "Batch 383, Loss: 2.3530\n",
      "Batch 384, Loss: 2.2446\n",
      "Batch 385, Loss: 2.2747\n",
      "Batch 386, Loss: 2.2551\n",
      "Batch 387, Loss: 2.3919\n",
      "Batch 388, Loss: 2.0774\n",
      "Batch 389, Loss: 2.2635\n",
      "Batch 390, Loss: 2.2120\n",
      "Batch 391, Loss: 2.2653\n",
      "Batch 392, Loss: 2.3336\n",
      "Batch 393, Loss: 2.3668\n",
      "Batch 394, Loss: 2.1500\n",
      "Batch 395, Loss: 2.2289\n",
      "Batch 396, Loss: 2.2730\n",
      "Batch 397, Loss: 2.2172\n",
      "Batch 398, Loss: 2.2600\n",
      "Batch 399, Loss: 2.3386\n",
      "Batch 400, Loss: 2.3211\n",
      "Once upon a time, there was a young grass dog named Abby and he loved to done increase. One day, Abby went to the woods as fast as Abby was \n",
      "and she saw a big very pond. It was delicious of spinning around. Abby was deep, Abby saw a snucky with bright, colourful reficmase. She had had been more fun \n",
      "on the ground and it changed her tail. After a while, Abby had a bright idea. She ran around the house, making a funny high high in her elephant. She \n",
      "ran around the house with the animal, browns and said, \"Would you like be the noisy forest? Everywhere I go, sailed the car will buy for lunch and a treat!\" \n",
      "Everywhere With the tail wood. So, Eventually was troubled. She cold a minuttently squired it and had sweet. Seeve into the coins. She had a lot of fun and made \n",
      "every blug. The nice gun and the mouse was very excited. They wanted to done the monster to others. The monkey was so full of soup and took it, even \n",
      "when they talked. They wished for them to safetting them and belongated to them. They were very happy. Abby that day, they were in the cold and their house had \n",
      "no munning, and gave them the monster on their back. The monster stopped crying and touched it. It gave up their doth dily to zors and showed it to their \n",
      "questions. The end. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 12.40 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 401, Loss: 2.1717\n",
      "Batch 402, Loss: 2.2340\n",
      "Batch 403, Loss: 2.3346\n",
      "Batch 404, Loss: 2.2076\n",
      "Batch 405, Loss: 2.1283\n",
      "Batch 406, Loss: 2.2611\n",
      "Batch 407, Loss: 2.2777\n",
      "Batch 408, Loss: 2.3389\n",
      "Batch 409, Loss: 2.1457\n",
      "Batch 410, Loss: 2.3104\n",
      "Batch 411, Loss: 2.1973\n",
      "Batch 412, Loss: 2.3016\n",
      "Batch 413, Loss: 2.1288\n",
      "Batch 414, Loss: 2.2462\n",
      "Batch 415, Loss: 2.1495\n",
      "Batch 416, Loss: 2.1168\n",
      "Batch 417, Loss: 2.1553\n",
      "Batch 418, Loss: 2.0758\n",
      "Epoch 5, Average Loss: 2.2970\n",
      "Once upon a time, there was a little girl named Lily.e. The mommy was very happy because she helped her mommy was going on a trip. Trally loved the village \n",
      "on the beach. One day, Lily's mommy went to the zoo to drink. She saw a big tree with many stones and turns around it. Lily loved feeling scared, but \n",
      "she didn't want to smell the less tree. As they came acrossed along, they saw yacht leave. They decided to take a walk with their toy trip and said they \n",
      "would like a free or a tree. There were rich toy spicy bushes and bugs. They were happy again and made sure to see that their moms come back soon. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.1773\n",
      "Batch 2, Loss: 2.2532\n",
      "Batch 3, Loss: 2.2321\n",
      "Batch 4, Loss: 2.1715\n",
      "Batch 5, Loss: 2.1793\n",
      "Batch 6, Loss: 2.2538\n",
      "Batch 7, Loss: 2.3091\n",
      "Batch 8, Loss: 2.2746\n",
      "Batch 9, Loss: 2.1340\n",
      "Batch 10, Loss: 2.0637\n",
      "Batch 11, Loss: 2.1653\n",
      "Batch 12, Loss: 2.3253\n",
      "Batch 13, Loss: 2.1535\n",
      "Batch 14, Loss: 2.2286\n",
      "Batch 15, Loss: 2.2354\n",
      "Batch 16, Loss: 2.3356\n",
      "Batch 17, Loss: 2.2126\n",
      "Batch 18, Loss: 2.1216\n",
      "Batch 19, Loss: 2.1057\n",
      "Batch 20, Loss: 2.1013\n",
      "Batch 21, Loss: 2.2605\n",
      "Batch 22, Loss: 2.2765\n",
      "Batch 23, Loss: 2.3414\n",
      "Batch 24, Loss: 2.2550\n",
      "Batch 25, Loss: 2.0580\n",
      "Batch 26, Loss: 2.2249\n",
      "Batch 27, Loss: 2.2114\n",
      "Batch 28, Loss: 2.2470\n",
      "Batch 29, Loss: 2.2335\n",
      "Batch 30, Loss: 2.2978\n",
      "Batch 31, Loss: 2.1127\n",
      "Batch 32, Loss: 2.1943\n",
      "Batch 33, Loss: 2.2118\n",
      "Batch 34, Loss: 2.2449\n",
      "Batch 35, Loss: 2.1707\n",
      "Batch 36, Loss: 2.1127\n",
      "Batch 37, Loss: 2.1800\n",
      "Batch 38, Loss: 2.3171\n",
      "Batch 39, Loss: 2.3271\n",
      "Batch 40, Loss: 2.2306\n",
      "Batch 41, Loss: 2.3036\n",
      "Batch 42, Loss: 2.1621\n",
      "Batch 43, Loss: 2.0991\n",
      "Batch 44, Loss: 2.2234\n",
      "Batch 45, Loss: 2.1997\n",
      "Batch 46, Loss: 2.2767\n",
      "Batch 47, Loss: 2.0940\n",
      "Batch 48, Loss: 2.2024\n",
      "Batch 49, Loss: 2.2150\n",
      "Batch 50, Loss: 2.1164\n",
      "Batch 51, Loss: 2.3155\n",
      "Batch 52, Loss: 2.2075\n",
      "Batch 53, Loss: 2.2409\n",
      "Batch 54, Loss: 2.2462\n",
      "Batch 55, Loss: 2.2299\n",
      "Batch 56, Loss: 2.2503\n",
      "Batch 57, Loss: 2.0687\n",
      "Batch 58, Loss: 2.1935\n",
      "Batch 59, Loss: 2.1526\n",
      "Batch 60, Loss: 2.2471\n",
      "Batch 61, Loss: 2.3016\n",
      "Batch 62, Loss: 2.2480\n",
      "Batch 63, Loss: 2.0786\n",
      "Batch 64, Loss: 2.1867\n",
      "Batch 65, Loss: 2.2205\n",
      "Batch 66, Loss: 2.2437\n",
      "Batch 67, Loss: 2.2054\n",
      "Batch 68, Loss: 2.1777\n",
      "Batch 69, Loss: 2.2288\n",
      "Batch 70, Loss: 2.1786\n",
      "Batch 71, Loss: 2.0167\n",
      "Batch 72, Loss: 2.1860\n",
      "Batch 73, Loss: 2.3260\n",
      "Batch 74, Loss: 2.2095\n",
      "Batch 75, Loss: 2.0587\n",
      "Batch 76, Loss: 2.2331\n",
      "Batch 77, Loss: 2.3465\n",
      "Batch 78, Loss: 2.2543\n",
      "Batch 79, Loss: 2.2426\n",
      "Batch 80, Loss: 2.3171\n",
      "Batch 81, Loss: 2.1907\n",
      "Batch 82, Loss: 2.2153\n",
      "Batch 83, Loss: 2.3261\n",
      "Batch 84, Loss: 2.3183\n",
      "Batch 85, Loss: 2.1124\n",
      "Batch 86, Loss: 2.2198\n",
      "Batch 87, Loss: 2.0474\n",
      "Batch 88, Loss: 2.1367\n",
      "Batch 89, Loss: 2.1830\n",
      "Batch 90, Loss: 2.2031\n",
      "Batch 91, Loss: 2.2098\n",
      "Batch 92, Loss: 2.2796\n",
      "Batch 93, Loss: 2.1327\n",
      "Batch 94, Loss: 2.2899\n",
      "Batch 95, Loss: 2.1732\n",
      "Batch 96, Loss: 2.0558\n",
      "Batch 97, Loss: 2.0456\n",
      "Batch 98, Loss: 2.1698\n",
      "Batch 99, Loss: 2.2985\n",
      "Batch 100, Loss: 2.1510\n",
      "Batch 101, Loss: 2.3055\n",
      "Batch 102, Loss: 2.0616\n",
      "Batch 103, Loss: 2.3258\n",
      "Batch 104, Loss: 2.0931\n",
      "Batch 105, Loss: 2.2671\n",
      "Batch 106, Loss: 2.1593\n",
      "Batch 107, Loss: 2.1983\n",
      "Batch 108, Loss: 2.1082\n",
      "Batch 109, Loss: 2.2472\n",
      "Batch 110, Loss: 2.3260\n",
      "Batch 111, Loss: 2.2877\n",
      "Batch 112, Loss: 2.3048\n",
      "Batch 113, Loss: 2.1291\n",
      "Batch 114, Loss: 2.1338\n",
      "Batch 115, Loss: 2.1290\n",
      "Batch 116, Loss: 2.1650\n",
      "Batch 117, Loss: 2.2954\n",
      "Batch 118, Loss: 2.2285\n",
      "Batch 119, Loss: 2.2747\n",
      "Batch 120, Loss: 2.1617\n",
      "Batch 121, Loss: 2.2870\n",
      "Batch 122, Loss: 2.2593\n",
      "Batch 123, Loss: 2.0748\n",
      "Batch 124, Loss: 2.1027\n",
      "Batch 125, Loss: 2.1627\n",
      "Batch 126, Loss: 2.0952\n",
      "Batch 127, Loss: 2.0878\n",
      "Batch 128, Loss: 2.1372\n",
      "Batch 129, Loss: 2.3105\n",
      "Batch 130, Loss: 2.2655\n",
      "Batch 131, Loss: 2.2989\n",
      "Batch 132, Loss: 2.2990\n",
      "Batch 133, Loss: 2.0889\n",
      "Batch 134, Loss: 2.1579\n",
      "Batch 135, Loss: 2.1712\n",
      "Batch 136, Loss: 2.1499\n",
      "Batch 137, Loss: 2.0741\n",
      "Batch 138, Loss: 2.0946\n",
      "Batch 139, Loss: 2.1629\n",
      "Batch 140, Loss: 2.3074\n",
      "Batch 141, Loss: 2.3058\n",
      "Batch 142, Loss: 2.1991\n",
      "Batch 143, Loss: 2.1551\n",
      "Batch 144, Loss: 2.0831\n",
      "Batch 145, Loss: 2.1127\n",
      "Batch 146, Loss: 2.2786\n",
      "Batch 147, Loss: 2.3247\n",
      "Batch 148, Loss: 2.1826\n",
      "Batch 149, Loss: 2.2125\n",
      "Batch 150, Loss: 2.3088\n",
      "Batch 151, Loss: 2.3332\n",
      "Batch 152, Loss: 2.1609\n",
      "Batch 153, Loss: 2.2509\n",
      "Batch 154, Loss: 2.1841\n",
      "Batch 155, Loss: 2.1582\n",
      "Batch 156, Loss: 2.2861\n",
      "Batch 157, Loss: 2.0989\n",
      "Batch 158, Loss: 2.2366\n",
      "Batch 159, Loss: 2.1339\n",
      "Batch 160, Loss: 2.3428\n",
      "Batch 161, Loss: 2.2170\n",
      "Batch 162, Loss: 2.1478\n",
      "Batch 163, Loss: 2.2201\n",
      "Batch 164, Loss: 2.2200\n",
      "Batch 165, Loss: 2.3210\n",
      "Batch 166, Loss: 2.1358\n",
      "Batch 167, Loss: 2.0226\n",
      "Batch 168, Loss: 2.2518\n",
      "Batch 169, Loss: 2.2779\n",
      "Batch 170, Loss: 2.2342\n",
      "Batch 171, Loss: 2.1283\n",
      "Batch 172, Loss: 2.1434\n",
      "Batch 173, Loss: 2.1745\n",
      "Batch 174, Loss: 2.1090\n",
      "Batch 175, Loss: 2.2669\n",
      "Batch 176, Loss: 2.0376\n",
      "Batch 177, Loss: 2.0830\n",
      "Batch 178, Loss: 2.2524\n",
      "Batch 179, Loss: 2.1769\n",
      "Batch 180, Loss: 2.2273\n",
      "Batch 181, Loss: 2.0869\n",
      "Batch 182, Loss: 2.2152\n",
      "Batch 183, Loss: 2.2255\n",
      "Batch 184, Loss: 2.1701\n",
      "Batch 185, Loss: 2.1799\n",
      "Batch 186, Loss: 2.1614\n",
      "Batch 187, Loss: 2.0736\n",
      "Batch 188, Loss: 2.3200\n",
      "Batch 189, Loss: 2.1658\n",
      "Batch 190, Loss: 2.1649\n",
      "Batch 191, Loss: 2.2143\n",
      "Batch 192, Loss: 2.0990\n",
      "Batch 193, Loss: 2.0943\n",
      "Batch 194, Loss: 2.3216\n",
      "Batch 195, Loss: 2.1200\n",
      "Batch 196, Loss: 2.1420\n",
      "Batch 197, Loss: 2.3029\n",
      "Batch 198, Loss: 2.2872\n",
      "Batch 199, Loss: 2.1759\n",
      "Batch 200, Loss: 2.2944\n",
      "Once upon a time, while the sun started to shine. It was a deep voice as a song, and couldn't have their owner. The shopkeeper of all kinds of things \n",
      "in hast! The girl saw the shop of animals and scary was playing in the har. She picked up a shine. \"Wow don't bird fun,\" said the teacher. \"It's okay, \n",
      "I am stop them alone.\" She asked \"Did you know, just like this for some minutes.\" The boy replied: \"Now, why about you minutes!\" The shoper continue the friends stop \n",
      "under their shop. Some of the night sound you flew up into his mouth. It will fall from the top of the shin swimp and started to climb. The man \n",
      "laughed and waved at the top of the shinovel. While two, he was safet and the shin safely cat was happy and expensing a heavier. He was feeling happy for \n",
      "his travelight feelings. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 14.27 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.1489\n",
      "Batch 202, Loss: 2.1095\n",
      "Batch 203, Loss: 2.0690\n",
      "Batch 204, Loss: 2.0063\n",
      "Batch 205, Loss: 2.2038\n",
      "Batch 206, Loss: 2.2162\n",
      "Batch 207, Loss: 2.1425\n",
      "Batch 208, Loss: 2.0798\n",
      "Batch 209, Loss: 2.1517\n",
      "Batch 210, Loss: 2.0662\n",
      "Batch 211, Loss: 2.0583\n",
      "Batch 212, Loss: 2.0907\n",
      "Batch 213, Loss: 2.2473\n",
      "Batch 214, Loss: 2.0309\n",
      "Batch 215, Loss: 2.0454\n",
      "Batch 216, Loss: 2.0667\n",
      "Batch 217, Loss: 2.0447\n",
      "Batch 218, Loss: 2.0975\n",
      "Batch 219, Loss: 2.2953\n",
      "Batch 220, Loss: 2.0975\n",
      "Batch 221, Loss: 2.2013\n",
      "Batch 222, Loss: 2.1576\n",
      "Batch 223, Loss: 2.0876\n",
      "Batch 224, Loss: 2.2812\n",
      "Batch 225, Loss: 2.1070\n",
      "Batch 226, Loss: 2.1354\n",
      "Batch 227, Loss: 2.0497\n",
      "Batch 228, Loss: 2.2030\n",
      "Batch 229, Loss: 2.0294\n",
      "Batch 230, Loss: 2.2108\n",
      "Batch 231, Loss: 2.1897\n",
      "Batch 232, Loss: 2.2706\n",
      "Batch 233, Loss: 2.1054\n",
      "Batch 234, Loss: 2.1601\n",
      "Batch 235, Loss: 2.2891\n",
      "Batch 236, Loss: 2.1795\n",
      "Batch 237, Loss: 2.2888\n",
      "Batch 238, Loss: 2.3020\n",
      "Batch 239, Loss: 2.2888\n",
      "Batch 240, Loss: 2.1787\n",
      "Batch 241, Loss: 2.1329\n",
      "Batch 242, Loss: 2.2419\n",
      "Batch 243, Loss: 2.2597\n",
      "Batch 244, Loss: 2.0694\n",
      "Batch 245, Loss: 2.2838\n",
      "Batch 246, Loss: 2.1842\n",
      "Batch 247, Loss: 2.1359\n",
      "Batch 248, Loss: 2.0483\n",
      "Batch 249, Loss: 2.2309\n",
      "Batch 250, Loss: 2.0826\n",
      "Batch 251, Loss: 2.2511\n",
      "Batch 252, Loss: 2.0430\n",
      "Batch 253, Loss: 2.2767\n",
      "Batch 254, Loss: 2.2930\n",
      "Batch 255, Loss: 2.2861\n",
      "Batch 256, Loss: 2.2423\n",
      "Batch 257, Loss: 2.2498\n",
      "Batch 258, Loss: 2.3215\n",
      "Batch 259, Loss: 2.2343\n",
      "Batch 260, Loss: 2.2665\n",
      "Batch 261, Loss: 2.2037\n",
      "Batch 262, Loss: 2.0964\n",
      "Batch 263, Loss: 2.1386\n",
      "Batch 264, Loss: 2.2102\n",
      "Batch 265, Loss: 2.0872\n",
      "Batch 266, Loss: 2.2948\n",
      "Batch 267, Loss: 2.2447\n",
      "Batch 268, Loss: 2.2943\n",
      "Batch 269, Loss: 2.2630\n",
      "Batch 270, Loss: 2.1050\n",
      "Batch 271, Loss: 2.0631\n",
      "Batch 272, Loss: 2.2189\n",
      "Batch 273, Loss: 2.2299\n",
      "Batch 274, Loss: 2.1294\n",
      "Batch 275, Loss: 2.1524\n",
      "Batch 276, Loss: 2.0943\n",
      "Batch 277, Loss: 2.2039\n",
      "Batch 278, Loss: 2.0803\n",
      "Batch 279, Loss: 2.2179\n",
      "Batch 280, Loss: 2.0528\n",
      "Batch 281, Loss: 2.2886\n",
      "Batch 282, Loss: 2.2428\n",
      "Batch 283, Loss: 2.1437\n",
      "Batch 284, Loss: 2.2404\n",
      "Batch 285, Loss: 2.1438\n",
      "Batch 286, Loss: 2.2412\n",
      "Batch 287, Loss: 2.1588\n",
      "Batch 288, Loss: 2.0539\n",
      "Batch 289, Loss: 2.1675\n",
      "Batch 290, Loss: 2.0782\n",
      "Batch 291, Loss: 2.2590\n",
      "Batch 292, Loss: 2.1488\n",
      "Batch 293, Loss: 2.2071\n",
      "Batch 294, Loss: 2.0328\n",
      "Batch 295, Loss: 2.1004\n",
      "Batch 296, Loss: 2.2034\n",
      "Batch 297, Loss: 2.2296\n",
      "Batch 298, Loss: 2.1241\n",
      "Batch 299, Loss: 2.0516\n",
      "Batch 300, Loss: 2.1128\n",
      "Batch 301, Loss: 2.1116\n",
      "Batch 302, Loss: 2.1381\n",
      "Batch 303, Loss: 2.2897\n",
      "Batch 304, Loss: 2.2384\n",
      "Batch 305, Loss: 2.1680\n",
      "Batch 306, Loss: 2.1030\n",
      "Batch 307, Loss: 2.2243\n",
      "Batch 308, Loss: 2.2860\n",
      "Batch 309, Loss: 2.0653\n",
      "Batch 310, Loss: 2.2000\n",
      "Batch 311, Loss: 2.1811\n",
      "Batch 312, Loss: 2.2146\n",
      "Batch 313, Loss: 2.1869\n",
      "Batch 314, Loss: 2.2370\n",
      "Batch 315, Loss: 2.3195\n",
      "Batch 316, Loss: 2.1119\n",
      "Batch 317, Loss: 2.1664\n",
      "Batch 318, Loss: 2.1872\n",
      "Batch 319, Loss: 2.2069\n",
      "Batch 320, Loss: 2.1869\n",
      "Batch 321, Loss: 2.0816\n",
      "Batch 322, Loss: 2.2542\n",
      "Batch 323, Loss: 2.2160\n",
      "Batch 324, Loss: 2.3033\n",
      "Batch 325, Loss: 2.0500\n",
      "Batch 326, Loss: 2.1907\n",
      "Batch 327, Loss: 2.1874\n",
      "Batch 328, Loss: 2.1547\n",
      "Batch 329, Loss: 2.2756\n",
      "Batch 330, Loss: 2.2741\n",
      "Batch 331, Loss: 2.2774\n",
      "Batch 332, Loss: 2.1514\n",
      "Batch 333, Loss: 2.0527\n",
      "Batch 334, Loss: 2.2825\n",
      "Batch 335, Loss: 2.0892\n",
      "Batch 336, Loss: 2.1386\n",
      "Batch 337, Loss: 2.2067\n",
      "Batch 338, Loss: 2.0806\n",
      "Batch 339, Loss: 1.9963\n",
      "Batch 340, Loss: 2.2047\n",
      "Batch 341, Loss: 2.1948\n",
      "Batch 342, Loss: 2.1956\n",
      "Batch 343, Loss: 2.2550\n",
      "Batch 344, Loss: 1.9949\n",
      "Batch 345, Loss: 2.1727\n",
      "Batch 346, Loss: 2.1600\n",
      "Batch 347, Loss: 2.0075\n",
      "Batch 348, Loss: 2.2654\n",
      "Batch 349, Loss: 2.1386\n",
      "Batch 350, Loss: 2.1917\n",
      "Batch 351, Loss: 2.0942\n",
      "Batch 352, Loss: 2.0586\n",
      "Batch 353, Loss: 2.1070\n",
      "Batch 354, Loss: 2.1158\n",
      "Batch 355, Loss: 2.1290\n",
      "Batch 356, Loss: 2.0852\n",
      "Batch 357, Loss: 2.0452\n",
      "Batch 358, Loss: 2.1873\n",
      "Batch 359, Loss: 2.1747\n",
      "Batch 360, Loss: 2.0623\n",
      "Batch 361, Loss: 2.1245\n",
      "Batch 362, Loss: 2.1664\n",
      "Batch 363, Loss: 2.2340\n",
      "Batch 364, Loss: 2.1857\n",
      "Batch 365, Loss: 2.2195\n",
      "Batch 366, Loss: 2.0534\n",
      "Batch 367, Loss: 2.1724\n",
      "Batch 368, Loss: 2.1756\n",
      "Batch 369, Loss: 2.2807\n",
      "Batch 370, Loss: 2.2479\n",
      "Batch 371, Loss: 2.2780\n",
      "Batch 372, Loss: 2.1997\n",
      "Batch 373, Loss: 2.2195\n",
      "Batch 374, Loss: 2.1234\n",
      "Batch 375, Loss: 2.2556\n",
      "Batch 376, Loss: 2.1100\n",
      "Batch 377, Loss: 2.1730\n",
      "Batch 378, Loss: 2.0892\n",
      "Batch 379, Loss: 2.0988\n",
      "Batch 380, Loss: 2.2424\n",
      "Batch 381, Loss: 2.2331\n",
      "Batch 382, Loss: 2.1790\n",
      "Batch 383, Loss: 2.1179\n",
      "Batch 384, Loss: 2.1517\n",
      "Batch 385, Loss: 2.1360\n",
      "Batch 386, Loss: 2.1572\n",
      "Batch 387, Loss: 2.2783\n",
      "Batch 388, Loss: 2.2224\n",
      "Batch 389, Loss: 2.1382\n",
      "Batch 390, Loss: 2.1312\n",
      "Batch 391, Loss: 2.1280\n",
      "Batch 392, Loss: 2.2614\n",
      "Batch 393, Loss: 2.1763\n",
      "Batch 394, Loss: 2.0925\n",
      "Batch 395, Loss: 2.1620\n",
      "Batch 396, Loss: 1.9911\n",
      "Batch 397, Loss: 2.1322\n",
      "Batch 398, Loss: 2.2361\n",
      "Batch 399, Loss: 2.2940\n",
      "Batch 400, Loss: 2.0784\n",
      "Once upon a time there were two friends. Rocky wanted to order the world around him. So he said to him, \"Yes!\" It said might finding it to a little \n",
      "first places - itâ€™d get together and needed to sit. Grandma would help the world to help come. One day, as Jack left with a big box to make a \n",
      "mysterious row rot. In her fit but her most pair so it could visit! The two of them filled the cray into a beautiful party with a strange. They both \n",
      "laughed at the rest of the creature that her secret had just a the scail. \"Whank youth helping me. This is a great day together. Now you are the best \n",
      "problem?\" asked Mum smiled with joy. \"It has a special choiceman, then look for - it looks like it's wonderful to explore. Rocky went over to the living room. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 12.19 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 401, Loss: 2.0765\n",
      "Batch 402, Loss: 1.9997\n",
      "Batch 403, Loss: 2.2684\n",
      "Batch 404, Loss: 2.1452\n",
      "Batch 405, Loss: 2.1473\n",
      "Batch 406, Loss: 2.2252\n",
      "Batch 407, Loss: 2.1221\n",
      "Batch 408, Loss: 2.0700\n",
      "Batch 409, Loss: 2.0400\n",
      "Batch 410, Loss: 2.2155\n",
      "Batch 411, Loss: 2.0315\n",
      "Batch 412, Loss: 2.2448\n",
      "Batch 413, Loss: 2.3259\n",
      "Batch 414, Loss: 2.2193\n",
      "Batch 415, Loss: 2.1298\n",
      "Batch 416, Loss: 2.0949\n",
      "Batch 417, Loss: 2.2218\n",
      "Batch 418, Loss: 2.2334\n",
      "Epoch 6, Average Loss: 2.1833\n",
      "Once upon a time, there was a little bit numbers. It morning to raise admired things wear exciting things like the most of the stories. The little girl was so \n",
      "excited and she blew in the object. She started to mescaped again when the shopterfly. She wished she had seen in the ric castle without as a part of herself. \n",
      "The best worries of pattern safet, kids, and had lots of fun. But then far away, catâ€™s dad talking. The sun was so happy! All often could run and exploring \n",
      "the sun, caring people around the party. He' It was very good to her. The three-old was so amazing and happily ever after. He would wearing a look around in \n",
      "bed. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.0914\n",
      "Batch 2, Loss: 2.2543\n",
      "Batch 3, Loss: 2.0054\n",
      "Batch 4, Loss: 2.1558\n",
      "Batch 5, Loss: 2.0930\n",
      "Batch 6, Loss: 2.2040\n",
      "Batch 7, Loss: 2.2094\n",
      "Batch 8, Loss: 2.1411\n",
      "Batch 9, Loss: 2.1288\n",
      "Batch 10, Loss: 1.9734\n",
      "Batch 11, Loss: 2.0222\n",
      "Batch 12, Loss: 2.2069\n",
      "Batch 13, Loss: 2.0882\n",
      "Batch 14, Loss: 2.1220\n",
      "Batch 15, Loss: 2.1120\n",
      "Batch 16, Loss: 1.9923\n",
      "Batch 17, Loss: 2.2344\n",
      "Batch 18, Loss: 2.0252\n",
      "Batch 19, Loss: 2.1749\n",
      "Batch 20, Loss: 2.0852\n",
      "Batch 21, Loss: 2.1491\n",
      "Batch 22, Loss: 2.2155\n",
      "Batch 23, Loss: 2.2464\n",
      "Batch 24, Loss: 2.0624\n",
      "Batch 25, Loss: 2.2510\n",
      "Batch 26, Loss: 2.1357\n",
      "Batch 27, Loss: 2.0853\n",
      "Batch 28, Loss: 2.2025\n",
      "Batch 29, Loss: 2.0208\n",
      "Batch 30, Loss: 2.1273\n",
      "Batch 31, Loss: 2.1044\n",
      "Batch 32, Loss: 2.1372\n",
      "Batch 33, Loss: 1.9980\n",
      "Batch 34, Loss: 2.1592\n",
      "Batch 35, Loss: 2.1471\n",
      "Batch 36, Loss: 2.1531\n",
      "Batch 37, Loss: 2.1677\n",
      "Batch 38, Loss: 2.2068\n",
      "Batch 39, Loss: 2.1474\n",
      "Batch 40, Loss: 2.1866\n",
      "Batch 41, Loss: 2.2079\n",
      "Batch 42, Loss: 2.1496\n",
      "Batch 43, Loss: 2.2497\n",
      "Batch 44, Loss: 2.0634\n",
      "Batch 45, Loss: 2.1471\n",
      "Batch 46, Loss: 2.0925\n",
      "Batch 47, Loss: 2.2130\n",
      "Batch 48, Loss: 2.1979\n",
      "Batch 49, Loss: 2.2643\n",
      "Batch 50, Loss: 2.2183\n",
      "Batch 51, Loss: 2.0958\n",
      "Batch 52, Loss: 2.1381\n",
      "Batch 53, Loss: 2.3359\n",
      "Batch 54, Loss: 2.2315\n",
      "Batch 55, Loss: 2.1215\n",
      "Batch 56, Loss: 2.2826\n",
      "Batch 57, Loss: 2.1663\n",
      "Batch 58, Loss: 2.2049\n",
      "Batch 59, Loss: 2.0348\n",
      "Batch 60, Loss: 2.0581\n",
      "Batch 61, Loss: 2.1577\n",
      "Batch 62, Loss: 2.1561\n",
      "Batch 63, Loss: 2.1565\n",
      "Batch 64, Loss: 2.1268\n",
      "Batch 65, Loss: 2.2155\n",
      "Batch 66, Loss: 2.1384\n",
      "Batch 67, Loss: 2.0504\n",
      "Batch 68, Loss: 2.0634\n",
      "Batch 69, Loss: 2.0429\n",
      "Batch 70, Loss: 2.1224\n",
      "Batch 71, Loss: 2.2634\n",
      "Batch 72, Loss: 2.0908\n",
      "Batch 73, Loss: 2.2188\n",
      "Batch 74, Loss: 2.1961\n",
      "Batch 75, Loss: 2.0757\n",
      "Batch 76, Loss: 2.0220\n",
      "Batch 77, Loss: 2.0452\n",
      "Batch 78, Loss: 2.1240\n",
      "Batch 79, Loss: 2.0883\n",
      "Batch 80, Loss: 2.0238\n",
      "Batch 81, Loss: 2.1353\n",
      "Batch 82, Loss: 1.9729\n",
      "Batch 83, Loss: 2.2507\n",
      "Batch 84, Loss: 2.2060\n",
      "Batch 85, Loss: 2.0161\n",
      "Batch 86, Loss: 2.0213\n",
      "Batch 87, Loss: 2.0212\n",
      "Batch 88, Loss: 2.3219\n",
      "Batch 89, Loss: 2.2690\n",
      "Batch 90, Loss: 2.0470\n",
      "Batch 91, Loss: 2.2745\n",
      "Batch 92, Loss: 1.9698\n",
      "Batch 93, Loss: 2.0943\n",
      "Batch 94, Loss: 2.2009\n",
      "Batch 95, Loss: 2.0456\n",
      "Batch 96, Loss: 2.1414\n",
      "Batch 97, Loss: 2.0337\n",
      "Batch 98, Loss: 2.0562\n",
      "Batch 99, Loss: 2.0069\n",
      "Batch 100, Loss: 2.1090\n",
      "Batch 101, Loss: 2.1165\n",
      "Batch 102, Loss: 2.0672\n",
      "Batch 103, Loss: 2.0435\n",
      "Batch 104, Loss: 1.9985\n",
      "Batch 105, Loss: 2.2263\n",
      "Batch 106, Loss: 2.0578\n",
      "Batch 107, Loss: 2.0079\n",
      "Batch 108, Loss: 2.0184\n",
      "Batch 109, Loss: 2.1185\n",
      "Batch 110, Loss: 2.0369\n",
      "Batch 111, Loss: 2.2042\n",
      "Batch 112, Loss: 2.0808\n",
      "Batch 113, Loss: 1.9879\n",
      "Batch 114, Loss: 1.9897\n",
      "Batch 115, Loss: 2.0780\n",
      "Batch 116, Loss: 2.2775\n",
      "Batch 117, Loss: 1.9921\n",
      "Batch 118, Loss: 2.2340\n",
      "Batch 119, Loss: 2.2271\n",
      "Batch 120, Loss: 2.0470\n",
      "Batch 121, Loss: 2.2359\n",
      "Batch 122, Loss: 2.0240\n",
      "Batch 123, Loss: 2.0363\n",
      "Batch 124, Loss: 2.1349\n",
      "Batch 125, Loss: 2.1783\n",
      "Batch 126, Loss: 2.1201\n",
      "Batch 127, Loss: 2.1989\n",
      "Batch 128, Loss: 2.0614\n",
      "Batch 129, Loss: 2.1353\n",
      "Batch 130, Loss: 2.0394\n",
      "Batch 131, Loss: 2.1684\n",
      "Batch 132, Loss: 1.9791\n",
      "Batch 133, Loss: 2.0915\n",
      "Batch 134, Loss: 2.2351\n",
      "Batch 135, Loss: 2.1523\n",
      "Batch 136, Loss: 2.0567\n",
      "Batch 137, Loss: 2.0837\n",
      "Batch 138, Loss: 1.9775\n",
      "Batch 139, Loss: 1.9961\n",
      "Batch 140, Loss: 2.0597\n",
      "Batch 141, Loss: 2.1073\n",
      "Batch 142, Loss: 2.1752\n",
      "Batch 143, Loss: 2.1184\n",
      "Batch 144, Loss: 2.0308\n",
      "Batch 145, Loss: 1.9855\n",
      "Batch 146, Loss: 2.2951\n",
      "Batch 147, Loss: 2.0413\n",
      "Batch 148, Loss: 2.1744\n",
      "Batch 149, Loss: 2.0220\n",
      "Batch 150, Loss: 1.9797\n",
      "Batch 151, Loss: 2.0061\n",
      "Batch 152, Loss: 2.2699\n",
      "Batch 153, Loss: 2.0981\n",
      "Batch 154, Loss: 2.1644\n",
      "Batch 155, Loss: 2.0350\n",
      "Batch 156, Loss: 2.1712\n",
      "Batch 157, Loss: 2.2401\n",
      "Batch 158, Loss: 2.1973\n",
      "Batch 159, Loss: 2.1794\n",
      "Batch 160, Loss: 2.2290\n",
      "Batch 161, Loss: 2.1935\n",
      "Batch 162, Loss: 2.0211\n",
      "Batch 163, Loss: 2.0452\n",
      "Batch 164, Loss: 2.0080\n",
      "Batch 165, Loss: 2.1706\n",
      "Batch 166, Loss: 1.9655\n",
      "Batch 167, Loss: 2.2048\n",
      "Batch 168, Loss: 2.0879\n",
      "Batch 169, Loss: 2.2313\n",
      "Batch 170, Loss: 2.1629\n",
      "Batch 171, Loss: 2.1466\n",
      "Batch 172, Loss: 2.2055\n",
      "Batch 173, Loss: 2.1884\n",
      "Batch 174, Loss: 1.9577\n",
      "Batch 175, Loss: 2.0723\n",
      "Batch 176, Loss: 2.2333\n",
      "Batch 177, Loss: 1.9567\n",
      "Batch 178, Loss: 2.1732\n",
      "Batch 179, Loss: 2.0743\n",
      "Batch 180, Loss: 2.1092\n",
      "Batch 181, Loss: 2.0221\n",
      "Batch 182, Loss: 1.9950\n",
      "Batch 183, Loss: 2.0705\n",
      "Batch 184, Loss: 2.1910\n",
      "Batch 185, Loss: 2.1429\n",
      "Batch 186, Loss: 1.9844\n",
      "Batch 187, Loss: 2.0091\n",
      "Batch 188, Loss: 2.1899\n",
      "Batch 189, Loss: 2.0067\n",
      "Batch 190, Loss: 2.0668\n",
      "Batch 191, Loss: 2.1499\n",
      "Batch 192, Loss: 2.1161\n",
      "Batch 193, Loss: 1.9797\n",
      "Batch 194, Loss: 2.2015\n",
      "Batch 195, Loss: 2.0360\n",
      "Batch 196, Loss: 2.0724\n",
      "Batch 197, Loss: 1.9619\n",
      "Batch 198, Loss: 2.1902\n",
      "Batch 199, Loss: 2.1615\n",
      "Batch 200, Loss: 1.9401\n",
      "Once upon a time, there was a young boy named Tim. Tim loved to play outside and play with his friends. One day, Tim's friend Fusie couldn't play with his \n",
      "friends. Tim wanted to play with his friends. One day, Tim fet go to go to Spike's room to play with Spe. The squirrel got loosh tylin's car to the \n",
      "table. Tim started to movell fast and play with his friends. When the sun came out, Tim realized, they played nicely with Span in the park. They played all day. \n",
      "Timmy loved to play with Mama and Spot's favorite toys were his favorite dog. Spot toys, even if he played with Spie. When it was time to go home, they \n",
      "jumped back. Tim gave Spot very much and looked happy too. From on, they always knew that Spot could bal Spot! \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 14.09 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 201, Loss: 2.0803\n",
      "Batch 202, Loss: 2.1655\n",
      "Batch 203, Loss: 2.2625\n",
      "Batch 204, Loss: 2.0839\n",
      "Batch 205, Loss: 2.0920\n",
      "Batch 206, Loss: 2.2554\n",
      "Batch 207, Loss: 2.1356\n",
      "Batch 208, Loss: 2.2310\n",
      "Batch 209, Loss: 2.0651\n",
      "Batch 210, Loss: 2.1863\n",
      "Batch 211, Loss: 2.1866\n",
      "Batch 212, Loss: 2.1910\n",
      "Batch 213, Loss: 2.1408\n",
      "Batch 214, Loss: 2.1889\n",
      "Batch 215, Loss: 2.1849\n",
      "Batch 216, Loss: 2.1159\n",
      "Batch 217, Loss: 2.1149\n",
      "Batch 218, Loss: 2.2276\n",
      "Batch 219, Loss: 2.0914\n",
      "Batch 220, Loss: 2.0980\n",
      "Batch 221, Loss: 2.1341\n",
      "Batch 222, Loss: 2.1000\n",
      "Batch 223, Loss: 2.1914\n",
      "Batch 224, Loss: 2.2113\n",
      "Batch 225, Loss: 2.1694\n",
      "Batch 226, Loss: 2.0527\n",
      "Batch 227, Loss: 2.1458\n",
      "Batch 228, Loss: 2.0698\n",
      "Batch 229, Loss: 2.2015\n",
      "Batch 230, Loss: 2.1219\n",
      "Batch 231, Loss: 2.2020\n",
      "Batch 232, Loss: 2.1060\n",
      "Batch 233, Loss: 2.0910\n",
      "Batch 234, Loss: 1.9766\n",
      "Batch 235, Loss: 2.1932\n",
      "Batch 236, Loss: 2.2074\n",
      "Batch 237, Loss: 2.0058\n",
      "Batch 238, Loss: 2.0515\n",
      "Batch 239, Loss: 2.0166\n",
      "Batch 240, Loss: 2.1208\n",
      "Batch 241, Loss: 2.0779\n",
      "Batch 242, Loss: 2.2263\n",
      "Batch 243, Loss: 1.9758\n",
      "Batch 244, Loss: 1.9912\n",
      "Batch 245, Loss: 2.1926\n",
      "Batch 246, Loss: 2.2085\n",
      "Batch 247, Loss: 2.1491\n",
      "Batch 248, Loss: 2.0265\n",
      "Batch 249, Loss: 2.2343\n",
      "Batch 250, Loss: 2.0837\n",
      "Batch 251, Loss: 2.1048\n",
      "Batch 252, Loss: 2.0232\n",
      "Batch 253, Loss: 2.1774\n",
      "Batch 254, Loss: 2.1444\n",
      "Batch 255, Loss: 2.1972\n",
      "Batch 256, Loss: 2.2275\n",
      "Batch 257, Loss: 2.0482\n",
      "Batch 258, Loss: 2.0219\n",
      "Batch 259, Loss: 2.1783\n",
      "Batch 260, Loss: 1.9821\n",
      "Batch 261, Loss: 2.1733\n",
      "Batch 262, Loss: 2.2269\n",
      "Batch 263, Loss: 2.0498\n",
      "Batch 264, Loss: 1.9784\n",
      "Batch 265, Loss: 2.1428\n",
      "Batch 266, Loss: 2.0798\n",
      "Batch 267, Loss: 2.1868\n",
      "Batch 268, Loss: 2.0605\n",
      "Batch 269, Loss: 2.0826\n",
      "Batch 270, Loss: 2.1044\n",
      "Batch 271, Loss: 1.9849\n",
      "Batch 272, Loss: 2.0518\n",
      "Batch 273, Loss: 2.1666\n",
      "Batch 274, Loss: 2.0905\n",
      "Batch 275, Loss: 1.9652\n",
      "Batch 276, Loss: 2.1168\n",
      "Batch 277, Loss: 2.2529\n",
      "Batch 278, Loss: 2.0090\n",
      "Batch 279, Loss: 2.1559\n",
      "Batch 280, Loss: 2.0053\n",
      "Batch 281, Loss: 2.0116\n",
      "Batch 282, Loss: 2.0104\n",
      "Batch 283, Loss: 1.9572\n",
      "Batch 284, Loss: 2.0589\n",
      "Batch 285, Loss: 2.1916\n",
      "Batch 286, Loss: 2.1384\n",
      "Batch 287, Loss: 2.0271\n",
      "Batch 288, Loss: 2.2155\n",
      "Batch 289, Loss: 2.2373\n",
      "Batch 290, Loss: 2.1060\n",
      "Batch 291, Loss: 2.0532\n",
      "Batch 292, Loss: 2.1425\n",
      "Batch 293, Loss: 2.1807\n",
      "Batch 294, Loss: 2.2528\n",
      "Batch 295, Loss: 2.1694\n",
      "Batch 296, Loss: 1.9966\n",
      "Batch 297, Loss: 2.1314\n",
      "Batch 298, Loss: 2.0565\n",
      "Batch 299, Loss: 2.2842\n",
      "Batch 300, Loss: 1.9626\n",
      "Batch 301, Loss: 2.0755\n",
      "Batch 302, Loss: 2.0873\n",
      "Batch 303, Loss: 2.2099\n",
      "Batch 304, Loss: 2.1951\n",
      "Batch 305, Loss: 2.0185\n",
      "Batch 306, Loss: 1.9745\n",
      "Batch 307, Loss: 2.0360\n",
      "Batch 308, Loss: 2.1788\n",
      "Batch 309, Loss: 2.1583\n",
      "Batch 310, Loss: 2.1136\n",
      "Batch 311, Loss: 2.0636\n",
      "Batch 312, Loss: 2.2048\n",
      "Batch 313, Loss: 1.9585\n",
      "Batch 314, Loss: 2.0798\n",
      "Batch 315, Loss: 2.1231\n",
      "Batch 316, Loss: 2.1605\n",
      "Batch 317, Loss: 2.0894\n",
      "Batch 318, Loss: 2.0881\n",
      "Batch 319, Loss: 2.0180\n",
      "Batch 320, Loss: 2.0909\n",
      "Batch 321, Loss: 2.0204\n",
      "Batch 322, Loss: 2.2075\n",
      "Batch 323, Loss: 2.1313\n",
      "Batch 324, Loss: 2.1206\n",
      "Batch 325, Loss: 2.1464\n",
      "Batch 326, Loss: 2.0056\n",
      "Batch 327, Loss: 2.0313\n",
      "Batch 328, Loss: 2.1750\n",
      "Batch 329, Loss: 2.1883\n",
      "Batch 330, Loss: 2.1729\n",
      "Batch 331, Loss: 2.1497\n",
      "Batch 332, Loss: 2.0190\n",
      "Batch 333, Loss: 2.1244\n",
      "Batch 334, Loss: 1.9793\n",
      "Batch 335, Loss: 1.9972\n",
      "Batch 336, Loss: 2.1861\n",
      "Batch 337, Loss: 1.9416\n",
      "Batch 338, Loss: 2.1521\n",
      "Batch 339, Loss: 2.1524\n",
      "Batch 340, Loss: 1.9308\n",
      "Batch 341, Loss: 2.1940\n",
      "Batch 342, Loss: 2.1168\n",
      "Batch 343, Loss: 2.0312\n",
      "Batch 344, Loss: 2.1495\n",
      "Batch 345, Loss: 2.1947\n",
      "Batch 346, Loss: 2.1700\n",
      "Batch 347, Loss: 2.0592\n",
      "Batch 348, Loss: 2.0878\n",
      "Batch 349, Loss: 2.0762\n",
      "Batch 350, Loss: 2.1423\n",
      "Batch 351, Loss: 2.1911\n",
      "Batch 352, Loss: 1.9167\n",
      "Batch 353, Loss: 2.1460\n",
      "Batch 354, Loss: 2.2047\n",
      "Batch 355, Loss: 2.1376\n",
      "Batch 356, Loss: 2.1973\n",
      "Batch 357, Loss: 2.0205\n",
      "Batch 358, Loss: 1.9142\n",
      "Batch 359, Loss: 2.0124\n",
      "Batch 360, Loss: 2.0302\n",
      "Batch 361, Loss: 2.1841\n",
      "Batch 362, Loss: 2.0709\n",
      "Batch 363, Loss: 2.0300\n",
      "Batch 364, Loss: 2.0094\n",
      "Batch 365, Loss: 2.2128\n",
      "Batch 366, Loss: 2.1758\n",
      "Batch 367, Loss: 2.1919\n",
      "Batch 368, Loss: 2.2408\n",
      "Batch 369, Loss: 2.0689\n",
      "Batch 370, Loss: 2.0578\n",
      "Batch 371, Loss: 2.1241\n",
      "Batch 372, Loss: 2.1204\n",
      "Batch 373, Loss: 2.1896\n",
      "Batch 374, Loss: 2.1651\n",
      "Batch 375, Loss: 2.1289\n",
      "Batch 376, Loss: 2.1061\n",
      "Batch 377, Loss: 1.9830\n",
      "Batch 378, Loss: 2.1266\n",
      "Batch 379, Loss: 2.0508\n",
      "Batch 380, Loss: 2.2046\n",
      "Batch 381, Loss: 2.0954\n",
      "Batch 382, Loss: 2.0967\n",
      "Batch 383, Loss: 2.2078\n",
      "Batch 384, Loss: 2.0664\n",
      "Batch 385, Loss: 2.0199\n",
      "Batch 386, Loss: 2.1165\n",
      "Batch 387, Loss: 2.1406\n",
      "Batch 388, Loss: 2.1890\n",
      "Batch 389, Loss: 2.0158\n",
      "Batch 390, Loss: 2.1352\n",
      "Batch 391, Loss: 1.9624\n",
      "Batch 392, Loss: 1.9629\n",
      "Batch 393, Loss: 2.1456\n",
      "Batch 394, Loss: 2.1709\n",
      "Batch 395, Loss: 2.0590\n",
      "Batch 396, Loss: 2.1575\n",
      "Batch 397, Loss: 2.0288\n",
      "Batch 398, Loss: 1.9498\n",
      "Batch 399, Loss: 2.1536\n",
      "Batch 400, Loss: 2.0616\n",
      "Once upon a time, there was a big farmer named Fredie. Bred had a bead and was always excited to hear his bucket to the woods near his home. One \n",
      "sunny day, the farmer decided to bloom. As Fred lived in a park, with a beautiful truck, a wolfish gem and he decided to peek out. Fred was so happy. \n",
      "He asked, why he had a plan. Fred wanted to keep himself better. With his content, but he knew he had to buy a ring full of tunnel. Fred always, \n",
      "he saw something. He picked up the mess from all the fancy soc opened and saw lots of fish, running out! Fred wanted to wave the fancy embreaky's eyes of \n",
      "his truck. He was surprised but happy that he had didn't have seen a delicious for his farm. But Fred away from up the hallway, he quickly ran back to \n",
      "solve with it. Fred out, the fan lit upside the fancy fancy home from his farmer, and embarrained it with Fredier. Fred away from then give up. He kept upsuce \n",
      "and knew he had made his clever gement back. \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Active memory: 0.02 GB\n",
      "Cache memory: 20.13 GB\n",
      "Peak memory: 37.54 GB\n",
      "--------------------\n",
      "Batch 401, Loss: 2.0511\n",
      "Batch 402, Loss: 2.1271\n",
      "Batch 403, Loss: 2.0753\n",
      "Batch 404, Loss: 2.1682\n",
      "Batch 405, Loss: 2.0851\n",
      "Batch 406, Loss: 2.1908\n",
      "Batch 407, Loss: 2.0730\n",
      "Batch 408, Loss: 2.1449\n",
      "Batch 409, Loss: 1.9073\n",
      "Batch 410, Loss: 1.9358\n",
      "Batch 411, Loss: 2.1639\n",
      "Batch 412, Loss: 2.0541\n",
      "Batch 413, Loss: 2.0201\n",
      "Batch 414, Loss: 2.1721\n",
      "Batch 415, Loss: 2.0738\n",
      "Batch 416, Loss: 2.0822\n",
      "Batch 417, Loss: 1.9610\n",
      "Batch 418, Loss: 2.0708\n",
      "Epoch 7, Average Loss: 2.1109\n",
      "Once upon a time, there was a little girl named Lily. She loved to play in the park with her friends. One day, Lily's mom told her to spike with \n",
      "a clopard. Lily ran to her hands and got so yummy to hurt her. She could figa it all day. Lily started to pack her new clop all up some \n",
      "more and squeezed. She ate all of Lily's clothes and mixed her wound towers together and her family was very happy too. When they got to the clop, Lily remembered \n",
      "to have a lucky chasing Lily. After a young picking off her cloth was happy to have a fun day. From that day on, Lily played together every day and \n",
      "never comforted. \n",
      "\n",
      "--------------------\n",
      "Batch 1, Loss: 2.1114\n",
      "Batch 2, Loss: 2.2109\n",
      "Batch 3, Loss: 2.1157\n",
      "Batch 4, Loss: 2.2205\n",
      "Batch 5, Loss: 1.9850\n",
      "Batch 6, Loss: 1.9499\n",
      "Batch 7, Loss: 2.2251\n",
      "Batch 8, Loss: 2.1926\n",
      "Batch 9, Loss: 1.9239\n",
      "Batch 10, Loss: 2.1492\n",
      "Batch 11, Loss: 2.2288\n",
      "Batch 12, Loss: 2.0196\n",
      "Batch 13, Loss: 2.0317\n",
      "Batch 14, Loss: 2.1606\n",
      "Batch 15, Loss: 2.1241\n",
      "Batch 16, Loss: 2.1901\n",
      "Batch 17, Loss: 2.0716\n",
      "Batch 18, Loss: 2.1458\n",
      "Batch 19, Loss: 1.9668\n",
      "Batch 20, Loss: 2.0364\n",
      "Batch 21, Loss: 1.9391\n",
      "Batch 22, Loss: 1.9992\n",
      "Batch 23, Loss: 2.0115\n",
      "Batch 24, Loss: 1.9783\n",
      "Batch 25, Loss: 2.0866\n",
      "Batch 26, Loss: 2.1399\n",
      "Batch 27, Loss: 1.9518\n",
      "Batch 28, Loss: 2.0590\n",
      "Batch 29, Loss: 1.8943\n",
      "Batch 30, Loss: 2.1202\n",
      "Batch 31, Loss: 1.9141\n",
      "Batch 32, Loss: 2.1011\n",
      "Batch 33, Loss: 2.1905\n",
      "Batch 34, Loss: 2.0497\n",
      "Batch 35, Loss: 2.0835\n",
      "Batch 36, Loss: 2.0961\n",
      "Batch 37, Loss: 1.9689\n",
      "Batch 38, Loss: 1.9392\n",
      "Batch 39, Loss: 2.0156\n",
      "Batch 40, Loss: 1.9859\n",
      "Batch 41, Loss: 2.2075\n",
      "Batch 42, Loss: 2.0621\n",
      "Batch 43, Loss: 2.0145\n",
      "Batch 44, Loss: 2.0838\n",
      "Batch 45, Loss: 2.2084\n",
      "Batch 46, Loss: 2.2047\n",
      "Batch 47, Loss: 2.0960\n",
      "Batch 48, Loss: 2.1762\n",
      "Batch 49, Loss: 2.0249\n",
      "Batch 50, Loss: 2.0551\n",
      "Batch 51, Loss: 2.0242\n",
      "Batch 52, Loss: 1.9799\n",
      "Batch 53, Loss: 2.0876\n",
      "Batch 54, Loss: 2.1443\n",
      "Batch 55, Loss: 2.1148\n",
      "Batch 56, Loss: 2.0979\n",
      "Batch 57, Loss: 1.9339\n",
      "Batch 58, Loss: 2.1215\n",
      "Batch 59, Loss: 2.0561\n",
      "Batch 60, Loss: 2.0979\n",
      "Batch 61, Loss: 2.0930\n",
      "Batch 62, Loss: 2.0203\n",
      "Batch 63, Loss: 2.1607\n",
      "Batch 64, Loss: 2.1317\n",
      "Batch 65, Loss: 2.1658\n",
      "Batch 66, Loss: 2.2074\n",
      "Batch 67, Loss: 1.9843\n",
      "Batch 68, Loss: 2.1048\n",
      "Batch 69, Loss: 1.9426\n",
      "Batch 70, Loss: 2.0921\n",
      "Batch 71, Loss: 2.1138\n",
      "Batch 72, Loss: 2.1228\n",
      "Batch 73, Loss: 1.9593\n",
      "Batch 74, Loss: 2.1241\n",
      "Batch 75, Loss: 2.0963\n",
      "Batch 76, Loss: 2.1224\n",
      "Batch 77, Loss: 2.1690\n",
      "Batch 78, Loss: 2.1152\n",
      "Batch 79, Loss: 1.9805\n",
      "Batch 80, Loss: 2.0540\n",
      "Batch 81, Loss: 1.9456\n",
      "Batch 82, Loss: 1.9866\n",
      "Batch 83, Loss: 2.1032\n",
      "Batch 84, Loss: 2.1091\n",
      "Batch 85, Loss: 2.1631\n",
      "Batch 86, Loss: 2.0761\n",
      "Batch 87, Loss: 2.1347\n",
      "Batch 88, Loss: 2.0871\n",
      "Batch 89, Loss: 2.0918\n",
      "Batch 90, Loss: 2.0064\n",
      "Batch 91, Loss: 2.1854\n",
      "Batch 92, Loss: 2.1554\n",
      "Batch 93, Loss: 2.0199\n",
      "Batch 94, Loss: 2.1632\n",
      "Batch 95, Loss: 2.1140\n",
      "Batch 96, Loss: 2.1045\n",
      "Batch 97, Loss: 2.0931\n",
      "Batch 98, Loss: 2.0003\n",
      "Batch 99, Loss: 1.9771\n",
      "Batch 100, Loss: 2.1973\n",
      "Batch 101, Loss: 2.1838\n",
      "Batch 102, Loss: 2.0907\n",
      "Batch 103, Loss: 1.9492\n",
      "Batch 104, Loss: 2.0096\n",
      "Batch 105, Loss: 2.0165\n",
      "Batch 106, Loss: 2.1186\n",
      "Batch 107, Loss: 2.2197\n",
      "Batch 108, Loss: 1.9048\n",
      "Batch 109, Loss: 1.9218\n",
      "Batch 110, Loss: 1.9719\n",
      "Batch 111, Loss: 2.2164\n",
      "Batch 112, Loss: 2.2065\n",
      "Batch 113, Loss: 2.1703\n",
      "Batch 114, Loss: 2.2257\n",
      "Batch 115, Loss: 2.0116\n",
      "Batch 116, Loss: 2.1559\n",
      "Batch 117, Loss: 1.9126\n",
      "Batch 118, Loss: 2.0157\n",
      "Batch 119, Loss: 1.9592\n",
      "Batch 120, Loss: 2.2099\n",
      "Batch 121, Loss: 2.0908\n",
      "Batch 122, Loss: 2.1139\n",
      "Batch 123, Loss: 1.9326\n",
      "Batch 124, Loss: 1.9584\n",
      "Batch 125, Loss: 2.1316\n",
      "Batch 126, Loss: 1.9458\n",
      "Batch 127, Loss: 1.9452\n",
      "Batch 128, Loss: 2.1524\n",
      "Batch 129, Loss: 2.0846\n",
      "Batch 130, Loss: 2.1541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m loss, grads = loss_and_grad(model, input_seq, target_seq)\n\u001b[32m     13\u001b[39m optimizer.update(model, grads)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mmx\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i+\u001b[32m1\u001b[39m) % save_freq == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "save_freq = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    if last_epoch and epoch + 1 < last_epoch:\n",
    "        continue\n",
    "    for i, seq in enumerate(stream):\n",
    "        if last_batch and i + 1 <= last_batch:\n",
    "            continue\n",
    "        mx_seq = mx.array(seq[DICT_LABEL])\n",
    "        input_seq = mx_seq[:, :-1]  # Exclude the last token for input\n",
    "        target_seq = mx_seq[:, 1:]  # Exclude the first token for target\n",
    "        loss, grads = loss_and_grad(model, input_seq, target_seq)\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        print(f\"Batch {i + 1}, Loss: {loss:.4f}\")\n",
    "        if (i+1) % save_freq == 0:\n",
    "            generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "            model.save_weights(f'./data/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1}.npz')\n",
    "            if i+1 != save_freq: os.remove(f'./data/model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{epoch+1}_{i+1-save_freq}.npz') if i > 0 else None\n",
    "            print('-'*20)\n",
    "            print(f\"Active memory: {mx.get_active_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Cache memory: {mx.get_cache_memory() / 1024**3:.2f} GB\")\n",
    "            print(f\"Peak memory: {mx.get_peak_memory() / 1024**3:.2f} GB\")\n",
    "            mx.clear_cache()\n",
    "            print('-'*20)\n",
    "        losses.append(loss)\n",
    "    stream.reset()\n",
    "    avg_loss = mx.array(losses).mean()\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    generate_story(model, tokenizer, \"[SOS]\", max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "model.save_weights(f'./data/trained_model_weights_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{EPOCHS}.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
