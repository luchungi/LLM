{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939879bc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "CONTEXT_LENGTH = 512  # Fixed context length for chunks\n",
    "EMBEDDING_DIM = 256  # Dimension of the token embeddings\n",
    "NUM_HEADS = 8  # Number of attention heads\n",
    "NUM_LAYERS = 6  # Number of transformer layers\n",
    "QK_HEAD_DIM = 16  # Dimension of the query and key heads\n",
    "V_HEAD_DIM = 32  # Dimension of the value head\n",
    "MLP_DIM = 1024  # Dimension of the hidden layers in the transformer\n",
    "DROPOUT_RATE = 0.1  # Dropout rate for regularization\n",
    "\n",
    "# EMBEDDING_DIM = 128  # Dimension of the token embeddings\n",
    "# NUM_HEADS = 4  # Number of attention heads\n",
    "# NUM_LAYERS = 1  # Number of transformer layers\n",
    "# QK_HEAD_DIM = 8  # Dimension of the query and key heads\n",
    "# V_HEAD_DIM = 16  # Dimension of the value head\n",
    "# MLP_DIM = 512  # Dimension of the hidden layers in the transformer\n",
    "\n",
    "# Data Configuration\n",
    "VOCAB_SIZE = 2048  # Size of the vocabulary\n",
    "PADDING = True # Whether to pad sequences\n",
    "PACKING = True # Whether to pack sequences for training\n",
    "\n",
    "# Training Configuration\n",
    "SEED = 42  # Random seed for reproducibility\n",
    "BATCH_SIZE = 128  # Batch size for training\n",
    "EPOCHS = 20 # Number of epochs to train\n",
    "SAMPLE_LIMIT = 1000000  # Set to None to process the entire dataset\n",
    "LR = 0.001  # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 0.01  # Weight decay for the optimizer\n",
    "BETA1 = 0.9  # Beta1 for the Adam optimizer\n",
    "BETA2 = 0.999  # Beta2 for the Adam optimizer\n",
    "\n",
    "# File Paths and Labels\n",
    "TOKENIZER_FILE = \"./data/tinystories-tokenizer\"\n",
    "CHUNK_FILE = \"./data/chunked_stories\"\n",
    "# LOG_DIR = None\n",
    "LOG_DIR = './runs/2025-08-26_17-09-10'\n",
    "DICT_LABEL = 'seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from llm.modules_torch import SmallLanguageModel, loss_fn, print_story\n",
    "from llm.data import train_tokenizer, chunk_story, create_dict_parameters, encode_story, pack_stories, pretty_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = create_dict_parameters(locals())\n",
    "LOG_DIR = f'runs/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}' if LOG_DIR is None else LOG_DIR\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "if len(list(Path(LOG_DIR).glob('events.out.tfevents.*'))) == 1:\n",
    "    print(f\"Logging parameters\")\n",
    "    writer.add_text('Parameters', pretty_json(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340807f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = f'{TOKENIZER_FILE}_{VOCAB_SIZE}_{SAMPLE_LIMIT}.json'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    sos_token_id = tokenizer.token_to_id('[SOS]')\n",
    "    eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "    pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "    print(f\"Tokenizer file {tokenizer_path} already exists. Skipping training.\")\n",
    "else:\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    if SAMPLE_LIMIT:\n",
    "        dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "    tokenizer = train_tokenizer(dataset, vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"\\n\"])\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "encoded = tokenizer.encode(\"Once upon a time, there was a little fox.\\nIt lived in a forest and loved to explore.\")\n",
    "\n",
    "print(\"\\n--- Testing the Tokenizer ---\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Decoded:\", tokenizer.decode(encoded.ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d52e8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa64fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PADDING and PACKING:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding_packing.npz'\n",
    "elif PADDING:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding.npz'\n",
    "elif PACKING:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_packing.npz'\n",
    "else:\n",
    "    chunk_file_path = f'{CHUNK_FILE}_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}.npz'\n",
    "figure_path = f'./figures/histogram_{VOCAB_SIZE}_{SAMPLE_LIMIT}.png'\n",
    "if os.path.exists(chunk_file_path):\n",
    "    print(f\"Chunk file {chunk_file_path} already exists. Skipping chunking.\")\n",
    "\n",
    "    # display the existing histogram\n",
    "    plt.imshow(plt.imread(figure_path))\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    # Load the tokenizer\n",
    "    if not ('dataset' in locals()):\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "        if SAMPLE_LIMIT:\n",
    "            dataset = dataset.select(range(min(SAMPLE_LIMIT, len(dataset))))\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    # Process all stories and collect chunks\n",
    "    all_chunks = []\n",
    "    unfinished_chunk = []\n",
    "    num_non_special_tokens = []\n",
    "    for story in tqdm(dataset[\"text\"], desc=\"Chunking stories\"):\n",
    "        if PACKING:\n",
    "            story_chunks, non_special_token_count = encode_story(story, tokenizer, '[SOS]', '[EOS]')\n",
    "            all_chunks.append(story_chunks)\n",
    "        else:\n",
    "            story_chunks, unfinished_chunk, non_special_token_count = chunk_story(story, tokenizer, '[SOS]', '[EOS]', CONTEXT_LENGTH,\n",
    "                                                            unfinished_chunk=unfinished_chunk, padding=PADDING, pad_token='[PAD]')\n",
    "            all_chunks.extend(story_chunks)\n",
    "        num_non_special_tokens.append(non_special_token_count)\n",
    "\n",
    "    # Convert list to numpy array for efficient storage\n",
    "    if PACKING:\n",
    "        chunks_array = np.array(pack_stories(all_chunks, CONTEXT_LENGTH, tokenizer.token_to_id('[PAD]')), dtype=np.int32)\n",
    "    else:\n",
    "        chunks_array = np.array(all_chunks, dtype=np.int32)\n",
    "    unique_tokens, counts = np.unique(chunks_array, return_counts=True)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Total tokens: {CONTEXT_LENGTH * chunks_array.shape[0]:,}\")\n",
    "    print(f\"Total non-special tokens: {np.sum(counts[3:]):,}\")\n",
    "    print(f\"Number of special tokens: {np.sum(counts[:3]):,}\")\n",
    "    print(f\"Array shape: {chunks_array.shape}\")\n",
    "\n",
    "    # Save the chunks to a compressed file\n",
    "    print(f\"Saving chunks to {chunk_file_path}...\")\n",
    "    np.savez_compressed(chunk_file_path, chunks=chunks_array)\n",
    "    print(f\"Saved successfully! File size: {os.path.getsize(chunk_file_path) / (1024 * 1024):.2f} MB\")\n",
    "    if PADDING and PACKING:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding_packing.txt'\n",
    "    elif PADDING:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_padding.txt'\n",
    "    elif PACKING:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}_packing.txt'\n",
    "    else:\n",
    "        text_info_path = f'./data/chunk_info_{VOCAB_SIZE}_{CONTEXT_LENGTH}_{SAMPLE_LIMIT}.txt'\n",
    "\n",
    "    plt.hist(num_non_special_tokens, bins=50, color='blue')\n",
    "    plt.title(\"Distribution of Story Lengths\")\n",
    "    plt.xlabel(\"Length (number of tokens)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(figure_path)\n",
    "\n",
    "    with open(text_info_path, 'w') as f:\n",
    "        f.write(f\"Sample limit: {SAMPLE_LIMIT:,}\\n\")\n",
    "        f.write(f\"Vocabulary Size: {VOCAB_SIZE:,}\\n\")\n",
    "        f.write(f\"Context length: {CONTEXT_LENGTH:,}\\n\")\n",
    "        f.write(f\"Number of chunks: {chunks_array.shape[0]:,}\\n\")\n",
    "        f.write(f\"Number of tokens: {CONTEXT_LENGTH * chunks_array.shape[0]:,}\\n\")\n",
    "        f.write(f\"Number of non-special tokens: {np.sum(counts[3:]):,}\\n\")\n",
    "        f.write(f\"Number of special tokens: {np.sum(counts[:3]):,}\\n\")\n",
    "        f.write(f\"Padding used: {PADDING}\\n\")\n",
    "        f.write(f\"Packing used: {PACKING}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4788",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0657c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "data = np.load(chunk_file_path)['chunks']\n",
    "\n",
    "# Create dataset\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = StoryDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "shuffle=True)\n",
    "num_batches = len(dataloader)\n",
    "print(f\"Number of batches per epoch: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataloader:\n",
    "    x = x.numpy()\n",
    "    print(x.shape)\n",
    "    print(type(x))\n",
    "    text = tokenizer.decode(x[0], skip_special_tokens=False).split(' ')\n",
    "    for i in range(0, len(text), 30):\n",
    "        print(' '.join(text[i:i+30]))\n",
    "    text = tokenizer.decode(x[BATCH_SIZE-1], skip_special_tokens=False).split(' ')\n",
    "    for i in range(0, len(text), 30):\n",
    "        print(' '.join(text[i:i+30]))\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa72f0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallLanguageModel(vocab_dim=VOCAB_SIZE, embed_dim=EMBEDDING_DIM, n_head=NUM_HEADS, num_layers=NUM_LAYERS, qk_head_dim=QK_HEAD_DIM, v_head_dim=V_HEAD_DIM, mlp_dim=MLP_DIM, max_len=CONTEXT_LENGTH, dropout_rate=DROPOUT_RATE)\n",
    "model.to(device)\n",
    "# check number of parameters\n",
    "print(f\"Number of parameters in the model: {sum(param.numel() for param in model.parameters()):,}\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, betas=[BETA1, BETA2], weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0f44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0600c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{LOG_DIR}/checkpoint.pt'):\n",
    "    checkpoint = torch.load(f'{LOG_DIR}/checkpoint.pt', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    torch.set_rng_state(checkpoint['rng_state'])\n",
    "    start_epoch_from = checkpoint['start_epoch_from']\n",
    "    print(f\"Resuming training from epoch {start_epoch_from}.\")\n",
    "else:\n",
    "    start_epoch_from = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_story_freq = np.ceil(num_batches / 2) + 1\n",
    "for epoch in range(start_epoch_from, EPOCHS):\n",
    "    losses = []\n",
    "    for i, seq in enumerate(dataloader):\n",
    "        tensor_seq = seq.to(device)\n",
    "        input_seq = tensor_seq[:, :-1]  # Exclude the last token for input\n",
    "        target_seq = tensor_seq[:, 1:]  # Exclude the first token for target\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model, input_seq, target_seq, pad_token_id=pad_token_id if PADDING else None)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Batch {i}, Loss: {loss.item():.4f}\")\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * num_batches + i)\n",
    "        if (i+1) % generate_story_freq == 0:\n",
    "            tokens = model.generate('[SOS]', tokenizer, max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "            print_story(tokens, tokenizer)\n",
    "        losses.append(loss.item())\n",
    "    #     break\n",
    "    # break\n",
    "    avg_loss = np.mean(losses)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/epoch_train', avg_loss.item(), epoch)\n",
    "    tokens = model.generate('[SOS]', tokenizer, max_length=CONTEXT_LENGTH, eos_token_id=eos_token_id, temp=1.0)\n",
    "    print_story(tokens, tokenizer)\n",
    "    checkpoint = {\n",
    "        'start_epoch_from': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "        'loss': avg_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, f'{LOG_DIR}/checkpoint.pt')\n",
    "writer.add_hparams(params, {'hparam/last_loss': avg_loss.item()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
